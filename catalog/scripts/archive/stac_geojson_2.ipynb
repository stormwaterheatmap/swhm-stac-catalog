{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fdde9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import mapping\n",
    "from datetime import datetime, timezone\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import shlex\n",
    "import subprocess\n",
    "import shutil\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be8b2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Google Cloud Storage bucket and prefix (folder) where COGs are located.\n",
    "GCS_BUCKET = \"swhm_data\"  # e.g., \"my-imagery-bucket\"\n",
    "GCS_PREFIX = \"public/layers\"   # e.g., \"sentinel-2/l2a/\" or leave empty for root\n",
    "\n",
    "# The public-facing URL for your GCS bucket.\n",
    "# This is used to create accessible links in the STAC catalog.\n",
    "# For GCS, it's typically \"https://storage.googleapis.com/{BUCKET_NAME}/{FILE_PATH}\"\n",
    "# You could also use a custom domain.\n",
    "ROOT_CATALOG_URL = f\"https://storage.googleapis.com/{GCS_BUCKET}\"\n",
    "CATALOG_JSON_DEST = f\"{ROOT_CATALOG_URL}/{GCS_PREFIX}\"\n",
    "# Where the script will save the generated STAC catalog on your local machine.\n",
    "OUTPUT_DIR = \"../../stac_catalog\"\n",
    "\n",
    "# Details for your STAC Catalog.\n",
    "CATALOG_ID = \"swhm-catalog\"\n",
    "CATALOG_TITLE = \"Stormwater Heatmap Catalog\"\n",
    "CATALOG_DESCRIPTION = \"A STAC catalog for COG imagery stored in GCS, created with rio-stac.\"\n",
    "\n",
    "client = storage.Client(project=\"swhm-prod\")\n",
    "bucket = client.bucket(GCS_BUCKET)\n",
    "# --- End of Configuration ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d80d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs_with_prefix(\n",
    "    bucket_name: str,\n",
    "    prefix: str,\n",
    "    file_extension: str = '.geojson',\n",
    "    delimiter: str = None\n",
    ") -> list[storage.blob.Blob]:\n",
    "    \"\"\"\n",
    "    Lists all the blobs in a GCS bucket with a given prefix and file extension,\n",
    "    and returns the Blob objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    blobs = client.list_blobs(\n",
    "        bucket_name,\n",
    "        prefix=prefix,\n",
    "        delimiter=delimiter\n",
    "    )\n",
    "\n",
    "    print(f\"Fetching blobs from bucket '{bucket_name}' with prefix '{prefix}'...\")\n",
    "\n",
    "    matching_blobs = []\n",
    "    for blob in blobs:\n",
    "        name_lower = blob.name.lower()\n",
    "        if name_lower.endswith(file_extension) or name_lower.endswith('.geojson'):\n",
    "            \n",
    "            matching_blobs.append(blob)\n",
    "\n",
    "    if delimiter:\n",
    "        prefixes = getattr(blobs, 'prefixes', None)\n",
    "        if prefixes:\n",
    "            print(\"Sub-prefixes found:\")\n",
    "            for p in prefixes:\n",
    "                print(f\"  {p}\")\n",
    "\n",
    "    return matching_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "509c71ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching blobs from bucket 'swhm_data' with prefix 'public/layers'...\n"
     ]
    }
   ],
   "source": [
    "blobs = list_blobs_with_prefix(GCS_BUCKET, GCS_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a649d41",
   "metadata": {},
   "source": [
    "## Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46d2dd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Blob: swhm_data, public/layers/vector/cig_grid_wgs/cig_grid_wgs.geojson, 1751488021970745>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#base geom on the first blob \n",
    "first_blob = blobs[1]\n",
    "first_blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81d314c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from https://storage.googleapis.com/swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.geojson...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/15903z7124l3th5fm7wg0lgw0000gn/T/ipykernel_83383/82699998.py:8: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  footprint_geom = mapping(gdf.unary_union.convex_hull)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(-124.75864669837131),\n",
       " np.float64(46.604450743574134),\n",
       " np.float64(-121.60590179263949),\n",
       " np.float64(49.05931514730483)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_url = first_blob.public_url\n",
    "item_id = os.path.splitext(os.path.basename(fc_url))[0]\n",
    "item_id\n",
    "\n",
    "fc_url = first_blob.public_url\n",
    "print(f\"Reading data from {fc_url}...\")\n",
    "gdf = gpd.read_file(fc_url)\n",
    "footprint_geom = mapping(gdf.unary_union.convex_hull)\n",
    "bounds = gdf.total_bounds\n",
    "bbox = list(bounds)\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26230fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collection \n",
    "\n",
    "item_datetime = datetime.now(timezone.utc)\n",
    "collection_id = \"vector\"\n",
    "collection_description = \"Vectors and things.\"\n",
    "collection_license = \"PDDL-1.0\"  # Public Domain Dedication and License\n",
    "\n",
    "spatial_extent = pystac.SpatialExtent(bboxes=[bbox])\n",
    "temporal_extent = pystac.TemporalExtent(intervals=[[item_datetime, None]])\n",
    "collection_extent = pystac.Extent(spatial=spatial_extent, temporal=temporal_extent)\n",
    "\n",
    "collection = pystac.Collection(\n",
    "    id=collection_id,\n",
    "    description=collection_description,\n",
    "    extent=collection_extent,\n",
    "    license=collection_license,\n",
    "    title=\"vectors\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ea08fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from https://storage.googleapis.com/swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.geojson...\n",
      "Reading data from https://storage.googleapis.com/swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.geojson...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/15903z7124l3th5fm7wg0lgw0000gn/T/ipykernel_83383/2591796469.py:20: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  footprint_geom = mapping(gdf.unary_union.convex_hull)\n",
      "/var/folders/yn/15903z7124l3th5fm7wg0lgw0000gn/T/ipykernel_83383/2591796469.py:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  item_datetime = datetime.utcnow()\n",
      "/var/folders/yn/15903z7124l3th5fm7wg0lgw0000gn/T/ipykernel_83383/2591796469.py:20: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  footprint_geom = mapping(gdf.unary_union.convex_hull)\n",
      "/var/folders/yn/15903z7124l3th5fm7wg0lgw0000gn/T/ipykernel_83383/2591796469.py:23: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  item_datetime = datetime.utcnow()\n"
     ]
    }
   ],
   "source": [
    "# List to collect all items\n",
    "stac_items = []\n",
    "# Loop over all blobs (skip non-GeoJSON files)\n",
    "for blob in blobs:\n",
    "    fc_url = blob.public_url\n",
    "\n",
    "    # Skip non-GeoJSON files\n",
    "    if not fc_url.endswith(\".geojson\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading data from {fc_url}...\")\n",
    "        gdf = gpd.read_file(fc_url)\n",
    "\n",
    "        if gdf.empty:\n",
    "            print(f\"Warning: Skipping empty GeoDataFrame from {fc_url}\")\n",
    "            continue\n",
    "\n",
    "        item_id = os.path.splitext(os.path.basename(fc_url))[0]\n",
    "        footprint_geom = mapping(gdf.unary_union.convex_hull)\n",
    "        bbox = list(gdf.total_bounds)\n",
    "\n",
    "        item_datetime = datetime.utcnow()\n",
    "\n",
    "        item = pystac.Item(\n",
    "            id=item_id,\n",
    "            geometry=footprint_geom,\n",
    "            bbox=bbox,\n",
    "            datetime=item_datetime,\n",
    "            properties={\n",
    "                \"source\": \"GCS blob\",\n",
    "                \"feature_count\": len(gdf)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Optionally set custom metadata on the blob\n",
    "        blob.metadata = {'processed_by': 'STAC batch script'}\n",
    "\n",
    "        asset = pystac.Asset(\n",
    "            href=fc_url,\n",
    "            media_type=blob.content_type,\n",
    "            title=item_id,\n",
    "            roles=[\"data\"]\n",
    "        )\n",
    "\n",
    "        item.add_asset(\"GeoJSON_data\", asset)\n",
    "\n",
    "        stac_items.append(item)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fc_url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d644a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #make item from blob\n",
    "\n",
    "# first_blob = blobs[1]\n",
    "# first_blob\n",
    "# #<Blob: swhm_data, public/layers/vector/cig_grid_wgs/cig_grid_wgs.geojson, 1751488021970745>\n",
    "# fc_url = first_blob.public_url\n",
    "# item_id = os.path.splitext(os.path.basename(fc_url))[0]\n",
    "# item_id\n",
    "\n",
    "# fc_url = first_blob.public_url\n",
    "# print(f\"Reading data from {fc_url}...\")\n",
    "# gdf = gpd.read_file(fc_url)\n",
    "# footprint_geom = mapping(gdf.unary_union.convex_hull)\n",
    "# bounds = gdf.total_bounds\n",
    "# bbox = list(bounds)\n",
    "# bbox\n",
    "\n",
    "# ## Items \n",
    "\n",
    "# item_id = os.path.splitext(os.path.basename(fc_url))[0]\n",
    "# item = pystac.Item(\n",
    "#     id=item_id,\n",
    "#     geometry=footprint_geom,\n",
    "#     bbox=bbox,\n",
    "#     datetime=item_datetime,\n",
    "#     properties={}\n",
    "# )\n",
    "# ## Asset \n",
    "# metadata = {'color': 'Red', 'name': 'Test'}\n",
    "# first_blob.metadata = metadata\n",
    "\n",
    "# asset_href = first_blob.public_url\n",
    "# asset_media_type = first_blob.content_type\n",
    "# first_blob.metadata\n",
    "\n",
    "# asset_title = \"CIG GRID\"\n",
    "# asset = pystac.Asset(\n",
    "#     href=asset_href,\n",
    "#     media_type=asset_media_type,\n",
    "#     title=asset_title,\n",
    "#     roles=[\"data\"]\n",
    "# )\n",
    "\n",
    "# item.add_asset(\"GeoJSON_data\", asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f70c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62cffbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add item to collection \n",
    "for item in stac_items:\n",
    "    collection.add_item(item)\n",
    "\n",
    "collection.set_self_href(\"collection.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aefe6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.normalize_hrefs(\n",
    "    root_href=\"https://storage.googleapis.com/swhm_data/public/layers/vector\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e089de",
   "metadata": {},
   "source": [
    "## Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "26e347ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_stac_assets(root_dir, bucket, prefix):\n",
    "    \"\"\"\n",
    "    Finds and uploads STAC asset files to Google Cloud Storage.\n",
    "\n",
    "    This function walks through the specified root directory and looks for\n",
    "    JSON files that have the same name as their parent directory\n",
    "    (e.g., 'asset_a/asset_a.json'). It then uploads them to a\n",
    "    specified GCS bucket, maintaining the relative directory structure.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The absolute path to the directory to search in.\n",
    "        bucket (str): The name of the GCS bucket to upload to.\n",
    "        prefix (str): The prefix (sub-folder) within the GCS bucket.\n",
    "    \"\"\"\n",
    "    print(f\"Starting scan in: {root_dir}\")\n",
    "    print(f\"Uploading to: gs://{bucket}/{prefix}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # os.walk is perfect for recursively scanning a directory tree.\n",
    "    # It yields the current directory path, a list of subdirectories, and a list of files.\n",
    "    # We use '_' for dirnames as it's not used in this loop.\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        # Get the name of the current directory being processed\n",
    "        current_dir_name = os.path.basename(dirpath)\n",
    "        # Construct the expected filename (e.g., directory 'asset_a' -> file 'asset_a.json')\n",
    "        expected_filename = f\"{current_dir_name}.json\"\n",
    "\n",
    "        # Check if a file with the expected name exists in the current directory\n",
    "        if expected_filename in filenames:\n",
    "            # Construct the full local path to the source file\n",
    "            local_file_path = os.path.join(dirpath, expected_filename)\n",
    "\n",
    "            # Determine the relative path from the root_dir.\n",
    "            # This is used to replicate the directory structure in GCS.\n",
    "            relative_path = os.path.relpath(local_file_path, root_dir)\n",
    "\n",
    "            # Construct the destination path in GCS\n",
    "            gcs_destination = f\"gs://{bucket}/{prefix}{relative_path}\"\n",
    "\n",
    "            print(f\"Found matching asset: {local_file_path}\")\n",
    "            print(f\"  -> Uploading to: {gcs_destination}\")\n",
    "\n",
    "            try:\n",
    "                # Build the command as a list for security and reliability\n",
    "                gc_cmd = [\n",
    "                    \"gsutil\",\n",
    "                    \"cp\",\n",
    "                    local_file_path,\n",
    "                    gcs_destination\n",
    "                ]\n",
    "\n",
    "                # For logging, create a shell-safe string representation\n",
    "                bash_command = ' '.join(shlex.quote(arg) for arg in gc_cmd)\n",
    "                print(f\"  -> Executing: {bash_command}\")\n",
    "\n",
    "                # Execute the command\n",
    "                # check=True will raise an exception if gsutil returns an error\n",
    "                subprocess.run(gc_cmd, capture_output=True, text=True, check=True)\n",
    "\n",
    "                print(\"  -> Upload successful!\")\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                print(\"  -> ERROR: 'gsutil' command not found.\")\n",
    "                print(\"     Please ensure the Google Cloud SDK is installed and in your PATH.\")\n",
    "                # Stop the script if gsutil isn't available\n",
    "                return\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                # This block runs if gsutil returns a non-zero exit code (an error)\n",
    "                print(f\"  -> ERROR: Upload failed for {local_file_path}\")\n",
    "                print(f\"  -> gsutil stderr: {e.stderr}\")\n",
    "\n",
    "            print(\"-\" * 30)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a004a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_stac_assets(root_dir, bucket, prefix, dry_run=False, return_summary=False):\n",
    "    \"\"\"\n",
    "    Finds and uploads STAC asset JSON files to Google Cloud Storage.\n",
    "\n",
    "    - Uploads all .json files in the root directory (regardless of name).\n",
    "    - Uploads .json files in subdirectories that match the directory name (e.g. 'foo/foo.json').\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Absolute or relative path to the root directory.\n",
    "        bucket (str): GCS bucket name.\n",
    "        prefix (str): Path prefix within the GCS bucket.\n",
    "        dry_run (bool): If True, simulate uploads without executing them.\n",
    "        return_summary (bool): If True, return a summary dictionary of results.\n",
    "\n",
    "    Returns:\n",
    "        dict (optional): Summary of uploaded, skipped, and failed files.\n",
    "    \"\"\"\n",
    "    root_path = Path(root_dir).resolve()\n",
    "\n",
    "    if not root_path.is_dir():\n",
    "        print(f\"ERROR: {root_path} is not a valid directory.\")\n",
    "        return\n",
    "\n",
    "    if not shutil.which(\"gsutil\"):\n",
    "        print(\"ERROR: 'gsutil' command not found in PATH.\")\n",
    "        return\n",
    "\n",
    "    if not prefix.endswith(\"/\"):\n",
    "        prefix += \"/\"\n",
    "\n",
    "    print(f\"Scanning: {root_path}\")\n",
    "    print(f\"Uploading to: gs://{bucket}/{prefix}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    uploaded = []\n",
    "    skipped = []\n",
    "    failed = []\n",
    "\n",
    "    # Helper: Upload a single file\n",
    "    def upload_file(file_path):\n",
    "        relative_path = file_path.relative_to(root_path)\n",
    "        gcs_path = f\"gs://{bucket}/{prefix}{relative_path.as_posix()}\"\n",
    "\n",
    "        print(f\"Found: {file_path}\")\n",
    "        print(f\"  -> GCS Path: {gcs_path}\")\n",
    "\n",
    "        if dry_run:\n",
    "            print(f\"  -> DRY RUN: Skipping actual upload.\")\n",
    "            skipped.append(str(file_path))\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            gc_cmd = [\"gsutil\", \"cp\", str(file_path), gcs_path]\n",
    "            print(f\"  -> Executing: {' '.join(shlex.quote(arg) for arg in gc_cmd)}\")\n",
    "            subprocess.run(gc_cmd, capture_output=True, text=True, check=True)\n",
    "            print(\"  -> Upload successful!\")\n",
    "            uploaded.append(str(file_path))\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  -> ERROR: Upload failed for {file_path}\")\n",
    "            print(f\"     stderr: {e.stderr}\")\n",
    "            failed.append((str(file_path), e.stderr))\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Upload all .json files in root directory\n",
    "    for json_file in root_path.glob(\"*.json\"):\n",
    "        upload_file(json_file)\n",
    "\n",
    "    # Upload matching <dir>/<dir>.json files in subdirectories\n",
    "    for dir_path in root_path.rglob(\"*\"):\n",
    "        if dir_path.is_dir():\n",
    "            expected_json = dir_path / f\"{dir_path.name}.json\"\n",
    "            if expected_json.exists():\n",
    "                upload_file(expected_json)\n",
    "\n",
    "    if return_summary:\n",
    "        return {\n",
    "            \"uploaded\": uploaded,\n",
    "            \"skipped\": skipped,\n",
    "            \"failed\": failed,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfcdb45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector\n",
      "Uploading to: gs://swhm_data/public/layers/vector/\n",
      "----------------------------------------\n",
      "Found: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector/collection.json\n",
      "  -> GCS Path: gs://swhm_data/public/layers/vector/collection.json\n",
      "  -> Executing: gsutil cp /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector/collection.json gs://swhm_data/public/layers/vector/collection.json\n",
      "  -> Upload successful!\n",
      "----------------------------------------\n",
      "Found: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "  -> GCS Path: gs://swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "  -> Executing: gsutil cp /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector/cig_grid_wgs/cig_grid_wgs.json gs://swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "  -> Upload successful!\n",
      "----------------------------------------\n",
      "Found: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "  -> GCS Path: gs://swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json\n",
      "  -> Executing: gsutil cp /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/stac_catalog/vector/PugetSoundWA/PugetSoundWA.json gs://swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json\n",
      "  -> Upload successful!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "upload_stac_assets(root_dir='../../stac_catalog/vector', bucket=GCS_BUCKET,prefix=f\"{GCS_PREFIX}/vector/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb7e4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save collection \n",
    "output_path = f\"{OUTPUT_DIR}/{collection_id}\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "#collection.normalize_hrefs(os.path.dirname(output_path))\n",
    "collection.save(dest_href=output_path, catalog_type=\"ABSOLUTE PUBLISHED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
