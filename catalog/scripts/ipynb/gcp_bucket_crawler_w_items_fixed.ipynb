{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a8b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GCP Bucket Crawler and Catalog Generator\n",
    "Crawls a GCP storage bucket to discover vector and raster data,\n",
    "then generates collections, individual STAC items, and a comprehensive catalog.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import shutil\n",
    "import shlex\n",
    "import subprocess\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6f28e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPBucketCrawler:\n",
    "    def __init__(self, bucket_name: str, prefix: str = \"\", project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler with GCP bucket details.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the GCP storage bucket (e.g., 'swhm_data')\n",
    "            prefix: Prefix to filter objects (e.g., 'public/layers/')\n",
    "            project_id: GCP project ID (optional, will use default if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.project_id = project_id\n",
    "        self.vectors = []\n",
    "        self.rasters = []\n",
    "        self.processed_items = set()  # Track processed items to prevent duplicates\n",
    "        \n",
    "        # Initialize the GCS client\n",
    "        try:\n",
    "            if project_id:\n",
    "                self.client = storage.Client(project=project_id)\n",
    "            else:\n",
    "                self.client = storage.Client()\n",
    "            self.bucket = self.client.bucket(bucket_name)\n",
    "            print(f\"Successfully connected to bucket: {bucket_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing GCS client: {e}\")\n",
    "            print(\"Make sure you have proper authentication set up:\")\n",
    "            print(\"1. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
    "            print(\"2. Or run 'gcloud auth application-default login'\")\n",
    "            self.client = None\n",
    "            self.bucket = None\n",
    "        \n",
    "    def crawl_bucket(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl the GCP bucket to discover all vectors and rasters.\n",
    "        Returns a dictionary with discovered items.\n",
    "        \"\"\"\n",
    "        if not self.client or not self.bucket:\n",
    "            print(\"No valid GCS client available, creating sample data...\")\n",
    "            return self._create_sample_data()\n",
    "            \n",
    "        print(f\"Crawling bucket '{self.bucket_name}' with prefix '{self.prefix}'...\")\n",
    "        \n",
    "        try:\n",
    "            # List all blobs in the bucket with the specified prefix\n",
    "            blobs = self.bucket.list_blobs(prefix=self.prefix)\n",
    "            \n",
    "            blob_count = 0\n",
    "            for blob in blobs:\n",
    "                blob_count += 1\n",
    "                self._process_blob(blob)\n",
    "                \n",
    "            print(f\"Processed {blob_count} objects from bucket\")\n",
    "            print(f\"Found {len(self.vectors)} unique vectors and {len(self.rasters)} unique rasters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling bucket: {e}\")\n",
    "            return self._create_sample_data()\n",
    "            \n",
    "        return {\n",
    "            'vectors': self.vectors,\n",
    "            'rasters': self.rasters,\n",
    "            'total_items': len(self.vectors) + len(self.rasters)\n",
    "        }\n",
    "    \n",
    "    def _process_blob(self, blob):\n",
    "        \"\"\"Process a single blob to determine if it's a vector or raster.\"\"\"\n",
    "        blob_name = blob.name\n",
    "        blob_path = Path(blob_name)\n",
    "        \n",
    "        # Skip directories (blobs ending with '/')\n",
    "        if blob_name.endswith('/'):\n",
    "            return\n",
    "            \n",
    "        # Check for vector files - ONLY .geojson files, NOT .json files\n",
    "        if 'vector/' in blob_name and blob_path.suffix.lower() == '.geojson':\n",
    "            self._add_vector_item(blob)\n",
    "            \n",
    "        # Check for raster files - TIFF files including .gtiff  \n",
    "        elif 'raster/' in blob_name and blob_path.suffix.lower() in ['.tiff', '.tif', '.gtiff']:\n",
    "            self._add_raster_item(blob)\n",
    "    \n",
    "    def _add_vector_item(self, blob):\n",
    "        \"\"\"Add a vector item to the collection.\"\"\"\n",
    "        blob_path = Path(blob.name)\n",
    "        item_name = blob_path.stem\n",
    "        \n",
    "        # Create unique identifier to prevent duplicates\n",
    "        item_key = f\"vector:{item_name}\"\n",
    "        if item_key in self.processed_items:\n",
    "            print(f\"Skipping duplicate vector: {item_name}\")\n",
    "            return\n",
    "        \n",
    "        # Create public URL\n",
    "        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "        \n",
    "        # Get the directory path where the STAC item should be saved\n",
    "        # e.g., public/layers/vector/PugetSoundWA/PugetSoundWA.geojson -> vector/PugetSoundWA/\n",
    "        parent_dir = blob_path.parent\n",
    "        stac_dir = str(parent_dir).replace(self.prefix, '')\n",
    "        \n",
    "        vector_item = {\n",
    "            'name': item_name,\n",
    "            'filename': blob.name,\n",
    "            'url': public_url,\n",
    "            'type': 'vector',\n",
    "            'format': 'GeoJSON',\n",
    "            'size_bytes': blob.size,\n",
    "            'content_type': blob.content_type,\n",
    "            'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "            'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "            'discovered_at': datetime.now().isoformat(),\n",
    "            'etag': blob.etag,\n",
    "            'md5_hash': blob.md5_hash,\n",
    "            'stac_dir': stac_dir  # Directory where STAC item should be saved\n",
    "        }\n",
    "        \n",
    "        self.vectors.append(vector_item)\n",
    "        self.processed_items.add(item_key)\n",
    "        print(f\"Found vector: {item_name}\")\n",
    "    \n",
    "    def _add_raster_item(self, blob):\n",
    "        \"\"\"Add a raster item to the collection.\"\"\"\n",
    "        blob_path = Path(blob.name)\n",
    "        item_name = blob_path.stem\n",
    "        \n",
    "        # Create unique identifier to prevent duplicates\n",
    "        item_key = f\"raster:{item_name}\"\n",
    "        if item_key in self.processed_items:\n",
    "            print(f\"Skipping duplicate raster: {item_name}\")\n",
    "            return\n",
    "        \n",
    "        # Create public URL\n",
    "        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "        \n",
    "        # Get the directory path where the STAC item should be saved\n",
    "        # e.g., public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.tif -> raster/Age_of_Imperviousness/\n",
    "        parent_dir = blob_path.parent\n",
    "        stac_dir = str(parent_dir).replace(self.prefix, '')\n",
    "        \n",
    "        raster_item = {\n",
    "            'name': item_name,\n",
    "            'filename': blob.name,\n",
    "            'url': public_url,\n",
    "            'type': 'raster',\n",
    "            'format': 'GeoTIFF',\n",
    "            'size_bytes': blob.size,\n",
    "            'content_type': blob.content_type,\n",
    "            'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "            'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "            'discovered_at': datetime.now().isoformat(),\n",
    "            'etag': blob.etag,\n",
    "            'md5_hash': blob.md5_hash,\n",
    "            'stac_dir': stac_dir  # Directory where STAC item should be saved\n",
    "        }\n",
    "        \n",
    "        self.rasters.append(raster_item)\n",
    "        self.processed_items.add(item_key)\n",
    "        print(f\"Found raster: {item_name}\")\n",
    "    \n",
    "    def get_blob_info(self, blob_name: str) -> Optional[Dict]:\n",
    "        \"\"\"Get detailed information about a specific blob.\"\"\"\n",
    "        if not self.bucket:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            blob = self.bucket.blob(blob_name)\n",
    "            if blob.exists():\n",
    "                return {\n",
    "                    'name': blob.name,\n",
    "                    'size': blob.size,\n",
    "                    'content_type': blob.content_type,\n",
    "                    'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "                    'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "                    'etag': blob.etag,\n",
    "                    'md5_hash': blob.md5_hash,\n",
    "                    'public_url': f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting blob info for {blob_name}: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def _create_sample_data(self):\n",
    "        \"\"\"Create sample data structure when bucket can't be crawled directly.\"\"\"\n",
    "        print(\"Creating sample data structure...\")\n",
    "        \n",
    "        base_url = f\"https://storage.googleapis.com/{self.bucket_name}\"\n",
    "        \n",
    "        # Sample vectors based on your structure\n",
    "        sample_vectors = [\n",
    "            {\n",
    "                'name': 'vector1',\n",
    "                'filename': f'{self.prefix}vector/vector1/vector1.geojson',\n",
    "                'url': f\"{base_url}/{self.prefix}vector/vector1/vector1.geojson\",\n",
    "                'type': 'vector',\n",
    "                'format': 'GeoJSON',\n",
    "                'size_bytes': None,\n",
    "                'content_type': 'application/geo+json',\n",
    "                'created': None,\n",
    "                'updated': None,\n",
    "                'discovered_at': datetime.now().isoformat(),\n",
    "                'etag': None,\n",
    "                'md5_hash': None,\n",
    "                'stac_dir': 'vector/vector1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Sample rasters based on your structure\n",
    "        sample_rasters = [\n",
    "            {\n",
    "                'name': 'raster1',\n",
    "                'filename': f'{self.prefix}raster/raster1/raster1.tiff',\n",
    "                'url': f\"{base_url}/{self.prefix}raster/raster1/raster1.tiff\",\n",
    "                'type': 'raster',\n",
    "                'format': 'GeoTIFF',\n",
    "                'size_bytes': None,\n",
    "                'content_type': 'image/tiff',\n",
    "                'created': None,\n",
    "                'updated': None,\n",
    "                'discovered_at': datetime.now().isoformat(),\n",
    "                'etag': None,\n",
    "                'md5_hash': None,\n",
    "                'stac_dir': 'raster/raster1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.vectors = sample_vectors\n",
    "        self.rasters = sample_rasters\n",
    "        \n",
    "        return {\n",
    "            'vectors': self.vectors,\n",
    "            'rasters': self.rasters,\n",
    "            'total_items': len(self.vectors) + len(self.rasters)\n",
    "        }\n",
    "\n",
    "class CatalogGenerator:\n",
    "    def __init__(self, crawler_data: Dict):\n",
    "        \"\"\"Initialize with data from the crawler.\"\"\"\n",
    "        self.data = crawler_data\n",
    "        self.stac_items = []\n",
    "        \n",
    "    def generate_stac_item(self, item_data: Dict, item_type: str) -> Dict:\n",
    "        \"\"\"Generate a STAC item for vector or raster data.\"\"\"\n",
    "        item_id = item_data['name']\n",
    "        \n",
    "        # Base STAC item structure\n",
    "        stac_item = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": item_id,\n",
    "            \"properties\": {\n",
    "                \"title\": item_data['name'].replace('_', ' ').title(),\n",
    "                \"description\": f\"{item_type.title()} dataset: {item_data['name']}\",\n",
    "                \"datetime\": item_data['discovered_at'],\n",
    "                \"created\": item_data.get('created') or item_data['discovered_at'],\n",
    "                \"updated\": item_data.get('updated') or item_data['discovered_at'],\n",
    "                \"providers\": [\n",
    "                    {\n",
    "                        \"name\": \"SWHM Data\",\n",
    "                        \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                        \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"geometry\": None,  # Would need to extract from actual data\n",
    "            \"bbox\": None,  # Would need to calculate from geometry/bounds\n",
    "            \"assets\": {},\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{item_data['stac_dir']}/{item_id}.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{item_type}/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"collection\",\n",
    "                    \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{item_type}/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add assets based on type\n",
    "        if item_type == 'vector':\n",
    "            stac_item[\"assets\"][\"data\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"application/geo+json\",\n",
    "                \"title\": \"GeoJSON data\",\n",
    "                \"description\": \"Vector data in GeoJSON format\",\n",
    "                \"roles\": [\"data\"],\n",
    "                \"file:size\": item_data.get('size_bytes'),\n",
    "                \"file:checksum\": item_data.get('md5_hash')\n",
    "            }\n",
    "        elif item_type == 'raster':\n",
    "            stac_item[\"assets\"][\"data\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"image/tiff; application=geotiff\",\n",
    "                \"title\": \"GeoTIFF data\",\n",
    "                \"description\": \"Raster data in GeoTIFF format\",\n",
    "                \"roles\": [\"data\"],\n",
    "                \"file:size\": item_data.get('size_bytes'),\n",
    "                \"file:checksum\": item_data.get('md5_hash')\n",
    "            }\n",
    "            \n",
    "            # Add COG asset if it's a Cloud Optimized GeoTIFF\n",
    "            stac_item[\"assets\"][\"cog\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                \"title\": \"Cloud Optimized GeoTIFF\",\n",
    "                \"description\": \"Cloud Optimized GeoTIFF for web access\",\n",
    "                \"roles\": [\"data\", \"overview\"]\n",
    "            }\n",
    "        \n",
    "        # Only add metadata and thumbnail assets if they exist (no placeholders)\n",
    "        # This prevents STAC Browser from trying to load non-existent resources\n",
    "        \n",
    "        return stac_item\n",
    "    \n",
    "    def generate_vector_collection(self) -> Dict:\n",
    "        \"\"\"Generate a vector collection with individual STAC items.\"\"\"\n",
    "        collection = {\n",
    "            \"type\": \"Collection\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-vector\",\n",
    "            \"title\": \"SWHM Vector Collection\",\n",
    "            \"description\": \"Collection of vector datasets from SWHM data bucket\",\n",
    "            \"keywords\": [\"vector\", \"geojson\", \"swhm\"],\n",
    "            \"license\": \"proprietary\",\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\n",
    "                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n",
    "                },\n",
    "                \"temporal\": {\n",
    "                    \"interval\": [[None, None]]\n",
    "                }\n",
    "            },\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/vector/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ],\n",
    "            \"item_assets\": {\n",
    "                \"data\": {\n",
    "                    \"type\": \"application/geo+json\",\n",
    "                    \"title\": \"GeoJSON data\",\n",
    "                    \"roles\": [\"data\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add links to individual items - crawler already handles deduplication\n",
    "        for vector in self.data['vectors']:\n",
    "            item_id = vector['name']\n",
    "            collection[\"links\"].append({\n",
    "                \"rel\": \"item\",\n",
    "                \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{vector['stac_dir']}/{item_id}.json\",\n",
    "                \"type\": \"application/json\",\n",
    "                \"title\": vector['name'].replace('_', ' ').title()\n",
    "            })\n",
    "            \n",
    "        return collection\n",
    "    \n",
    "    def generate_raster_collection(self) -> Dict:\n",
    "        \"\"\"Generate a raster collection with individual STAC items.\"\"\"\n",
    "        collection = {\n",
    "            \"type\": \"Collection\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-raster\",\n",
    "            \"title\": \"SWHM Raster Collection\",\n",
    "            \"description\": \"Collection of raster datasets from SWHM data bucket\",\n",
    "            \"keywords\": [\"raster\", \"geotiff\", \"swhm\"],\n",
    "            \"license\": \"proprietary\",\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\n",
    "                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n",
    "                },\n",
    "                \"temporal\": {\n",
    "                    \"interval\": [[None, None]]\n",
    "                }\n",
    "            },\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/raster/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ],\n",
    "            \"item_assets\": {\n",
    "                \"data\": {\n",
    "                    \"type\": \"image/tiff; application=geotiff\",\n",
    "                    \"title\": \"GeoTIFF data\",\n",
    "                    \"roles\": [\"data\"]\n",
    "                },\n",
    "                \"cog\": {\n",
    "                    \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                    \"title\": \"Cloud Optimized GeoTIFF\",\n",
    "                    \"roles\": [\"data\", \"overview\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add links to individual items - crawler already handles deduplication\n",
    "        for raster in self.data['rasters']:\n",
    "            item_id = raster['name']\n",
    "            collection[\"links\"].append({\n",
    "                \"rel\": \"item\",\n",
    "                \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{raster['stac_dir']}/{item_id}.json\",\n",
    "                \"type\": \"application/json\",\n",
    "                \"title\": raster['name'].replace('_', ' ').title()\n",
    "            })\n",
    "            \n",
    "        return collection\n",
    "    \n",
    "    def generate_master_catalog(self, vector_collection: Dict, raster_collection: Dict) -> Dict:\n",
    "        \"\"\"Generate the master catalog containing all collections.\"\"\"\n",
    "        catalog = {\n",
    "            \"type\": \"Catalog\",\n",
    "            \"catalog_type\": \"Published_Absolute\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-data-catalog\",\n",
    "            \"title\": \"SWHM Data Catalog\",\n",
    "            \"description\": \"Master catalog for SWHM vector and raster datasets\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"updated\": datetime.now().isoformat(),\n",
    "            \"keywords\": [\"swhm\", \"vector\", \"raster\", \"geospatial\"],\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"SWHM Data Catalog\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"child\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/vector/collection.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"Vector Collection\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"child\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/raster/collection.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"Raster Collection\"\n",
    "                }\n",
    "            ],\n",
    "            \"conformsTo\": [\n",
    "                \"https://api.stacspec.org/v1.0.0/core\",\n",
    "                \"https://api.stacspec.org/v1.0.0/collections\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return catalog\n",
    "    \n",
    "    def generate_all_stac_items(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Generate all STAC items for vectors and rasters.\"\"\"\n",
    "        vector_items = []\n",
    "        raster_items = []\n",
    "        \n",
    "        # Generate vector items - crawler already handles deduplication\n",
    "        for vector in self.data['vectors']:\n",
    "            stac_item = self.generate_stac_item(vector, 'vector')\n",
    "            vector_items.append(stac_item)\n",
    "            \n",
    "        # Generate raster items - crawler already handles deduplication\n",
    "        for raster in self.data['rasters']:\n",
    "            stac_item = self.generate_stac_item(raster, 'raster')\n",
    "            raster_items.append(stac_item)\n",
    "            \n",
    "        return {\n",
    "            'vector_items': vector_items,\n",
    "            'raster_items': raster_items\n",
    "        }\n",
    "\n",
    "def save_json(data: Dict, filepath: str):\n",
    "    \"\"\"Save data to JSON file with pretty formatting.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c43e66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCSUploader:\n",
    "    \"\"\"\n",
    "    Handles uploading STAC catalog files to Google Cloud Storage.\n",
    "    Supports both gsutil and native GCS client approaches.\n",
    "    Sets Cache-Control headers to prevent browser caching of JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the GCS uploader.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the GCS bucket\n",
    "            project_id: GCP project ID (optional)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.project_id = project_id\n",
    "        self.use_gsutil = shutil.which(\"gsutil\") is not None\n",
    "        \n",
    "        # Try to initialize GCS client as fallback\n",
    "        self.client = None\n",
    "        self.bucket = None\n",
    "        if not self.use_gsutil:\n",
    "            try:\n",
    "                if project_id:\n",
    "                    self.client = storage.Client(project=project_id)\n",
    "                else:\n",
    "                    self.client = storage.Client()\n",
    "                self.bucket = self.client.bucket(bucket_name)\n",
    "                print(\"Using native GCS client for uploads\")\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: Could not initialize GCS client: {e}\")\n",
    "                print(\"Upload functionality will be limited\")\n",
    "        else:\n",
    "            print(\"Using gsutil for uploads\")\n",
    "    \n",
    "    def upload_directory(self, root_dir: str, prefix: str = \"\", dry_run: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Upload all STAC JSON files from a directory structure to GCS.\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Local directory containing STAC files\n",
    "            prefix: GCS path prefix (e.g., \"public/layers/\")\n",
    "            dry_run: If True, show what would be uploaded without doing it\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with upload results\n",
    "        \"\"\"\n",
    "        root_path = Path(root_dir).resolve()\n",
    "        \n",
    "        if not root_path.is_dir():\n",
    "            raise ValueError(f\"Directory does not exist: {root_path}\")\n",
    "        \n",
    "        if prefix and not prefix.endswith(\"/\"):\n",
    "            prefix += \"/\"\n",
    "        \n",
    "        print(f\"Scanning directory: {root_path}\")\n",
    "        print(f\"Target GCS location: gs://{self.bucket_name}/{prefix}\")\n",
    "        print(f\"Dry run: {dry_run}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = {\n",
    "            \"uploaded\": [],\n",
    "            \"skipped\": [],\n",
    "            \"failed\": [],\n",
    "            \"total_files\": 0\n",
    "        }\n",
    "        \n",
    "        # Find all JSON files to upload\n",
    "        json_files = list(root_path.rglob(\"*.json\"))\n",
    "        results[\"total_files\"] = len(json_files)\n",
    "        \n",
    "        if not json_files:\n",
    "            print(\"No JSON files found to upload\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files to upload\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                self._upload_single_file(json_file, root_path, prefix, dry_run, results)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Failed to upload {json_file}: {e}\")\n",
    "                results[\"failed\"].append({\n",
    "                    \"file\": str(json_file),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(results)\n",
    "        return results\n",
    "    \n",
    "    def _upload_single_file(self, file_path: Path, root_path: Path, prefix: str, \n",
    "                          dry_run: bool, results: Dict):\n",
    "        \"\"\"Upload a single file to GCS.\"\"\"\n",
    "        # Get relative path from the catalog root, not the full local path\n",
    "        relative_path = file_path.relative_to(root_path)\n",
    "        \n",
    "        # Remove any leading \"catalog/\" from the relative path if it exists\n",
    "        # This ensures we upload to the correct GCS structure\n",
    "        path_parts = relative_path.parts\n",
    "        if path_parts[0] == \"catalog\":\n",
    "            # Strip the \"catalog\" prefix - we want vector/collection.json not catalog/vector/collection.json\n",
    "            relative_path = Path(*path_parts[1:])\n",
    "        \n",
    "        gcs_path = f\"{prefix}{relative_path.as_posix()}\"\n",
    "        gcs_url = f\"gs://{self.bucket_name}/{gcs_path}\"\n",
    "        \n",
    "        print(f\"📄 {file_path.name}\")\n",
    "        print(f\"   Local:  {relative_path}\")\n",
    "        print(f\"   GCS:    {gcs_url}\")\n",
    "        \n",
    "        if dry_run:\n",
    "            print(\"   → DRY RUN: Would upload with Cache-Control headers\")\n",
    "            results[\"skipped\"].append(str(file_path))\n",
    "            return\n",
    "        \n",
    "        # Try gsutil first, fall back to native client\n",
    "        success = False\n",
    "        \n",
    "        if self.use_gsutil:\n",
    "            success = self._upload_with_gsutil(file_path, gcs_url)\n",
    "        \n",
    "        if not success and self.client:\n",
    "            success = self._upload_with_client(file_path, gcs_path)\n",
    "        \n",
    "        if success:\n",
    "            print(\"   ✅ Upload successful with Cache-Control headers\")\n",
    "            results[\"uploaded\"].append(str(file_path))\n",
    "        else:\n",
    "            print(\"   ❌ Upload failed\")\n",
    "            results[\"failed\"].append({\n",
    "                \"file\": str(file_path),\n",
    "                \"error\": \"All upload methods failed\"\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def _upload_with_gsutil(self, file_path: Path, gcs_url: str) -> bool:\n",
    "        \"\"\"Upload using gsutil command with Cache-Control headers.\"\"\"\n",
    "        try:\n",
    "            # Set Cache-Control header to prevent browser caching\n",
    "            # This forces browsers to re-fetch the STAC catalog files on each request\n",
    "            cmd = [\n",
    "                \"gsutil\", \n",
    "                \"-h\", \"Cache-Control:no-cache, no-store, must-revalidate\",\n",
    "                \"-h\", \"Content-Type:application/json\",\n",
    "                \"cp\", \n",
    "                str(file_path), \n",
    "                gcs_url\n",
    "            ]\n",
    "            result = subprocess.run(\n",
    "                cmd, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                check=True,\n",
    "                timeout=30\n",
    "            )\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"   gsutil error: {e.stderr.strip()}\")\n",
    "            return False\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   gsutil timeout\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"   gsutil exception: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _upload_with_client(self, file_path: Path, gcs_path: str) -> bool:\n",
    "        \"\"\"Upload using native GCS client with Cache-Control headers.\"\"\"\n",
    "        try:\n",
    "            blob = self.bucket.blob(gcs_path)\n",
    "            \n",
    "            # Set Cache-Control header to prevent browser caching\n",
    "            blob.cache_control = \"no-cache, no-store, must-revalidate\"\n",
    "            blob.content_type = \"application/json\"\n",
    "            \n",
    "            blob.upload_from_filename(str(file_path))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   GCS client error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _print_summary(self, results: Dict):\n",
    "        \"\"\"Print upload summary.\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"📊 UPLOAD SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total files found: {results['total_files']}\")\n",
    "        print(f\"Successfully uploaded: {len(results['uploaded'])}\")\n",
    "        print(f\"Skipped (dry run): {len(results['skipped'])}\")\n",
    "        print(f\"Failed: {len(results['failed'])}\")\n",
    "        \n",
    "        if results['failed']:\n",
    "            print(f\"\\n❌ Failed uploads:\")\n",
    "            for failure in results['failed']:\n",
    "                if isinstance(failure, dict):\n",
    "                    print(f\"   • {failure['file']}: {failure['error']}\")\n",
    "                else:\n",
    "                    print(f\"   • {failure}\")\n",
    "        \n",
    "        if results['uploaded']:\n",
    "            print(f\"\\n✅ Upload complete! Files available at:\")\n",
    "            print(f\"   gs://{self.bucket_name}/\")\n",
    "            print(f\"\\n🔄 Cache-Control headers set to 'no-cache, no-store, must-revalidate'\")\n",
    "            print(f\"   This ensures STAC Browser always fetches the latest catalog data\")\n",
    "\n",
    "\n",
    "def upload_stac_catalog(root_dir: str, bucket_name: str, prefix: str = \"\", \n",
    "                       dry_run: bool = False, project_id: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Convenience function to upload STAC catalog files to GCS with cache-busting headers.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Local directory containing STAC files (should point to the catalog folder)\n",
    "        bucket_name: GCS bucket name\n",
    "        prefix: GCS path prefix (e.g., \"public/layers/\")\n",
    "        dry_run: If True, show what would be uploaded without doing it\n",
    "        project_id: GCP project ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with upload results\n",
    "    \"\"\"\n",
    "    uploader = GCSUploader(bucket_name, project_id)\n",
    "    return uploader.upload_directory(root_dir, prefix, dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "main",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: swhm_data\n",
      "Starting bucket crawl...\n",
      "Crawling bucket 'swhm_data' with prefix 'public/layers/'...\n",
      "Found raster: Age_of_Imperviousness\n",
      "Found raster: Flow_Duration_Index\n",
      "Found raster: HSPF_Land_Cover_Type\n",
      "Found raster: Hydrologic_Response_Units\n",
      "Found raster: Imperviousness\n",
      "Found raster: Land_Cover\n",
      "Found raster: Land_Use\n",
      "Found raster: Population_Density\n",
      "Found raster: Precipitation_mm\n",
      "Found raster: Runoff_mm\n",
      "Found raster: Slope\n",
      "Found raster: Slope_Categories\n",
      "Found raster: Soils\n",
      "Found raster: Total_Copper_Concentration\n",
      "Found raster: Total_Kjeldahl_Nitrogen_Concentration\n",
      "Found raster: Total_Phosphorus_Concentration\n",
      "Found raster: Total_Suspended_Solids_Concentration\n",
      "Found raster: Total_Zinc_Concentration\n",
      "Found raster: Traffic\n",
      "Found raster: copper_concentration_ug_per_L\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: cig_grid_wgs\n",
      "Processed 23 objects from bucket\n",
      "Found 2 unique vectors and 20 unique rasters\n",
      "Found 2 vectors and 20 rasters\n",
      "Generating STAC items...\n",
      "\n",
      "Saving catalog files...\n",
      "Saved: catalog/catalog.json\n",
      "Saved: catalog/vector/collection.json\n",
      "Saved: catalog/raster/collection.json\n",
      "Saving individual STAC items...\n",
      "Saved: catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "Saved: catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "Saved: catalog/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "Saved: catalog/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "Saved: catalog/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "Saved: catalog/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "Saved: catalog/raster/Imperviousness/Imperviousness.json\n",
      "Saved: catalog/raster/Land_Cover/Land_Cover.json\n",
      "Saved: catalog/raster/Land_Use/Land_Use.json\n",
      "Saved: catalog/raster/Population_Density/Population_Density.json\n",
      "Saved: catalog/raster/Precipitation_mm/Precipitation_mm.json\n",
      "Saved: catalog/raster/Runoff_mm/Runoff_mm.json\n",
      "Saved: catalog/raster/Slope/Slope.json\n",
      "Saved: catalog/raster/Slope_Categories/Slope_Categories.json\n",
      "Saved: catalog/raster/Soils/Soils.json\n",
      "Saved: catalog/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "Saved: catalog/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "Saved: catalog/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "Saved: catalog/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "Saved: catalog/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "Saved: catalog/raster/Traffic/Traffic.json\n",
      "Saved: catalog/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "Saved: catalog/crawl_summary.json\n",
      "\n",
      "✅ Catalog generation complete!\n",
      "   - Master catalog: catalog/catalog.json\n",
      "   - Vector collection: catalog/vector/collection.json\n",
      "   - Raster collection: catalog/raster/collection.json\n",
      "   - Vector items: 2 items in their respective directories\n",
      "   - Raster items: 20 items in their respective directories\n",
      "   - Crawl summary: catalog/crawl_summary.json\n",
      "\n",
      "📁 Generated directory structure:\n",
      "   catalog/\n",
      "   ├── catalog.json\n",
      "   ├── crawl_summary.json\n",
      "   ├── vector/\n",
      "   │   ├── collection.json\n",
      "   │   ├── ItemName1/\n",
      "   │   │   └── ItemName1.json\n",
      "   │   └── ItemName2/\n",
      "   │       └── ItemName2.json\n",
      "   └── raster/\n",
      "       ├── collection.json\n",
      "       ├── ItemName1/\n",
      "       │   └── ItemName1.json\n",
      "       └── ItemName2/\n",
      "           └── ItemName2.json\n",
      "\n",
      "💡 If you encountered authentication errors:\n",
      "   1. Install: pip install google-cloud-storage\n",
      "   2. Set up authentication:\n",
      "      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\n",
      "      - Or use: gcloud auth application-default login\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to crawl bucket and generate catalog with fixed structure.\"\"\"\n",
    "    # Configuration - update these values for your specific bucket\n",
    "    bucket_name = \"swhm_data\"  # Just the bucket name, not the full URL\n",
    "    prefix = \"public/layers/\"  # Path prefix within the bucket\n",
    "    project_id = None  # Set your GCP project ID if needed\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = GCPBucketCrawler(bucket_name, prefix, project_id)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    print(\"Starting bucket crawl...\")\n",
    "    crawl_data = crawler.crawl_bucket()\n",
    "    \n",
    "    print(f\"Found {len(crawl_data['vectors'])} vectors and {len(crawl_data['rasters'])} rasters\")\n",
    "    \n",
    "    # Generate collections and catalog\n",
    "    generator = CatalogGenerator(crawl_data)\n",
    "    \n",
    "    # Generate all STAC items\n",
    "    print(\"Generating STAC items...\")\n",
    "    stac_items = generator.generate_all_stac_items()\n",
    "    \n",
    "    # Generate collections\n",
    "    vector_collection = generator.generate_vector_collection()\n",
    "    raster_collection = generator.generate_raster_collection()\n",
    "    \n",
    "    # Generate master catalog\n",
    "    master_catalog = generator.generate_master_catalog(vector_collection, raster_collection)\n",
    "    \n",
    "    # Save all files with corrected structure\n",
    "    print(\"\\nSaving catalog files...\")\n",
    "    \n",
    "    # Save master catalog\n",
    "    save_json(master_catalog, \"catalog/catalog.json\")\n",
    "    \n",
    "    # Save collections (singular names)\n",
    "    save_json(vector_collection, \"catalog/vector/collection.json\")\n",
    "    save_json(raster_collection, \"catalog/raster/collection.json\")\n",
    "    \n",
    "    # Save individual STAC items in same directories as source data\n",
    "    print(\"Saving individual STAC items...\")\n",
    "    \n",
    "    # Save vector items in their respective directories\n",
    "    for item in stac_items['vector_items']:\n",
    "        # Find the corresponding vector data to get the directory\n",
    "        vector_data = next((v for v in crawl_data['vectors'] if v['name'] == item['id']), None)\n",
    "        if vector_data:\n",
    "            item_path = f\"catalog/{vector_data['stac_dir']}/{item['id']}.json\"\n",
    "            save_json(item, item_path)\n",
    "    \n",
    "    # Save raster items in their respective directories\n",
    "    for item in stac_items['raster_items']:\n",
    "        # Find the corresponding raster data to get the directory\n",
    "        raster_data = next((r for r in crawl_data['rasters'] if r['name'] == item['id']), None)\n",
    "        if raster_data:\n",
    "            item_path = f\"catalog/{raster_data['stac_dir']}/{item['id']}.json\"\n",
    "            save_json(item, item_path)\n",
    "    \n",
    "    # Save summary with enhanced metadata\n",
    "    summary = {\n",
    "        \"crawl_summary\": {\n",
    "            \"bucket_name\": bucket_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"crawl_time\": datetime.now().isoformat(),\n",
    "            \"total_items\": crawl_data['total_items'],\n",
    "            \"vectors_found\": len(crawl_data['vectors']),\n",
    "            \"rasters_found\": len(crawl_data['rasters']),\n",
    "            \"stac_items_generated\": len(stac_items['vector_items']) + len(stac_items['raster_items'])\n",
    "        },\n",
    "        \"discovered_vectors\": crawl_data['vectors'],\n",
    "        \"discovered_rasters\": crawl_data['rasters'],\n",
    "        \"stac_structure\": {\n",
    "            \"catalog\": \"catalog/catalog.json\",\n",
    "            \"vector_collection\": \"catalog/vector/collection.json\",\n",
    "            \"raster_collection\": \"catalog/raster/collection.json\",\n",
    "            \"vector_items\": [f\"catalog/{v['stac_dir']}/{v['name']}.json\" for v in crawl_data['vectors']],\n",
    "            \"raster_items\": [f\"catalog/{r['stac_dir']}/{r['name']}.json\" for r in crawl_data['rasters']]\n",
    "        }\n",
    "    }\n",
    "    save_json(summary, \"catalog/crawl_summary.json\")\n",
    "    \n",
    "    print(f\"\\n✅ Catalog generation complete!\")\n",
    "    print(f\"   - Master catalog: catalog/catalog.json\")\n",
    "    print(f\"   - Vector collection: catalog/vector/collection.json\")\n",
    "    print(f\"   - Raster collection: catalog/raster/collection.json\")\n",
    "    print(f\"   - Vector items: {len(stac_items['vector_items'])} items in their respective directories\")\n",
    "    print(f\"   - Raster items: {len(stac_items['raster_items'])} items in their respective directories\")\n",
    "    print(f\"   - Crawl summary: catalog/crawl_summary.json\")\n",
    "    \n",
    "    # Print directory structure\n",
    "    print(f\"\\n📁 Generated directory structure:\")\n",
    "    print(f\"   catalog/\")\n",
    "    print(f\"   ├── catalog.json\")\n",
    "    print(f\"   ├── crawl_summary.json\")\n",
    "    print(f\"   ├── vector/\")\n",
    "    print(f\"   │   ├── collection.json\")\n",
    "    print(f\"   │   ├── ItemName1/\")\n",
    "    print(f\"   │   │   └── ItemName1.json\")\n",
    "    print(f\"   │   └── ItemName2/\")\n",
    "    print(f\"   │       └── ItemName2.json\")\n",
    "    print(f\"   └── raster/\")\n",
    "    print(f\"       ├── collection.json\")\n",
    "    print(f\"       ├── ItemName1/\")\n",
    "    print(f\"       │   └── ItemName1.json\")\n",
    "    print(f\"       └── ItemName2/\")\n",
    "    print(f\"           └── ItemName2.json\")\n",
    "    \n",
    "    # Print authentication help if needed\n",
    "    print(f\"\\n💡 If you encountered authentication errors:\")\n",
    "    print(f\"   1. Install: pip install google-cloud-storage\")\n",
    "    print(f\"   2. Set up authentication:\")\n",
    "    print(f\"      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\")\n",
    "    print(f\"      - Or use: gcloud auth application-default login\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "upload_example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPLOADING STAC CATALOG TO GCS ===\n",
      "\n",
      "1. Dry run to preview uploads:\n",
      "Using gsutil for uploads\n",
      "Scanning directory: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/scripts/ipynb/catalog\n",
      "Target GCS location: gs://swhm_data/public/layers/\n",
      "Dry run: True\n",
      "--------------------------------------------------\n",
      "Found 26 JSON files to upload\n",
      "--------------------------------------------------\n",
      "📄 crawl_summary.json\n",
      "   Local:  crawl_summary.json\n",
      "   GCS:    gs://swhm_data/public/layers/crawl_summary.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 catalog.json\n",
      "   Local:  catalog.json\n",
      "   GCS:    gs://swhm_data/public/layers/catalog.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 collection.json\n",
      "   Local:  raster/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/collection.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 collection.json\n",
      "   Local:  vector/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/collection.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 cig_grid_wgs.json\n",
      "   Local:  vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 PugetSoundWA.json\n",
      "   Local:  vector/PugetSoundWA/PugetSoundWA.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Traffic.json\n",
      "   Local:  raster/Traffic/Traffic.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Traffic/Traffic.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Slope.json\n",
      "   Local:  raster/Slope/Slope.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope/Slope.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Land_Use.json\n",
      "   Local:  raster/Land_Use/Land_Use.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Use/Land_Use.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Total_Suspended_Solids_Concentration.json\n",
      "   Local:  raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Land_Cover.json\n",
      "   Local:  raster/Land_Cover/Land_Cover.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Cover/Land_Cover.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Population_Density.json\n",
      "   Local:  raster/Population_Density/Population_Density.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Population_Density/Population_Density.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Total_Phosphorus_Concentration.json\n",
      "   Local:  raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Imperviousness.json\n",
      "   Local:  raster/Imperviousness/Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Imperviousness/Imperviousness.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 copper_concentration_ug_per_L.json\n",
      "   Local:  raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Age_of_Imperviousness.json\n",
      "   Local:  raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Flow_Duration_Index.json\n",
      "   Local:  raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Slope_Categories.json\n",
      "   Local:  raster/Slope_Categories/Slope_Categories.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope_Categories/Slope_Categories.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 HSPF_Land_Cover_Type.json\n",
      "   Local:  raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Hydrologic_Response_Units.json\n",
      "   Local:  raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Precipitation_mm.json\n",
      "   Local:  raster/Precipitation_mm/Precipitation_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Precipitation_mm/Precipitation_mm.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Total_Copper_Concentration.json\n",
      "   Local:  raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Soils.json\n",
      "   Local:  raster/Soils/Soils.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Soils/Soils.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Runoff_mm.json\n",
      "   Local:  raster/Runoff_mm/Runoff_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Runoff_mm/Runoff_mm.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   Local:  raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "📄 Total_Zinc_Concentration.json\n",
      "   Local:  raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   → DRY RUN: Would upload with Cache-Control headers\n",
      "==================================================\n",
      "📊 UPLOAD SUMMARY\n",
      "==================================================\n",
      "Total files found: 26\n",
      "Successfully uploaded: 0\n",
      "Skipped (dry run): 26\n",
      "Failed: 0\n",
      "\n",
      "2. Actual upload:\n",
      "Using gsutil for uploads\n",
      "Scanning directory: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/scripts/ipynb/catalog\n",
      "Target GCS location: gs://swhm_data/public/layers/\n",
      "Dry run: False\n",
      "--------------------------------------------------\n",
      "Found 26 JSON files to upload\n",
      "--------------------------------------------------\n",
      "📄 crawl_summary.json\n",
      "   Local:  crawl_summary.json\n",
      "   GCS:    gs://swhm_data/public/layers/crawl_summary.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 catalog.json\n",
      "   Local:  catalog.json\n",
      "   GCS:    gs://swhm_data/public/layers/catalog.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 collection.json\n",
      "   Local:  raster/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/collection.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 collection.json\n",
      "   Local:  vector/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/collection.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 cig_grid_wgs.json\n",
      "   Local:  vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 PugetSoundWA.json\n",
      "   Local:  vector/PugetSoundWA/PugetSoundWA.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Traffic.json\n",
      "   Local:  raster/Traffic/Traffic.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Traffic/Traffic.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Slope.json\n",
      "   Local:  raster/Slope/Slope.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope/Slope.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Land_Use.json\n",
      "   Local:  raster/Land_Use/Land_Use.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Use/Land_Use.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Total_Suspended_Solids_Concentration.json\n",
      "   Local:  raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Land_Cover.json\n",
      "   Local:  raster/Land_Cover/Land_Cover.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Cover/Land_Cover.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Population_Density.json\n",
      "   Local:  raster/Population_Density/Population_Density.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Population_Density/Population_Density.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Total_Phosphorus_Concentration.json\n",
      "   Local:  raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Imperviousness.json\n",
      "   Local:  raster/Imperviousness/Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Imperviousness/Imperviousness.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 copper_concentration_ug_per_L.json\n",
      "   Local:  raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Age_of_Imperviousness.json\n",
      "   Local:  raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Flow_Duration_Index.json\n",
      "   Local:  raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Slope_Categories.json\n",
      "   Local:  raster/Slope_Categories/Slope_Categories.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope_Categories/Slope_Categories.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 HSPF_Land_Cover_Type.json\n",
      "   Local:  raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Hydrologic_Response_Units.json\n",
      "   Local:  raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Precipitation_mm.json\n",
      "   Local:  raster/Precipitation_mm/Precipitation_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Precipitation_mm/Precipitation_mm.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Total_Copper_Concentration.json\n",
      "   Local:  raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Soils.json\n",
      "   Local:  raster/Soils/Soils.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Soils/Soils.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Runoff_mm.json\n",
      "   Local:  raster/Runoff_mm/Runoff_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Runoff_mm/Runoff_mm.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   Local:  raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "📄 Total_Zinc_Concentration.json\n",
      "   Local:  raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   ✅ Upload successful with Cache-Control headers\n",
      "\n",
      "==================================================\n",
      "📊 UPLOAD SUMMARY\n",
      "==================================================\n",
      "Total files found: 26\n",
      "Successfully uploaded: 26\n",
      "Skipped (dry run): 0\n",
      "Failed: 0\n",
      "\n",
      "✅ Upload complete! Files available at:\n",
      "   gs://swhm_data/\n",
      "\n",
      "🔄 Cache-Control headers set to 'no-cache, no-store, must-revalidate'\n",
      "   This ensures STAC Browser always fetches the latest catalog data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "GCS_BUCKET = \"swhm_data\"\n",
    "GCS_PREFIX = \"public/layers/\"\n",
    "\n",
    "# IMPORTANT: Point directly to the catalog directory, not a parent directory\n",
    "# This ensures relative paths are calculated correctly\n",
    "CATALOG_DIR = \"catalog\"  # Use relative path from notebook location, or absolute path to catalog folder\n",
    "\n",
    "# Example usage with new refactored uploader\n",
    "print(\"=== UPLOADING STAC CATALOG TO GCS ===\\n\")\n",
    "\n",
    "# Upload with dry run first to see what would be uploaded\n",
    "print(\"1. Dry run to preview uploads:\")\n",
    "results = upload_stac_catalog(\n",
    "    root_dir=CATALOG_DIR,\n",
    "    bucket_name=GCS_BUCKET,\n",
    "    prefix=GCS_PREFIX,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Actual upload:\")\n",
    "# Uncomment the line below to perform actual upload\n",
    "results = upload_stac_catalog(\n",
    "    root_dir=CATALOG_DIR,\n",
    "    bucket_name=GCS_BUCKET,\n",
    "    prefix=GCS_PREFIX,\n",
    "    dry_run=False\n",
    ")\n",
    "\n",
    "# Alternative: Use absolute path to catalog directory\n",
    "# CATALOG_DIR_ABSOLUTE = \"/Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
