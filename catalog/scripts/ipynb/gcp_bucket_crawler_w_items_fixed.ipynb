{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6a8b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GCP Bucket Crawler and Catalog Generator\n",
    "Crawls a GCP storage bucket to discover vector and raster data,\n",
    "then generates collections, individual STAC items, and a comprehensive catalog.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import shutil\n",
    "import shlex\n",
    "import subprocess\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6f28e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPBucketCrawler:\n",
    "    def __init__(self, bucket_name: str, prefix: str = \"\", project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler with GCP bucket details.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the GCP storage bucket (e.g., 'swhm_data')\n",
    "            prefix: Prefix to filter objects (e.g., 'public/layers/')\n",
    "            project_id: GCP project ID (optional, will use default if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.project_id = project_id\n",
    "        self.vectors = []\n",
    "        self.rasters = []\n",
    "        self.processed_items = set()  # Track processed items to prevent duplicates\n",
    "        \n",
    "        # Initialize the GCS client\n",
    "        try:\n",
    "            if project_id:\n",
    "                self.client = storage.Client(project=project_id)\n",
    "            else:\n",
    "                self.client = storage.Client()\n",
    "            self.bucket = self.client.bucket(bucket_name)\n",
    "            print(f\"Successfully connected to bucket: {bucket_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing GCS client: {e}\")\n",
    "            print(\"Make sure you have proper authentication set up:\")\n",
    "            print(\"1. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
    "            print(\"2. Or run 'gcloud auth application-default login'\")\n",
    "            self.client = None\n",
    "            self.bucket = None\n",
    "        \n",
    "    def crawl_bucket(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl the GCP bucket to discover all vectors and rasters.\n",
    "        Returns a dictionary with discovered items.\n",
    "        \"\"\"\n",
    "        if not self.client or not self.bucket:\n",
    "            print(\"No valid GCS client available, creating sample data...\")\n",
    "            return self._create_sample_data()\n",
    "            \n",
    "        print(f\"Crawling bucket '{self.bucket_name}' with prefix '{self.prefix}'...\")\n",
    "        \n",
    "        try:\n",
    "            # List all blobs in the bucket with the specified prefix\n",
    "            blobs = self.bucket.list_blobs(prefix=self.prefix)\n",
    "            \n",
    "            blob_count = 0\n",
    "            for blob in blobs:\n",
    "                blob_count += 1\n",
    "                self._process_blob(blob)\n",
    "                \n",
    "            print(f\"Processed {blob_count} objects from bucket\")\n",
    "            print(f\"Found {len(self.vectors)} unique vectors and {len(self.rasters)} unique rasters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling bucket: {e}\")\n",
    "            return self._create_sample_data()\n",
    "            \n",
    "        return {\n",
    "            'vectors': self.vectors,\n",
    "            'rasters': self.rasters,\n",
    "            'total_items': len(self.vectors) + len(self.rasters)\n",
    "        }\n",
    "    \n",
    "    def _process_blob(self, blob):\n",
    "        \"\"\"Process a single blob to determine if it's a vector or raster.\"\"\"\n",
    "        blob_name = blob.name\n",
    "        blob_path = Path(blob_name)\n",
    "        \n",
    "        # Skip directories (blobs ending with '/')\n",
    "        if blob_name.endswith('/'):\n",
    "            return\n",
    "            \n",
    "        # Check for vector files - ONLY .geojson files, NOT .json files\n",
    "        if 'vector/' in blob_name and blob_path.suffix.lower() == '.geojson':\n",
    "            self._add_vector_item(blob)\n",
    "            \n",
    "        # Check for raster files - TIFF files including .gtiff  \n",
    "        elif 'raster/' in blob_name and blob_path.suffix.lower() in ['.tiff', '.tif', '.gtiff']:\n",
    "            self._add_raster_item(blob)\n",
    "    \n",
    "    def _add_vector_item(self, blob):\n",
    "        \"\"\"Add a vector item to the collection.\"\"\"\n",
    "        blob_path = Path(blob.name)\n",
    "        item_name = blob_path.stem\n",
    "        \n",
    "        # Create unique identifier to prevent duplicates\n",
    "        item_key = f\"vector:{item_name}\"\n",
    "        if item_key in self.processed_items:\n",
    "            print(f\"Skipping duplicate vector: {item_name}\")\n",
    "            return\n",
    "        \n",
    "        # Create public URL\n",
    "        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "        \n",
    "        # Get the directory path where the STAC item should be saved\n",
    "        # e.g., public/layers/vector/PugetSoundWA/PugetSoundWA.geojson -> vector/PugetSoundWA/\n",
    "        parent_dir = blob_path.parent\n",
    "        stac_dir = str(parent_dir).replace(self.prefix, '')\n",
    "        \n",
    "        vector_item = {\n",
    "            'name': item_name,\n",
    "            'filename': blob.name,\n",
    "            'url': public_url,\n",
    "            'type': 'vector',\n",
    "            'format': 'GeoJSON',\n",
    "            'size_bytes': blob.size,\n",
    "            'content_type': blob.content_type,\n",
    "            'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "            'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "            'discovered_at': datetime.now().isoformat(),\n",
    "            'etag': blob.etag,\n",
    "            'md5_hash': blob.md5_hash,\n",
    "            'stac_dir': stac_dir  # Directory where STAC item should be saved\n",
    "        }\n",
    "        \n",
    "        self.vectors.append(vector_item)\n",
    "        self.processed_items.add(item_key)\n",
    "        print(f\"Found vector: {item_name}\")\n",
    "    \n",
    "    def _add_raster_item(self, blob):\n",
    "        \"\"\"Add a raster item to the collection.\"\"\"\n",
    "        blob_path = Path(blob.name)\n",
    "        item_name = blob_path.stem\n",
    "        \n",
    "        # Create unique identifier to prevent duplicates\n",
    "        item_key = f\"raster:{item_name}\"\n",
    "        if item_key in self.processed_items:\n",
    "            print(f\"Skipping duplicate raster: {item_name}\")\n",
    "            return\n",
    "        \n",
    "        # Create public URL\n",
    "        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "        \n",
    "        # Get the directory path where the STAC item should be saved\n",
    "        # e.g., public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.tif -> raster/Age_of_Imperviousness/\n",
    "        parent_dir = blob_path.parent\n",
    "        stac_dir = str(parent_dir).replace(self.prefix, '')\n",
    "        \n",
    "        raster_item = {\n",
    "            'name': item_name,\n",
    "            'filename': blob.name,\n",
    "            'url': public_url,\n",
    "            'type': 'raster',\n",
    "            'format': 'GeoTIFF',\n",
    "            'size_bytes': blob.size,\n",
    "            'content_type': blob.content_type,\n",
    "            'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "            'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "            'discovered_at': datetime.now().isoformat(),\n",
    "            'etag': blob.etag,\n",
    "            'md5_hash': blob.md5_hash,\n",
    "            'stac_dir': stac_dir  # Directory where STAC item should be saved\n",
    "        }\n",
    "        \n",
    "        self.rasters.append(raster_item)\n",
    "        self.processed_items.add(item_key)\n",
    "        print(f\"Found raster: {item_name}\")\n",
    "    \n",
    "    def get_blob_info(self, blob_name: str) -> Optional[Dict]:\n",
    "        \"\"\"Get detailed information about a specific blob.\"\"\"\n",
    "        if not self.bucket:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            blob = self.bucket.blob(blob_name)\n",
    "            if blob.exists():\n",
    "                return {\n",
    "                    'name': blob.name,\n",
    "                    'size': blob.size,\n",
    "                    'content_type': blob.content_type,\n",
    "                    'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "                    'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "                    'etag': blob.etag,\n",
    "                    'md5_hash': blob.md5_hash,\n",
    "                    'public_url': f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting blob info for {blob_name}: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def _create_sample_data(self):\n",
    "        \"\"\"Create sample data structure when bucket can't be crawled directly.\"\"\"\n",
    "        print(\"Creating sample data structure...\")\n",
    "        \n",
    "        base_url = f\"https://storage.googleapis.com/{self.bucket_name}\"\n",
    "        \n",
    "        # Sample vectors based on your structure\n",
    "        sample_vectors = [\n",
    "            {\n",
    "                'name': 'vector1',\n",
    "                'filename': f'{self.prefix}vector/vector1/vector1.geojson',\n",
    "                'url': f\"{base_url}/{self.prefix}vector/vector1/vector1.geojson\",\n",
    "                'type': 'vector',\n",
    "                'format': 'GeoJSON',\n",
    "                'size_bytes': None,\n",
    "                'content_type': 'application/geo+json',\n",
    "                'created': None,\n",
    "                'updated': None,\n",
    "                'discovered_at': datetime.now().isoformat(),\n",
    "                'etag': None,\n",
    "                'md5_hash': None,\n",
    "                'stac_dir': 'vector/vector1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Sample rasters based on your structure\n",
    "        sample_rasters = [\n",
    "            {\n",
    "                'name': 'raster1',\n",
    "                'filename': f'{self.prefix}raster/raster1/raster1.tiff',\n",
    "                'url': f\"{base_url}/{self.prefix}raster/raster1/raster1.tiff\",\n",
    "                'type': 'raster',\n",
    "                'format': 'GeoTIFF',\n",
    "                'size_bytes': None,\n",
    "                'content_type': 'image/tiff',\n",
    "                'created': None,\n",
    "                'updated': None,\n",
    "                'discovered_at': datetime.now().isoformat(),\n",
    "                'etag': None,\n",
    "                'md5_hash': None,\n",
    "                'stac_dir': 'raster/raster1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.vectors = sample_vectors\n",
    "        self.rasters = sample_rasters\n",
    "        \n",
    "        return {\n",
    "            'vectors': self.vectors,\n",
    "            'rasters': self.rasters,\n",
    "            'total_items': len(self.vectors) + len(self.rasters)\n",
    "        }\n",
    "\n",
    "class CatalogGenerator:\n",
    "    def __init__(self, crawler_data: Dict):\n",
    "        \"\"\"Initialize with data from the crawler.\"\"\"\n",
    "        self.data = crawler_data\n",
    "        self.stac_items = []\n",
    "        \n",
    "    def generate_stac_item(self, item_data: Dict, item_type: str) -> Dict:\n",
    "        \"\"\"Generate a STAC item for vector or raster data.\"\"\"\n",
    "        item_id = item_data['name']\n",
    "        \n",
    "        # Base STAC item structure\n",
    "        stac_item = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": item_id,\n",
    "            \"properties\": {\n",
    "                \"title\": item_data['name'].replace('_', ' ').title(),\n",
    "                \"description\": f\"{item_type.title()} dataset: {item_data['name']}\",\n",
    "                \"datetime\": item_data['discovered_at'],\n",
    "                \"created\": item_data.get('created') or item_data['discovered_at'],\n",
    "                \"updated\": item_data.get('updated') or item_data['discovered_at'],\n",
    "                \"providers\": [\n",
    "                    {\n",
    "                        \"name\": \"SWHM Data\",\n",
    "                        \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                        \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"geometry\": None,  # Would need to extract from actual data\n",
    "            \"bbox\": None,  # Would need to calculate from geometry/bounds\n",
    "            \"assets\": {},\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{item_data['stac_dir']}/{item_id}.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{item_type}/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"collection\",\n",
    "                    \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{item_type}/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add assets based on type\n",
    "        if item_type == 'vector':\n",
    "            stac_item[\"assets\"][\"data\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"application/geo+json\",\n",
    "                \"title\": \"GeoJSON data\",\n",
    "                \"description\": \"Vector data in GeoJSON format\",\n",
    "                \"roles\": [\"data\"],\n",
    "                \"file:size\": item_data.get('size_bytes'),\n",
    "                \"file:checksum\": item_data.get('md5_hash')\n",
    "            }\n",
    "        elif item_type == 'raster':\n",
    "            stac_item[\"assets\"][\"data\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"image/tiff; application=geotiff\",\n",
    "                \"title\": \"GeoTIFF data\",\n",
    "                \"description\": \"Raster data in GeoTIFF format\",\n",
    "                \"roles\": [\"data\"],\n",
    "                \"file:size\": item_data.get('size_bytes'),\n",
    "                \"file:checksum\": item_data.get('md5_hash')\n",
    "            }\n",
    "            \n",
    "            # Add COG asset if it's a Cloud Optimized GeoTIFF\n",
    "            stac_item[\"assets\"][\"cog\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                \"title\": \"Cloud Optimized GeoTIFF\",\n",
    "                \"description\": \"Cloud Optimized GeoTIFF for web access\",\n",
    "                \"roles\": [\"data\", \"overview\"]\n",
    "            }\n",
    "        \n",
    "        # Only add metadata and thumbnail assets if they exist (no placeholders)\n",
    "        # This prevents STAC Browser from trying to load non-existent resources\n",
    "        \n",
    "        return stac_item\n",
    "    \n",
    "    def generate_vector_collection(self) -> Dict:\n",
    "        \"\"\"Generate a vector collection with individual STAC items.\"\"\"\n",
    "        collection = {\n",
    "            \"type\": \"Collection\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-vector\",\n",
    "            \"title\": \"SWHM Vector Collection\",\n",
    "            \"description\": \"Collection of vector datasets from SWHM data bucket\",\n",
    "            \"keywords\": [\"vector\", \"geojson\", \"swhm\"],\n",
    "            \"license\": \"proprietary\",\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\n",
    "                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n",
    "                },\n",
    "                \"temporal\": {\n",
    "                    \"interval\": [[None, None]]\n",
    "                }\n",
    "            },\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/vector/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ],\n",
    "            \"item_assets\": {\n",
    "                \"data\": {\n",
    "                    \"type\": \"application/geo+json\",\n",
    "                    \"title\": \"GeoJSON data\",\n",
    "                    \"roles\": [\"data\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add links to individual items - crawler already handles deduplication\n",
    "        for vector in self.data['vectors']:\n",
    "            item_id = vector['name']\n",
    "            collection[\"links\"].append({\n",
    "                \"rel\": \"item\",\n",
    "                \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{vector['stac_dir']}/{item_id}.json\",\n",
    "                \"type\": \"application/json\",\n",
    "                \"title\": vector['name'].replace('_', ' ').title()\n",
    "            })\n",
    "            \n",
    "        return collection\n",
    "    \n",
    "    def generate_raster_collection(self) -> Dict:\n",
    "        \"\"\"Generate a raster collection with individual STAC items.\"\"\"\n",
    "        collection = {\n",
    "            \"type\": \"Collection\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-raster\",\n",
    "            \"title\": \"SWHM Raster Collection\",\n",
    "            \"description\": \"Collection of raster datasets from SWHM data bucket\",\n",
    "            \"keywords\": [\"raster\", \"geotiff\", \"swhm\"],\n",
    "            \"license\": \"proprietary\",\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\n",
    "                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n",
    "                },\n",
    "                \"temporal\": {\n",
    "                    \"interval\": [[None, None]]\n",
    "                }\n",
    "            },\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/raster/collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ],\n",
    "            \"item_assets\": {\n",
    "                \"data\": {\n",
    "                    \"type\": \"image/tiff; application=geotiff\",\n",
    "                    \"title\": \"GeoTIFF data\",\n",
    "                    \"roles\": [\"data\"]\n",
    "                },\n",
    "                \"cog\": {\n",
    "                    \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                    \"title\": \"Cloud Optimized GeoTIFF\",\n",
    "                    \"roles\": [\"data\", \"overview\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add links to individual items - crawler already handles deduplication\n",
    "        for raster in self.data['rasters']:\n",
    "            item_id = raster['name']\n",
    "            collection[\"links\"].append({\n",
    "                \"rel\": \"item\",\n",
    "                \"href\": f\"https://storage.googleapis.com/swhm_data/public/layers/{raster['stac_dir']}/{item_id}.json\",\n",
    "                \"type\": \"application/json\",\n",
    "                \"title\": raster['name'].replace('_', ' ').title()\n",
    "            })\n",
    "            \n",
    "        return collection\n",
    "    \n",
    "    def generate_master_catalog(self, vector_collection: Dict, raster_collection: Dict) -> Dict:\n",
    "        \"\"\"Generate the master catalog containing all collections.\"\"\"\n",
    "        catalog = {\n",
    "            \"type\": \"Catalog\",\n",
    "            \"catalog_type\": \"Published_Absolute\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-data-catalog\",\n",
    "            \"title\": \"SWHM Data Catalog\",\n",
    "            \"description\": \"Master catalog for SWHM vector and raster datasets\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"updated\": datetime.now().isoformat(),\n",
    "            \"keywords\": [\"swhm\", \"vector\", \"raster\", \"geospatial\"],\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/catalog.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"SWHM Data Catalog\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"child\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/vector/collection.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"Vector Collection\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"child\",\n",
    "                    \"href\": \"https://storage.googleapis.com/swhm_data/public/layers/raster/collection.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"Raster Collection\"\n",
    "                }\n",
    "            ],\n",
    "            \"conformsTo\": [\n",
    "                \"https://api.stacspec.org/v1.0.0/core\",\n",
    "                \"https://api.stacspec.org/v1.0.0/collections\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return catalog\n",
    "    \n",
    "    def generate_all_stac_items(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Generate all STAC items for vectors and rasters.\"\"\"\n",
    "        vector_items = []\n",
    "        raster_items = []\n",
    "        \n",
    "        # Generate vector items - crawler already handles deduplication\n",
    "        for vector in self.data['vectors']:\n",
    "            stac_item = self.generate_stac_item(vector, 'vector')\n",
    "            vector_items.append(stac_item)\n",
    "            \n",
    "        # Generate raster items - crawler already handles deduplication\n",
    "        for raster in self.data['rasters']:\n",
    "            stac_item = self.generate_stac_item(raster, 'raster')\n",
    "            raster_items.append(stac_item)\n",
    "            \n",
    "        return {\n",
    "            'vector_items': vector_items,\n",
    "            'raster_items': raster_items\n",
    "        }\n",
    "\n",
    "def save_json(data: Dict, filepath: str):\n",
    "    \"\"\"Save data to JSON file with pretty formatting.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c43e66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCSUploader:\n",
    "    \"\"\"\n",
    "    Handles uploading STAC catalog files to Google Cloud Storage.\n",
    "    Supports both gsutil and native GCS client approaches.\n",
    "    Sets Cache-Control headers to prevent browser caching of JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the GCS uploader.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the GCS bucket\n",
    "            project_id: GCP project ID (optional)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.project_id = project_id\n",
    "        self.use_gsutil = shutil.which(\"gsutil\") is not None\n",
    "        \n",
    "        # Try to initialize GCS client as fallback\n",
    "        self.client = None\n",
    "        self.bucket = None\n",
    "        if not self.use_gsutil:\n",
    "            try:\n",
    "                if project_id:\n",
    "                    self.client = storage.Client(project=project_id)\n",
    "                else:\n",
    "                    self.client = storage.Client()\n",
    "                self.bucket = self.client.bucket(bucket_name)\n",
    "                print(\"Using native GCS client for uploads\")\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: Could not initialize GCS client: {e}\")\n",
    "                print(\"Upload functionality will be limited\")\n",
    "        else:\n",
    "            print(\"Using gsutil for uploads\")\n",
    "    \n",
    "    def upload_directory(self, root_dir: str, prefix: str = \"\", dry_run: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Upload all STAC JSON files from a directory structure to GCS.\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Local directory containing STAC files\n",
    "            prefix: GCS path prefix (e.g., \"public/layers/\")\n",
    "            dry_run: If True, show what would be uploaded without doing it\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with upload results\n",
    "        \"\"\"\n",
    "        root_path = Path(root_dir).resolve()\n",
    "        \n",
    "        if not root_path.is_dir():\n",
    "            raise ValueError(f\"Directory does not exist: {root_path}\")\n",
    "        \n",
    "        if prefix and not prefix.endswith(\"/\"):\n",
    "            prefix += \"/\"\n",
    "        \n",
    "        print(f\"Scanning directory: {root_path}\")\n",
    "        print(f\"Target GCS location: gs://{self.bucket_name}/{prefix}\")\n",
    "        print(f\"Dry run: {dry_run}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = {\n",
    "            \"uploaded\": [],\n",
    "            \"skipped\": [],\n",
    "            \"failed\": [],\n",
    "            \"total_files\": 0\n",
    "        }\n",
    "        \n",
    "        # Find all JSON files to upload\n",
    "        json_files = list(root_path.rglob(\"*.json\"))\n",
    "        results[\"total_files\"] = len(json_files)\n",
    "        \n",
    "        if not json_files:\n",
    "            print(\"No JSON files found to upload\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files to upload\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                self._upload_single_file(json_file, root_path, prefix, dry_run, results)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Failed to upload {json_file}: {e}\")\n",
    "                results[\"failed\"].append({\n",
    "                    \"file\": str(json_file),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(results)\n",
    "        return results\n",
    "    \n",
    "    def _upload_single_file(self, file_path: Path, root_path: Path, prefix: str, \n",
    "                          dry_run: bool, results: Dict):\n",
    "        \"\"\"Upload a single file to GCS.\"\"\"\n",
    "        # Get relative path from the catalog root, not the full local path\n",
    "        relative_path = file_path.relative_to(root_path)\n",
    "        \n",
    "        # Remove any leading \"catalog/\" from the relative path if it exists\n",
    "        # This ensures we upload to the correct GCS structure\n",
    "        path_parts = relative_path.parts\n",
    "        if path_parts[0] == \"catalog\":\n",
    "            # Strip the \"catalog\" prefix - we want vector/collection.json not catalog/vector/collection.json\n",
    "            relative_path = Path(*path_parts[1:])\n",
    "        \n",
    "        gcs_path = f\"{prefix}{relative_path.as_posix()}\"\n",
    "        gcs_url = f\"gs://{self.bucket_name}/{gcs_path}\"\n",
    "        \n",
    "        print(f\"üìÑ {file_path.name}\")\n",
    "        print(f\"   Local:  {relative_path}\")\n",
    "        print(f\"   GCS:    {gcs_url}\")\n",
    "        \n",
    "        if dry_run:\n",
    "            print(\"   ‚Üí DRY RUN: Would upload with Cache-Control headers\")\n",
    "            results[\"skipped\"].append(str(file_path))\n",
    "            return\n",
    "        \n",
    "        # Try gsutil first, fall back to native client\n",
    "        success = False\n",
    "        \n",
    "        if self.use_gsutil:\n",
    "            success = self._upload_with_gsutil(file_path, gcs_url)\n",
    "        \n",
    "        if not success and self.client:\n",
    "            success = self._upload_with_client(file_path, gcs_path)\n",
    "        \n",
    "        if success:\n",
    "            print(\"   ‚úÖ Upload successful with Cache-Control headers\")\n",
    "            results[\"uploaded\"].append(str(file_path))\n",
    "        else:\n",
    "            print(\"   ‚ùå Upload failed\")\n",
    "            results[\"failed\"].append({\n",
    "                \"file\": str(file_path),\n",
    "                \"error\": \"All upload methods failed\"\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def _upload_with_gsutil(self, file_path: Path, gcs_url: str) -> bool:\n",
    "        \"\"\"Upload using gsutil command with Cache-Control headers.\"\"\"\n",
    "        try:\n",
    "            # Set Cache-Control header to prevent browser caching\n",
    "            # This forces browsers to re-fetch the STAC catalog files on each request\n",
    "            cmd = [\n",
    "                \"gsutil\", \n",
    "                \"-h\", \"Cache-Control:no-cache, no-store, must-revalidate\",\n",
    "                \"-h\", \"Content-Type:application/json\",\n",
    "                \"cp\", \n",
    "                str(file_path), \n",
    "                gcs_url\n",
    "            ]\n",
    "            result = subprocess.run(\n",
    "                cmd, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                check=True,\n",
    "                timeout=30\n",
    "            )\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"   gsutil error: {e.stderr.strip()}\")\n",
    "            return False\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   gsutil timeout\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"   gsutil exception: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _upload_with_client(self, file_path: Path, gcs_path: str) -> bool:\n",
    "        \"\"\"Upload using native GCS client with Cache-Control headers.\"\"\"\n",
    "        try:\n",
    "            blob = self.bucket.blob(gcs_path)\n",
    "            \n",
    "            # Set Cache-Control header to prevent browser caching\n",
    "            blob.cache_control = \"no-cache, no-store, must-revalidate\"\n",
    "            blob.content_type = \"application/json\"\n",
    "            \n",
    "            blob.upload_from_filename(str(file_path))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   GCS client error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _print_summary(self, results: Dict):\n",
    "        \"\"\"Print upload summary.\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üìä UPLOAD SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total files found: {results['total_files']}\")\n",
    "        print(f\"Successfully uploaded: {len(results['uploaded'])}\")\n",
    "        print(f\"Skipped (dry run): {len(results['skipped'])}\")\n",
    "        print(f\"Failed: {len(results['failed'])}\")\n",
    "        \n",
    "        if results['failed']:\n",
    "            print(f\"\\n‚ùå Failed uploads:\")\n",
    "            for failure in results['failed']:\n",
    "                if isinstance(failure, dict):\n",
    "                    print(f\"   ‚Ä¢ {failure['file']}: {failure['error']}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {failure}\")\n",
    "        \n",
    "        if results['uploaded']:\n",
    "            print(f\"\\n‚úÖ Upload complete! Files available at:\")\n",
    "            print(f\"   gs://{self.bucket_name}/\")\n",
    "            print(f\"\\nüîÑ Cache-Control headers set to 'no-cache, no-store, must-revalidate'\")\n",
    "            print(f\"   This ensures STAC Browser always fetches the latest catalog data\")\n",
    "\n",
    "\n",
    "def upload_stac_catalog(root_dir: str, bucket_name: str, prefix: str = \"\", \n",
    "                       dry_run: bool = False, project_id: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Convenience function to upload STAC catalog files to GCS with cache-busting headers.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Local directory containing STAC files (should point to the catalog folder)\n",
    "        bucket_name: GCS bucket name\n",
    "        prefix: GCS path prefix (e.g., \"public/layers/\")\n",
    "        dry_run: If True, show what would be uploaded without doing it\n",
    "        project_id: GCP project ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with upload results\n",
    "    \"\"\"\n",
    "    uploader = GCSUploader(bucket_name, project_id)\n",
    "    return uploader.upload_directory(root_dir, prefix, dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "main",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: swhm_data\n",
      "Starting bucket crawl...\n",
      "Crawling bucket 'swhm_data' with prefix 'public/layers/'...\n",
      "Found raster: Age_of_Imperviousness\n",
      "Found raster: Flow_Duration_Index\n",
      "Found raster: HSPF_Land_Cover_Type\n",
      "Found raster: Hydrologic_Response_Units\n",
      "Found raster: Imperviousness\n",
      "Found raster: Land_Cover\n",
      "Found raster: Land_Use\n",
      "Found raster: Population_Density\n",
      "Found raster: Precipitation_mm\n",
      "Found raster: Runoff_mm\n",
      "Found raster: Slope\n",
      "Found raster: Slope_Categories\n",
      "Found raster: Soils\n",
      "Found raster: Total_Copper_Concentration\n",
      "Found raster: Total_Kjeldahl_Nitrogen_Concentration\n",
      "Found raster: Total_Phosphorus_Concentration\n",
      "Found raster: Total_Suspended_Solids_Concentration\n",
      "Found raster: Total_Zinc_Concentration\n",
      "Found raster: Traffic\n",
      "Found raster: copper_concentration_ug_per_L\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: cig_grid_wgs\n",
      "Processed 23 objects from bucket\n",
      "Found 2 unique vectors and 20 unique rasters\n",
      "Found 2 vectors and 20 rasters\n",
      "Generating STAC items...\n",
      "\n",
      "Saving catalog files...\n",
      "Saved: catalog/catalog.json\n",
      "Saved: catalog/vector/collection.json\n",
      "Saved: catalog/raster/collection.json\n",
      "Saving individual STAC items...\n",
      "Saved: catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "Saved: catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "Saved: catalog/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "Saved: catalog/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "Saved: catalog/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "Saved: catalog/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "Saved: catalog/raster/Imperviousness/Imperviousness.json\n",
      "Saved: catalog/raster/Land_Cover/Land_Cover.json\n",
      "Saved: catalog/raster/Land_Use/Land_Use.json\n",
      "Saved: catalog/raster/Population_Density/Population_Density.json\n",
      "Saved: catalog/raster/Precipitation_mm/Precipitation_mm.json\n",
      "Saved: catalog/raster/Runoff_mm/Runoff_mm.json\n",
      "Saved: catalog/raster/Slope/Slope.json\n",
      "Saved: catalog/raster/Slope_Categories/Slope_Categories.json\n",
      "Saved: catalog/raster/Soils/Soils.json\n",
      "Saved: catalog/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "Saved: catalog/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "Saved: catalog/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "Saved: catalog/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "Saved: catalog/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "Saved: catalog/raster/Traffic/Traffic.json\n",
      "Saved: catalog/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "Saved: catalog/crawl_summary.json\n",
      "\n",
      "‚úÖ Catalog generation complete!\n",
      "   - Master catalog: catalog/catalog.json\n",
      "   - Vector collection: catalog/vector/collection.json\n",
      "   - Raster collection: catalog/raster/collection.json\n",
      "   - Vector items: 2 items in their respective directories\n",
      "   - Raster items: 20 items in their respective directories\n",
      "   - Crawl summary: catalog/crawl_summary.json\n",
      "\n",
      "üìÅ Generated directory structure:\n",
      "   catalog/\n",
      "   ‚îú‚îÄ‚îÄ catalog.json\n",
      "   ‚îú‚îÄ‚îÄ crawl_summary.json\n",
      "   ‚îú‚îÄ‚îÄ vector/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ collection.json\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ ItemName1/\n",
      "   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName2/\n",
      "   ‚îÇ       ‚îî‚îÄ‚îÄ ItemName2.json\n",
      "   ‚îî‚îÄ‚îÄ raster/\n",
      "       ‚îú‚îÄ‚îÄ collection.json\n",
      "       ‚îú‚îÄ‚îÄ ItemName1/\n",
      "       ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\n",
      "       ‚îî‚îÄ‚îÄ ItemName2/\n",
      "           ‚îî‚îÄ‚îÄ ItemName2.json\n",
      "\n",
      "üí° If you encountered authentication errors:\n",
      "   1. Install: pip install google-cloud-storage\n",
      "   2. Set up authentication:\n",
      "      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\n",
      "      - Or use: gcloud auth application-default login\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to crawl bucket and generate catalog with fixed structure.\"\"\"\n",
    "    # Configuration - update these values for your specific bucket\n",
    "    bucket_name = \"swhm_data\"  # Just the bucket name, not the full URL\n",
    "    prefix = \"public/layers/\"  # Path prefix within the bucket\n",
    "    project_id = None  # Set your GCP project ID if needed\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = GCPBucketCrawler(bucket_name, prefix, project_id)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    print(\"Starting bucket crawl...\")\n",
    "    crawl_data = crawler.crawl_bucket()\n",
    "    \n",
    "    print(f\"Found {len(crawl_data['vectors'])} vectors and {len(crawl_data['rasters'])} rasters\")\n",
    "    \n",
    "    # Generate collections and catalog\n",
    "    generator = CatalogGenerator(crawl_data)\n",
    "    \n",
    "    # Generate all STAC items\n",
    "    print(\"Generating STAC items...\")\n",
    "    stac_items = generator.generate_all_stac_items()\n",
    "    \n",
    "    # Generate collections\n",
    "    vector_collection = generator.generate_vector_collection()\n",
    "    raster_collection = generator.generate_raster_collection()\n",
    "    \n",
    "    # Generate master catalog\n",
    "    master_catalog = generator.generate_master_catalog(vector_collection, raster_collection)\n",
    "    \n",
    "    # Save all files with corrected structure\n",
    "    print(\"\\nSaving catalog files...\")\n",
    "    \n",
    "    # Save master catalog\n",
    "    save_json(master_catalog, \"catalog/catalog.json\")\n",
    "    \n",
    "    # Save collections (singular names)\n",
    "    save_json(vector_collection, \"catalog/vector/collection.json\")\n",
    "    save_json(raster_collection, \"catalog/raster/collection.json\")\n",
    "    \n",
    "    # Save individual STAC items in same directories as source data\n",
    "    print(\"Saving individual STAC items...\")\n",
    "    \n",
    "    # Save vector items in their respective directories\n",
    "    for item in stac_items['vector_items']:\n",
    "        # Find the corresponding vector data to get the directory\n",
    "        vector_data = next((v for v in crawl_data['vectors'] if v['name'] == item['id']), None)\n",
    "        if vector_data:\n",
    "            item_path = f\"catalog/{vector_data['stac_dir']}/{item['id']}.json\"\n",
    "            save_json(item, item_path)\n",
    "    \n",
    "    # Save raster items in their respective directories\n",
    "    for item in stac_items['raster_items']:\n",
    "        # Find the corresponding raster data to get the directory\n",
    "        raster_data = next((r for r in crawl_data['rasters'] if r['name'] == item['id']), None)\n",
    "        if raster_data:\n",
    "            item_path = f\"catalog/{raster_data['stac_dir']}/{item['id']}.json\"\n",
    "            save_json(item, item_path)\n",
    "    \n",
    "    # Save summary with enhanced metadata\n",
    "    summary = {\n",
    "        \"crawl_summary\": {\n",
    "            \"bucket_name\": bucket_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"crawl_time\": datetime.now().isoformat(),\n",
    "            \"total_items\": crawl_data['total_items'],\n",
    "            \"vectors_found\": len(crawl_data['vectors']),\n",
    "            \"rasters_found\": len(crawl_data['rasters']),\n",
    "            \"stac_items_generated\": len(stac_items['vector_items']) + len(stac_items['raster_items'])\n",
    "        },\n",
    "        \"discovered_vectors\": crawl_data['vectors'],\n",
    "        \"discovered_rasters\": crawl_data['rasters'],\n",
    "        \"stac_structure\": {\n",
    "            \"catalog\": \"catalog/catalog.json\",\n",
    "            \"vector_collection\": \"catalog/vector/collection.json\",\n",
    "            \"raster_collection\": \"catalog/raster/collection.json\",\n",
    "            \"vector_items\": [f\"catalog/{v['stac_dir']}/{v['name']}.json\" for v in crawl_data['vectors']],\n",
    "            \"raster_items\": [f\"catalog/{r['stac_dir']}/{r['name']}.json\" for r in crawl_data['rasters']]\n",
    "        }\n",
    "    }\n",
    "    save_json(summary, \"catalog/crawl_summary.json\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Catalog generation complete!\")\n",
    "    print(f\"   - Master catalog: catalog/catalog.json\")\n",
    "    print(f\"   - Vector collection: catalog/vector/collection.json\")\n",
    "    print(f\"   - Raster collection: catalog/raster/collection.json\")\n",
    "    print(f\"   - Vector items: {len(stac_items['vector_items'])} items in their respective directories\")\n",
    "    print(f\"   - Raster items: {len(stac_items['raster_items'])} items in their respective directories\")\n",
    "    print(f\"   - Crawl summary: catalog/crawl_summary.json\")\n",
    "    \n",
    "    # Print directory structure\n",
    "    print(f\"\\nüìÅ Generated directory structure:\")\n",
    "    print(f\"   catalog/\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ catalog.json\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ crawl_summary.json\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ vector/\")\n",
    "    print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ collection.json\")\n",
    "    print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ ItemName1/\")\n",
    "    print(f\"   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\")\n",
    "    print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName2/\")\n",
    "    print(f\"   ‚îÇ       ‚îî‚îÄ‚îÄ ItemName2.json\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ raster/\")\n",
    "    print(f\"       ‚îú‚îÄ‚îÄ collection.json\")\n",
    "    print(f\"       ‚îú‚îÄ‚îÄ ItemName1/\")\n",
    "    print(f\"       ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\")\n",
    "    print(f\"       ‚îî‚îÄ‚îÄ ItemName2/\")\n",
    "    print(f\"           ‚îî‚îÄ‚îÄ ItemName2.json\")\n",
    "    \n",
    "    # Print authentication help if needed\n",
    "    print(f\"\\nüí° If you encountered authentication errors:\")\n",
    "    print(f\"   1. Install: pip install google-cloud-storage\")\n",
    "    print(f\"   2. Set up authentication:\")\n",
    "    print(f\"      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\")\n",
    "    print(f\"      - Or use: gcloud auth application-default login\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "upload_example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPLOADING STAC CATALOG TO GCS ===\n",
      "\n",
      "1. Dry run to preview uploads:\n",
      "Using gsutil for uploads\n",
      "Scanning directory: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/scripts/ipynb/catalog\n",
      "Target GCS location: gs://swhm_data/public/layers/\n",
      "Dry run: True\n",
      "--------------------------------------------------\n",
      "Found 26 JSON files to upload\n",
      "--------------------------------------------------\n",
      "üìÑ crawl_summary.json\n",
      "   Local:  crawl_summary.json\n",
      "   GCS:    gs://swhm_data/public/layers/crawl_summary.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ catalog.json\n",
      "   Local:  catalog.json\n",
      "   GCS:    gs://swhm_data/public/layers/catalog.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ collection.json\n",
      "   Local:  raster/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/collection.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ collection.json\n",
      "   Local:  vector/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/collection.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ cig_grid_wgs.json\n",
      "   Local:  vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ PugetSoundWA.json\n",
      "   Local:  vector/PugetSoundWA/PugetSoundWA.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Traffic.json\n",
      "   Local:  raster/Traffic/Traffic.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Traffic/Traffic.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Slope.json\n",
      "   Local:  raster/Slope/Slope.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope/Slope.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Land_Use.json\n",
      "   Local:  raster/Land_Use/Land_Use.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Use/Land_Use.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Total_Suspended_Solids_Concentration.json\n",
      "   Local:  raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Land_Cover.json\n",
      "   Local:  raster/Land_Cover/Land_Cover.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Cover/Land_Cover.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Population_Density.json\n",
      "   Local:  raster/Population_Density/Population_Density.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Population_Density/Population_Density.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Total_Phosphorus_Concentration.json\n",
      "   Local:  raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Imperviousness.json\n",
      "   Local:  raster/Imperviousness/Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Imperviousness/Imperviousness.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ copper_concentration_ug_per_L.json\n",
      "   Local:  raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Age_of_Imperviousness.json\n",
      "   Local:  raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Flow_Duration_Index.json\n",
      "   Local:  raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Slope_Categories.json\n",
      "   Local:  raster/Slope_Categories/Slope_Categories.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope_Categories/Slope_Categories.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ HSPF_Land_Cover_Type.json\n",
      "   Local:  raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Hydrologic_Response_Units.json\n",
      "   Local:  raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Precipitation_mm.json\n",
      "   Local:  raster/Precipitation_mm/Precipitation_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Precipitation_mm/Precipitation_mm.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Total_Copper_Concentration.json\n",
      "   Local:  raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Soils.json\n",
      "   Local:  raster/Soils/Soils.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Soils/Soils.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Runoff_mm.json\n",
      "   Local:  raster/Runoff_mm/Runoff_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Runoff_mm/Runoff_mm.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   Local:  raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ Total_Zinc_Concentration.json\n",
      "   Local:  raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "==================================================\n",
      "üìä UPLOAD SUMMARY\n",
      "==================================================\n",
      "Total files found: 26\n",
      "Successfully uploaded: 0\n",
      "Skipped (dry run): 26\n",
      "Failed: 0\n",
      "\n",
      "2. Actual upload:\n",
      "Using gsutil for uploads\n",
      "Scanning directory: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog/scripts/ipynb/catalog\n",
      "Target GCS location: gs://swhm_data/public/layers/\n",
      "Dry run: False\n",
      "--------------------------------------------------\n",
      "Found 26 JSON files to upload\n",
      "--------------------------------------------------\n",
      "üìÑ crawl_summary.json\n",
      "   Local:  crawl_summary.json\n",
      "   GCS:    gs://swhm_data/public/layers/crawl_summary.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ catalog.json\n",
      "   Local:  catalog.json\n",
      "   GCS:    gs://swhm_data/public/layers/catalog.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ collection.json\n",
      "   Local:  raster/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/collection.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ collection.json\n",
      "   Local:  vector/collection.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/collection.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ cig_grid_wgs.json\n",
      "   Local:  vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ PugetSoundWA.json\n",
      "   Local:  vector/PugetSoundWA/PugetSoundWA.json\n",
      "   GCS:    gs://swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Traffic.json\n",
      "   Local:  raster/Traffic/Traffic.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Traffic/Traffic.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Slope.json\n",
      "   Local:  raster/Slope/Slope.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope/Slope.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Land_Use.json\n",
      "   Local:  raster/Land_Use/Land_Use.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Use/Land_Use.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Total_Suspended_Solids_Concentration.json\n",
      "   Local:  raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Land_Cover.json\n",
      "   Local:  raster/Land_Cover/Land_Cover.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Land_Cover/Land_Cover.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Population_Density.json\n",
      "   Local:  raster/Population_Density/Population_Density.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Population_Density/Population_Density.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Total_Phosphorus_Concentration.json\n",
      "   Local:  raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Imperviousness.json\n",
      "   Local:  raster/Imperviousness/Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Imperviousness/Imperviousness.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ copper_concentration_ug_per_L.json\n",
      "   Local:  raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Age_of_Imperviousness.json\n",
      "   Local:  raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Flow_Duration_Index.json\n",
      "   Local:  raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Slope_Categories.json\n",
      "   Local:  raster/Slope_Categories/Slope_Categories.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Slope_Categories/Slope_Categories.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ HSPF_Land_Cover_Type.json\n",
      "   Local:  raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Hydrologic_Response_Units.json\n",
      "   Local:  raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Precipitation_mm.json\n",
      "   Local:  raster/Precipitation_mm/Precipitation_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Precipitation_mm/Precipitation_mm.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Total_Copper_Concentration.json\n",
      "   Local:  raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Soils.json\n",
      "   Local:  raster/Soils/Soils.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Soils/Soils.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Runoff_mm.json\n",
      "   Local:  raster/Runoff_mm/Runoff_mm.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Runoff_mm/Runoff_mm.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   Local:  raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ Total_Zinc_Concentration.json\n",
      "   Local:  raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   GCS:    gs://swhm_data/public/layers/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "==================================================\n",
      "üìä UPLOAD SUMMARY\n",
      "==================================================\n",
      "Total files found: 26\n",
      "Successfully uploaded: 26\n",
      "Skipped (dry run): 0\n",
      "Failed: 0\n",
      "\n",
      "‚úÖ Upload complete! Files available at:\n",
      "   gs://swhm_data/\n",
      "\n",
      "üîÑ Cache-Control headers set to 'no-cache, no-store, must-revalidate'\n",
      "   This ensures STAC Browser always fetches the latest catalog data\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "GCS_BUCKET = \"swhm_data\"\n",
    "GCS_PREFIX = \"public/layers/\"\n",
    "\n",
    "# IMPORTANT: Point directly to the catalog directory, not a parent directory\n",
    "# This ensures relative paths are calculated correctly\n",
    "CATALOG_DIR = \"catalog\"  # Use relative path from notebook location, or absolute path to catalog folder\n",
    "\n",
    "# Example usage with new refactored uploader\n",
    "print(\"=== UPLOADING STAC CATALOG TO GCS ===\\n\")\n",
    "\n",
    "# Upload with dry run first to see what would be uploaded\n",
    "print(\"1. Dry run to preview uploads:\")\n",
    "results = upload_stac_catalog(\n",
    "    root_dir=CATALOG_DIR,\n",
    "    bucket_name=GCS_BUCKET,\n",
    "    prefix=GCS_PREFIX,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Actual upload:\")\n",
    "# Uncomment the line below to perform actual upload\n",
    "results = upload_stac_catalog(\n",
    "    root_dir=CATALOG_DIR,\n",
    "    bucket_name=GCS_BUCKET,\n",
    "    prefix=GCS_PREFIX,\n",
    "    dry_run=False\n",
    ")\n",
    "\n",
    "# Alternative: Use absolute path to catalog directory\n",
    "# CATALOG_DIR_ABSOLUTE = \"/Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
