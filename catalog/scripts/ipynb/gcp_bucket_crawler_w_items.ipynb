{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a8b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GCP Bucket Crawler and Catalog Generator\n",
    "Crawls a GCP storage bucket to discover vector and raster data,\n",
    "then generates collections, individual STAC items, and a comprehensive catalog.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6f28e6c",
   "metadata": {},
   "outputs": [],
   "source": "class GCPBucketCrawler:\n    def __init__(self, bucket_name: str, prefix: str = \"\", project_id: Optional[str] = None):\n        \"\"\"\n        Initialize the crawler with GCP bucket details.\n        \n        Args:\n            bucket_name: Name of the GCP storage bucket (e.g., 'swhm_data')\n            prefix: Prefix to filter objects (e.g., 'public/layers/')\n            project_id: GCP project ID (optional, will use default if not provided)\n        \"\"\"\n        self.bucket_name = bucket_name\n        self.prefix = prefix\n        self.project_id = project_id\n        self.vectors = []\n        self.rasters = []\n        \n        # Initialize the GCS client\n        try:\n            if project_id:\n                self.client = storage.Client(project=project_id)\n            else:\n                self.client = storage.Client()\n            self.bucket = self.client.bucket(bucket_name)\n            print(f\"Successfully connected to bucket: {bucket_name}\")\n        except Exception as e:\n            print(f\"Error initializing GCS client: {e}\")\n            print(\"Make sure you have proper authentication set up:\")\n            print(\"1. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n            print(\"2. Or run 'gcloud auth application-default login'\")\n            self.client = None\n            self.bucket = None\n        \n    def crawl_bucket(self) -> Dict:\n        \"\"\"\n        Crawl the GCP bucket to discover all vectors and rasters.\n        Returns a dictionary with discovered items.\n        \"\"\"\n        if not self.client or not self.bucket:\n            print(\"No valid GCS client available, creating sample data...\")\n            return self._create_sample_data()\n            \n        print(f\"Crawling bucket '{self.bucket_name}' with prefix '{self.prefix}'...\")\n        \n        try:\n            # List all blobs in the bucket with the specified prefix\n            blobs = self.bucket.list_blobs(prefix=self.prefix)\n            \n            blob_count = 0\n            for blob in blobs:\n                blob_count += 1\n                self._process_blob(blob)\n                \n            print(f\"Processed {blob_count} objects from bucket\")\n            \n        except Exception as e:\n            print(f\"Error crawling bucket: {e}\")\n            return self._create_sample_data()\n            \n        return {\n            'vectors': self.vectors,\n            'rasters': self.rasters,\n            'total_items': len(self.vectors) + len(self.rasters)\n        }\n    \n    def _process_blob(self, blob):\n        \"\"\"Process a single blob to determine if it's a vector or raster.\"\"\"\n        blob_name = blob.name\n        blob_path = Path(blob_name)\n        \n        # Skip directories (blobs ending with '/')\n        if blob_name.endswith('/'):\n            return\n            \n        # Check for vector files - ONLY .geojson files, NOT .json files\n        if 'vector/' in blob_name and blob_path.suffix.lower() == '.geojson':\n            self._add_vector_item(blob)\n            \n        # Check for raster files - TIFF files including .gtiff  \n        elif 'raster/' in blob_name and blob_path.suffix.lower() in ['.tiff', '.tif', '.gtiff']:\n            self._add_raster_item(blob)\n    \n    def _add_vector_item(self, blob):\n        \"\"\"Add a vector item to the collection.\"\"\"\n        blob_path = Path(blob.name)\n        item_name = blob_path.stem\n        \n        # Skip if already exists (prevent duplicates)\n        if any(v['name'] == item_name for v in self.vectors):\n            print(f\"Skipping duplicate vector: {item_name}\")\n            return\n        \n        # Create public URL\n        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n        \n        vector_item = {\n            'name': item_name,\n            'filename': blob.name,\n            'url': public_url,\n            'type': 'vector',\n            'format': 'GeoJSON',\n            'size_bytes': blob.size,\n            'content_type': blob.content_type,\n            'created': blob.time_created.isoformat() if blob.time_created else None,\n            'updated': blob.updated.isoformat() if blob.updated else None,\n            'discovered_at': datetime.now().isoformat(),\n            'etag': blob.etag,\n            'md5_hash': blob.md5_hash\n        }\n        \n        self.vectors.append(vector_item)\n        print(f\"Found vector: {item_name}\")\n    \n    def _add_raster_item(self, blob):\n        \"\"\"Add a raster item to the collection.\"\"\"\n        blob_path = Path(blob.name)\n        item_name = blob_path.stem\n        \n        # Skip if already exists (prevent duplicates)\n        if any(r['name'] == item_name for r in self.rasters):\n            print(f\"Skipping duplicate raster: {item_name}\")\n            return\n        \n        # Create public URL\n        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n        \n        raster_item = {\n            'name': item_name,\n            'filename': blob.name,\n            'url': public_url,\n            'type': 'raster',\n            'format': 'GeoTIFF',\n            'size_bytes': blob.size,\n            'content_type': blob.content_type,\n            'created': blob.time_created.isoformat() if blob.time_created else None,\n            'updated': blob.updated.isoformat() if blob.updated else None,\n            'discovered_at': datetime.now().isoformat(),\n            'etag': blob.etag,\n            'md5_hash': blob.md5_hash\n        }\n        \n        self.rasters.append(raster_item)\n        print(f\"Found raster: {item_name}\")\n    \n    def get_blob_info(self, blob_name: str) -> Optional[Dict]:\n        \"\"\"Get detailed information about a specific blob.\"\"\"\n        if not self.bucket:\n            return None\n            \n        try:\n            blob = self.bucket.blob(blob_name)\n            if blob.exists():\n                return {\n                    'name': blob.name,\n                    'size': blob.size,\n                    'content_type': blob.content_type,\n                    'created': blob.time_created.isoformat() if blob.time_created else None,\n                    'updated': blob.updated.isoformat() if blob.updated else None,\n                    'etag': blob.etag,\n                    'md5_hash': blob.md5_hash,\n                    'public_url': f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n                }\n        except Exception as e:\n            print(f\"Error getting blob info for {blob_name}: {e}\")\n            \n        return None\n    \n    def _create_sample_data(self):\n        \"\"\"Create sample data structure when bucket can't be crawled directly.\"\"\"\n        print(\"Creating sample data structure...\")\n        \n        base_url = f\"https://storage.googleapis.com/{self.bucket_name}\"\n        \n        # Sample vectors based on your structure\n        sample_vectors = [\n            {\n                'name': 'vector1',\n                'filename': f'{self.prefix}vectors/vector1/vector1.geojson',\n                'url': f\"{base_url}/{self.prefix}vectors/vector1/vector1.geojson\",\n                'type': 'vector',\n                'format': 'GeoJSON',\n                'size_bytes': None,\n                'content_type': 'application/geo+json',\n                'created': None,\n                'updated': None,\n                'discovered_at': datetime.now().isoformat(),\n                'etag': None,\n                'md5_hash': None\n            }\n        ]\n        \n        # Sample rasters based on your structure\n        sample_rasters = [\n            {\n                'name': 'raster1',\n                'filename': f'{self.prefix}rasters/raster1/raster1.tiff',\n                'url': f\"{base_url}/{self.prefix}rasters/raster1/raster1.tiff\",\n                'type': 'raster',\n                'format': 'GeoTIFF',\n                'size_bytes': None,\n                'content_type': 'image/tiff',\n                'created': None,\n                'updated': None,\n                'discovered_at': datetime.now().isoformat(),\n                'etag': None,\n                'md5_hash': None\n            }\n        ]\n        \n        self.vectors = sample_vectors\n        self.rasters = sample_rasters\n        \n        return {\n            'vectors': self.vectors,\n            'rasters': self.rasters,\n            'total_items': len(self.vectors) + len(self.rasters)\n        }\n\nclass CatalogGenerator:\n    def __init__(self, crawler_data: Dict):\n        \"\"\"Initialize with data from the crawler.\"\"\"\n        self.data = crawler_data\n        self.stac_items = []\n        \n    def generate_stac_item(self, item_data: Dict, item_type: str) -> Dict:\n        \"\"\"Generate a STAC item for vector or raster data.\"\"\"\n        item_id = item_data['name']\n        \n        # Base STAC item structure\n        stac_item = {\n            \"type\": \"Feature\",\n            \"stac_version\": \"1.0.0\",\n            \"id\": item_id,\n            \"properties\": {\n                \"title\": item_data['name'].replace('_', ' ').title(),\n                \"description\": f\"{item_type.title()} dataset: {item_data['name']}\",\n                \"datetime\": item_data['discovered_at'],\n                \"created\": item_data.get('created') or item_data['discovered_at'],\n                \"updated\": item_data.get('updated') or item_data['discovered_at'],\n                \"providers\": [\n                    {\n                        \"name\": \"SWHM Data\",\n                        \"roles\": [\"producer\", \"processor\", \"host\"],\n                        \"url\": \"https://storage.googleapis.com/swhm_data/\"\n                    }\n                ]\n            },\n            \"geometry\": None,  # Would need to extract from actual data\n            \"bbox\": None,  # Would need to calculate from geometry/bounds\n            \"assets\": {},\n            \"links\": [\n                {\n                    \"rel\": \"self\",\n                    \"href\": f\"./{item_id}.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"parent\",\n                    \"href\": \"../collection.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"collection\",\n                    \"href\": \"../collection.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"root\",\n                    \"href\": \"../../catalog.json\",\n                    \"type\": \"application/json\"\n                }\n            ]\n        }\n        \n        # Add assets based on type\n        if item_type == 'vector':\n            stac_item[\"assets\"][\"data\"] = {\n                \"href\": item_data['url'],\n                \"type\": \"application/geo+json\",\n                \"title\": \"GeoJSON data\",\n                \"description\": \"Vector data in GeoJSON format\",\n                \"roles\": [\"data\"],\n                \"file:size\": item_data.get('size_bytes'),\n                \"file:checksum\": item_data.get('md5_hash')\n            }\n        elif item_type == 'raster':\n            stac_item[\"assets\"][\"data\"] = {\n                \"href\": item_data['url'],\n                \"type\": \"image/tiff; application=geotiff\",\n                \"title\": \"GeoTIFF data\",\n                \"description\": \"Raster data in GeoTIFF format\",\n                \"roles\": [\"data\"],\n                \"file:size\": item_data.get('size_bytes'),\n                \"file:checksum\": item_data.get('md5_hash')\n            }\n            \n            # Add COG asset if it's a Cloud Optimized GeoTIFF\n            stac_item[\"assets\"][\"cog\"] = {\n                \"href\": item_data['url'],\n                \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n                \"title\": \"Cloud Optimized GeoTIFF\",\n                \"description\": \"Cloud Optimized GeoTIFF for web access\",\n                \"roles\": [\"data\", \"overview\"]\n            }\n        \n        # Only add metadata and thumbnail assets if they exist (no placeholders)\n        # This prevents STAC Browser from trying to load non-existent resources\n        \n        return stac_item\n    \n    def generate_vector_collection(self) -> Dict:\n        \"\"\"Generate a vector collection with individual STAC items.\"\"\"\n        collection = {\n            \"type\": \"Collection\",\n            \"stac_version\": \"1.0.0\",\n            \"id\": \"swhm-vectors\",\n            \"title\": \"SWHM Vector Collection\",\n            \"description\": \"Collection of vector datasets from SWHM data bucket\",\n            \"keywords\": [\"vectors\", \"geojson\", \"swhm\"],\n            \"license\": \"proprietary\",\n            \"extent\": {\n                \"spatial\": {\n                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n                },\n                \"temporal\": {\n                    \"interval\": [[None, None]]\n                }\n            },\n            \"providers\": [\n                {\n                    \"name\": \"SWHM Data\",\n                    \"roles\": [\"producer\", \"processor\", \"host\"],\n                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n                }\n            ],\n            \"links\": [\n                {\n                    \"rel\": \"self\",\n                    \"href\": \"./collection.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"parent\",\n                    \"href\": \"../catalog.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"root\",\n                    \"href\": \"../catalog.json\",\n                    \"type\": \"application/json\"\n                }\n            ],\n            \"item_assets\": {\n                \"data\": {\n                    \"type\": \"application/geo+json\",\n                    \"title\": \"GeoJSON data\",\n                    \"roles\": [\"data\"]\n                }\n            }\n        }\n        \n        # Add links to individual items (with deduplication)\n        added_items = set()\n        for vector in self.data['vectors']:\n            item_id = vector['name']\n            if item_id not in added_items:\n                collection[\"links\"].append({\n                    \"rel\": \"item\",\n                    \"href\": f\"./items/{item_id}.json\",\n                    \"type\": \"application/json\",\n                    \"title\": vector['name'].replace('_', ' ').title()\n                })\n                added_items.add(item_id)\n            \n        return collection\n    \n    def generate_raster_collection(self) -> Dict:\n        \"\"\"Generate a raster collection with individual STAC items.\"\"\"\n        collection = {\n            \"type\": \"Collection\",\n            \"stac_version\": \"1.0.0\",\n            \"id\": \"swhm-rasters\",\n            \"title\": \"SWHM Raster Collection\",\n            \"description\": \"Collection of raster datasets from SWHM data bucket\",\n            \"keywords\": [\"rasters\", \"geotiff\", \"swhm\"],\n            \"license\": \"proprietary\",\n            \"extent\": {\n                \"spatial\": {\n                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n                },\n                \"temporal\": {\n                    \"interval\": [[None, None]]\n                }\n            },\n            \"providers\": [\n                {\n                    \"name\": \"SWHM Data\",\n                    \"roles\": [\"producer\", \"processor\", \"host\"],\n                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n                }\n            ],\n            \"links\": [\n                {\n                    \"rel\": \"self\",\n                    \"href\": \"./collection.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"parent\",\n                    \"href\": \"../catalog.json\",\n                    \"type\": \"application/json\"\n                },\n                {\n                    \"rel\": \"root\",\n                    \"href\": \"../catalog.json\",\n                    \"type\": \"application/json\"\n                }\n            ],\n            \"item_assets\": {\n                \"data\": {\n                    \"type\": \"image/tiff; application=geotiff\",\n                    \"title\": \"GeoTIFF data\",\n                    \"roles\": [\"data\"]\n                },\n                \"cog\": {\n                    \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n                    \"title\": \"Cloud Optimized GeoTIFF\",\n                    \"roles\": [\"data\", \"overview\"]\n                }\n            }\n        }\n        \n        # Add links to individual items (with deduplication)\n        added_items = set()\n        for raster in self.data['rasters']:\n            item_id = raster['name']\n            if item_id not in added_items:\n                collection[\"links\"].append({\n                    \"rel\": \"item\",\n                    \"href\": f\"./items/{item_id}.json\",\n                    \"type\": \"application/json\",\n                    \"title\": raster['name'].replace('_', ' ').title()\n                })\n                added_items.add(item_id)\n            \n        return collection\n    \n    def generate_master_catalog(self, vector_collection: Dict, raster_collection: Dict) -> Dict:\n        \"\"\"Generate the master catalog containing all collections.\"\"\"\n        catalog = {\n            \"type\": \"Catalog\",\n            \"stac_version\": \"1.0.0\",\n            \"id\": \"swhm-data-catalog\",\n            \"title\": \"SWHM Data Catalog\",\n            \"description\": \"Master catalog for SWHM vector and raster datasets\",\n            \"created\": datetime.now().isoformat(),\n            \"updated\": datetime.now().isoformat(),\n            \"keywords\": [\"swhm\", \"vectors\", \"rasters\", \"geospatial\"],\n            \"providers\": [\n                {\n                    \"name\": \"SWHM Data\",\n                    \"roles\": [\"producer\", \"processor\", \"host\"],\n                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n                }\n            ],\n            \"links\": [\n                {\n                    \"rel\": \"self\",\n                    \"href\": \"./catalog.json\",\n                    \"type\": \"application/json\",\n                    \"title\": \"SWHM Data Catalog\"\n                },\n                {\n                    \"rel\": \"child\",\n                    \"href\": \"./vectors/collection.json\",\n                    \"type\": \"application/json\",\n                    \"title\": \"Vector Collection\"\n                },\n                {\n                    \"rel\": \"child\",\n                    \"href\": \"./rasters/collection.json\",\n                    \"type\": \"application/json\",\n                    \"title\": \"Raster Collection\"\n                }\n            ],\n            \"conformsTo\": [\n                \"https://api.stacspec.org/v1.0.0/core\",\n                \"https://api.stacspec.org/v1.0.0/collections\"\n            ]\n        }\n        \n        return catalog\n    \n    def generate_all_stac_items(self) -> Dict[str, List[Dict]]:\n        \"\"\"Generate all STAC items for vectors and rasters.\"\"\"\n        vector_items = []\n        raster_items = []\n        \n        # Generate vector items (with deduplication)\n        added_vector_ids = set()\n        for vector in self.data['vectors']:\n            if vector['name'] not in added_vector_ids:\n                stac_item = self.generate_stac_item(vector, 'vector')\n                vector_items.append(stac_item)\n                added_vector_ids.add(vector['name'])\n            \n        # Generate raster items (with deduplication)\n        added_raster_ids = set()\n        for raster in self.data['rasters']:\n            if raster['name'] not in added_raster_ids:\n                stac_item = self.generate_stac_item(raster, 'raster')\n                raster_items.append(stac_item)\n                added_raster_ids.add(raster['name'])\n            \n        return {\n            'vector_items': vector_items,\n            'raster_items': raster_items\n        }\n\ndef save_json(data: Dict, filepath: str):\n    \"\"\"Save data to JSON file with pretty formatting.\"\"\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n    with open(filepath, 'w') as f:\n        json.dump(data, f, indent=2, ensure_ascii=False)\n    print(f\"Saved: {filepath}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f71dc9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: swhm_data\n",
      "Starting bucket crawl...\n",
      "Crawling bucket 'swhm_data' with prefix 'public/layers/'...\n",
      "Found raster: Age_of_Imperviousness\n",
      "Found raster: Flow_Duration_Index\n",
      "Found raster: HSPF_Land_Cover_Type\n",
      "Found raster: Hydrologic_Response_Units\n",
      "Found raster: Imperviousness\n",
      "Found raster: Land_Cover\n",
      "Found raster: Land_Use\n",
      "Found raster: Population_Density\n",
      "Found raster: Precipitation_mm\n",
      "Found raster: Runoff_mm\n",
      "Found raster: Slope\n",
      "Found raster: Slope_Categories\n",
      "Found raster: Soils\n",
      "Found raster: Total_Copper_Concentration\n",
      "Found raster: Total_Kjeldahl_Nitrogen_Concentration\n",
      "Found raster: Total_Phosphorus_Concentration\n",
      "Found raster: Total_Suspended_Solids_Concentration\n",
      "Found raster: Total_Zinc_Concentration\n",
      "Found raster: Traffic\n",
      "Found raster: copper_concentration_ug_per_L\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: cig_grid_wgs\n",
      "Found vector: cig_grid_wgs\n",
      "Found vector: collection\n",
      "Processed 48 objects from bucket\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"swhm_data\"  # Just the bucket name, not the full URL\n",
    "prefix = \"public/layers/\"  # Path prefix within the bucket\n",
    "project_id = None  # Set your GCP project ID if needed\n",
    "    \n",
    "# Initialize crawler\n",
    "crawler = GCPBucketCrawler(bucket_name, prefix, project_id)\n",
    "# Crawl the bucket\n",
    "print(\"Starting bucket crawl...\")\n",
    "crawl_data = crawler.crawl_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0d7830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectors': [{'name': 'PugetSoundWA',\n",
       "   'filename': 'public/layers/vector/PugetSoundWA/PugetSoundWA.geojson',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.geojson',\n",
       "   'type': 'vector',\n",
       "   'format': 'GeoJSON',\n",
       "   'size_bytes': 1532246,\n",
       "   'content_type': 'application/geo+json',\n",
       "   'created': '2025-07-02T20:31:52.564000+00:00',\n",
       "   'updated': '2025-07-02T20:31:52.564000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390549',\n",
       "   'etag': 'CPGky7WCn44DEAE=',\n",
       "   'md5_hash': 'hwh3Z4n83TKJDjiixjSBxA=='},\n",
       "  {'name': 'PugetSoundWA',\n",
       "   'filename': 'public/layers/vector/PugetSoundWA/PugetSoundWA.json',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/vector/PugetSoundWA/PugetSoundWA.json',\n",
       "   'type': 'vector',\n",
       "   'format': 'GeoJSON',\n",
       "   'size_bytes': 4642,\n",
       "   'content_type': 'application/json',\n",
       "   'created': '2025-07-02T23:30:02.745000+00:00',\n",
       "   'updated': '2025-07-02T23:30:02.745000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390597',\n",
       "   'etag': 'CPTqh5+qn44DEAE=',\n",
       "   'md5_hash': 'HT1xBeQV1dmY5r2NSRnwlQ=='},\n",
       "  {'name': 'cig_grid_wgs',\n",
       "   'filename': 'public/layers/vector/cig_grid_wgs/cig_grid_wgs.geojson',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.geojson',\n",
       "   'type': 'vector',\n",
       "   'format': 'GeoJSON',\n",
       "   'size_bytes': 473161,\n",
       "   'content_type': 'application/geo+json',\n",
       "   'created': '2025-07-02T20:27:01.973000+00:00',\n",
       "   'updated': '2025-07-02T20:27:01.973000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390644',\n",
       "   'etag': 'CLn+gquBn44DEAE=',\n",
       "   'md5_hash': 'zB7rQgb8wU+b9MAFGcmgsg=='},\n",
       "  {'name': 'cig_grid_wgs',\n",
       "   'filename': 'public/layers/vector/cig_grid_wgs/cig_grid_wgs.json',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/vector/cig_grid_wgs/cig_grid_wgs.json',\n",
       "   'type': 'vector',\n",
       "   'format': 'GeoJSON',\n",
       "   'size_bytes': 3452,\n",
       "   'content_type': 'application/json',\n",
       "   'created': '2025-07-02T23:30:00.913000+00:00',\n",
       "   'updated': '2025-07-02T23:30:00.913000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390705',\n",
       "   'etag': 'CKqFmJ6qn44DEAE=',\n",
       "   'md5_hash': 'p3+7BcpLN7L8MQyEaRXlwg=='},\n",
       "  {'name': 'collection',\n",
       "   'filename': 'public/layers/vector/collection.json',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/vector/collection.json',\n",
       "   'type': 'vector',\n",
       "   'format': 'GeoJSON',\n",
       "   'size_bytes': 1222,\n",
       "   'content_type': 'application/json',\n",
       "   'created': '2025-07-02T23:19:26.697000+00:00',\n",
       "   'updated': '2025-07-02T23:19:26.697000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390751',\n",
       "   'etag': 'CIHm4u+nn44DEAE=',\n",
       "   'md5_hash': 'dde7SEyxUf6uGxohoH1T0A=='}],\n",
       " 'rasters': [{'name': 'Age_of_Imperviousness',\n",
       "   'filename': 'public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 11861750,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:13+00:00',\n",
       "   'updated': '2025-07-02T16:09:13+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.388731',\n",
       "   'etag': 'CPDu69rHno4DEAE=',\n",
       "   'md5_hash': 'LPPLq3xaCtJfgHfL9rlVkQ=='},\n",
       "  {'name': 'Flow_Duration_Index',\n",
       "   'filename': 'public/layers/raster/Flow_Duration_Index/Flow_Duration_Index.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Flow_Duration_Index/Flow_Duration_Index.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 2462802183,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:13.400000+00:00',\n",
       "   'updated': '2025-07-02T16:09:13.400000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389098',\n",
       "   'etag': 'CLaOhNvHno4DEAE=',\n",
       "   'md5_hash': 'FU76chTVAYkh+aspkIvnlQ=='},\n",
       "  {'name': 'HSPF_Land_Cover_Type',\n",
       "   'filename': 'public/layers/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 789553258,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:13.881000+00:00',\n",
       "   'updated': '2025-07-02T16:09:13.881000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389207',\n",
       "   'etag': 'CPamodvHno4DEAE=',\n",
       "   'md5_hash': None},\n",
       "  {'name': 'Hydrologic_Response_Units',\n",
       "   'filename': 'public/layers/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1067170878,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:14.329000+00:00',\n",
       "   'updated': '2025-07-02T16:09:14.329000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389319',\n",
       "   'etag': 'CMPOvNvHno4DEAE=',\n",
       "   'md5_hash': None},\n",
       "  {'name': 'Imperviousness',\n",
       "   'filename': 'public/layers/raster/Imperviousness/Imperviousness.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Imperviousness/Imperviousness.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 14816117190,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:14.827000+00:00',\n",
       "   'updated': '2025-07-02T16:09:14.827000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389388',\n",
       "   'etag': 'CMfZ2tvHno4DEAE=',\n",
       "   'md5_hash': None},\n",
       "  {'name': 'Land_Cover',\n",
       "   'filename': 'public/layers/raster/Land_Cover/Land_Cover.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Land_Cover/Land_Cover.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 2603697773,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:15.427000+00:00',\n",
       "   'updated': '2025-07-02T16:09:15.427000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389472',\n",
       "   'etag': 'CO2P/9vHno4DEAE=',\n",
       "   'md5_hash': None},\n",
       "  {'name': 'Land_Use',\n",
       "   'filename': 'public/layers/raster/Land_Use/Land_Use.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Land_Use/Land_Use.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 292776614,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:15.880000+00:00',\n",
       "   'updated': '2025-07-02T16:09:15.880000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389561',\n",
       "   'etag': 'CKzKm9zHno4DEAE=',\n",
       "   'md5_hash': 'NiT/WuZVgTQgu/DNktpeAg=='},\n",
       "  {'name': 'Population_Density',\n",
       "   'filename': 'public/layers/raster/Population_Density/Population_Density.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Population_Density/Population_Density.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 11961723,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:16.258000+00:00',\n",
       "   'updated': '2025-07-02T16:09:16.258000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389632',\n",
       "   'etag': 'CNTPstzHno4DEAE=',\n",
       "   'md5_hash': 'SStpcZl0ieyOQ3fu96U8tg=='},\n",
       "  {'name': 'Precipitation_mm',\n",
       "   'filename': 'public/layers/raster/Precipitation_mm/Precipitation_mm.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Precipitation_mm/Precipitation_mm.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 184853,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:16.601000+00:00',\n",
       "   'updated': '2025-07-02T16:09:16.601000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389706',\n",
       "   'etag': 'CO7Ox9zHno4DEAE=',\n",
       "   'md5_hash': 'SdfJ3o+QTi3Q9zVyUnB3eg=='},\n",
       "  {'name': 'Runoff_mm',\n",
       "   'filename': 'public/layers/raster/Runoff_mm/Runoff_mm.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Runoff_mm/Runoff_mm.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 5723145271,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:16.957000+00:00',\n",
       "   'updated': '2025-07-02T16:09:16.957000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389775',\n",
       "   'etag': 'CKea3dzHno4DEAE=',\n",
       "   'md5_hash': 'Z0XYBnipMVrt4VKCqxTD/w=='},\n",
       "  {'name': 'Slope',\n",
       "   'filename': 'public/layers/raster/Slope/Slope.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Slope/Slope.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 9657033234,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:17.364000+00:00',\n",
       "   'updated': '2025-07-02T16:09:17.364000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389861',\n",
       "   'etag': 'CLiO9tzHno4DEAE=',\n",
       "   'md5_hash': 'ESdu/Mz2Bx1y5owE/DzY4Q=='},\n",
       "  {'name': 'Slope_Categories',\n",
       "   'filename': 'public/layers/raster/Slope_Categories/Slope_Categories.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Slope_Categories/Slope_Categories.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1162360741,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:17.773000+00:00',\n",
       "   'updated': '2025-07-02T16:09:17.773000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.389940',\n",
       "   'etag': 'CIqDj93Hno4DEAE=',\n",
       "   'md5_hash': '14rJoVBBo6xBaDiF0JJ0dg=='},\n",
       "  {'name': 'Soils',\n",
       "   'filename': 'public/layers/raster/Soils/Soils.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Soils/Soils.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 377864655,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:18.198000+00:00',\n",
       "   'updated': '2025-07-02T16:09:18.198000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390018',\n",
       "   'etag': 'CN/qqN3Hno4DEAE=',\n",
       "   'md5_hash': None},\n",
       "  {'name': 'Total_Copper_Concentration',\n",
       "   'filename': 'public/layers/raster/Total_Copper_Concentration/Total_Copper_Concentration.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Total_Copper_Concentration/Total_Copper_Concentration.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1751254482,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:18.939000+00:00',\n",
       "   'updated': '2025-07-02T16:09:18.939000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390086',\n",
       "   'etag': 'CM+b1t3Hno4DEAE=',\n",
       "   'md5_hash': 'R6DPd2H6mue/Ccl5ox5yQw=='},\n",
       "  {'name': 'Total_Kjeldahl_Nitrogen_Concentration',\n",
       "   'filename': 'public/layers/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1167714752,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:19.315000+00:00',\n",
       "   'updated': '2025-07-02T16:09:19.315000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390154',\n",
       "   'etag': 'CPGU7d3Hno4DEAE=',\n",
       "   'md5_hash': 'v/UFLQRzhK9m0vLn5SrFGg=='},\n",
       "  {'name': 'Total_Phosphorus_Concentration',\n",
       "   'filename': 'public/layers/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1750395809,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:19.700000+00:00',\n",
       "   'updated': '2025-07-02T16:09:19.700000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390214',\n",
       "   'etag': 'CIPMhN7Hno4DEAE=',\n",
       "   'md5_hash': 't4UNBqzoMt/G0+35LYsbBA=='},\n",
       "  {'name': 'Total_Suspended_Solids_Concentration',\n",
       "   'filename': 'public/layers/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1185797094,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:20.078000+00:00',\n",
       "   'updated': '2025-07-02T16:09:20.078000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390283',\n",
       "   'etag': 'CJ/Ym97Hno4DEAE=',\n",
       "   'md5_hash': '9nibBDGImHP+Bf2tWtwu0w=='},\n",
       "  {'name': 'Total_Zinc_Concentration',\n",
       "   'filename': 'public/layers/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 1335807449,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:20.498000+00:00',\n",
       "   'updated': '2025-07-02T16:09:20.498000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390375',\n",
       "   'etag': 'CJOjtd7Hno4DEAE=',\n",
       "   'md5_hash': 'gEaKk/f+l5bm73JpdLbEOQ=='},\n",
       "  {'name': 'Traffic',\n",
       "   'filename': 'public/layers/raster/Traffic/Traffic.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/Traffic/Traffic.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 2055252928,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:20.883000+00:00',\n",
       "   'updated': '2025-07-02T16:09:20.883000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390429',\n",
       "   'etag': 'CInwzN7Hno4DEAE=',\n",
       "   'md5_hash': 'H6kt5a7DXu7CU7bb7gYoTg=='},\n",
       "  {'name': 'copper_concentration_ug_per_L',\n",
       "   'filename': 'public/layers/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.tif',\n",
       "   'url': 'https://storage.googleapis.com/swhm_data/public/layers/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.tif',\n",
       "   'type': 'raster',\n",
       "   'format': 'GeoTIFF',\n",
       "   'size_bytes': 8159008104,\n",
       "   'content_type': 'image/tiff',\n",
       "   'created': '2025-07-02T16:09:21.619000+00:00',\n",
       "   'updated': '2025-07-02T16:09:21.619000+00:00',\n",
       "   'discovered_at': '2025-07-02T20:33:14.390489',\n",
       "   'etag': 'CM7W+N7Hno4DEAE=',\n",
       "   'md5_hash': None}],\n",
       " 'total_items': 25}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df5d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = CatalogGenerator(crawl_data)\n",
    "\n",
    "stac_items = generator.generate_all_stac_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "210c1222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: swhm_data\n",
      "Starting bucket crawl...\n",
      "Crawling bucket 'swhm_data' with prefix 'public/layers/'...\n",
      "Found raster: Age_of_Imperviousness\n",
      "Found raster: Flow_Duration_Index\n",
      "Found raster: HSPF_Land_Cover_Type\n",
      "Found raster: Hydrologic_Response_Units\n",
      "Found raster: Imperviousness\n",
      "Found raster: Land_Cover\n",
      "Found raster: Land_Use\n",
      "Found raster: Population_Density\n",
      "Found raster: Precipitation_mm\n",
      "Found raster: Runoff_mm\n",
      "Found raster: Slope\n",
      "Found raster: Slope_Categories\n",
      "Found raster: Soils\n",
      "Found raster: Total_Copper_Concentration\n",
      "Found raster: Total_Kjeldahl_Nitrogen_Concentration\n",
      "Found raster: Total_Phosphorus_Concentration\n",
      "Found raster: Total_Suspended_Solids_Concentration\n",
      "Found raster: Total_Zinc_Concentration\n",
      "Found raster: Traffic\n",
      "Found raster: copper_concentration_ug_per_L\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: cig_grid_wgs\n",
      "Found vector: cig_grid_wgs\n",
      "Found vector: collection\n",
      "Processed 48 objects from bucket\n",
      "Found 5 vectors and 20 rasters\n",
      "Generating STAC items...\n",
      "\n",
      "Saving catalog files...\n",
      "Saved: catalog/catalog.json\n",
      "Saved: catalog/vectors/collection.json\n",
      "Saved: catalog/rasters/collection.json\n",
      "Saving individual STAC items...\n",
      "Saved: catalog/vectors/items/PugetSoundWA.json\n",
      "Saved: catalog/vectors/items/PugetSoundWA.json\n",
      "Saved: catalog/vectors/items/cig_grid_wgs.json\n",
      "Saved: catalog/vectors/items/cig_grid_wgs.json\n",
      "Saved: catalog/vectors/items/collection.json\n",
      "Saved: catalog/rasters/items/Age_of_Imperviousness.json\n",
      "Saved: catalog/rasters/items/Flow_Duration_Index.json\n",
      "Saved: catalog/rasters/items/HSPF_Land_Cover_Type.json\n",
      "Saved: catalog/rasters/items/Hydrologic_Response_Units.json\n",
      "Saved: catalog/rasters/items/Imperviousness.json\n",
      "Saved: catalog/rasters/items/Land_Cover.json\n",
      "Saved: catalog/rasters/items/Land_Use.json\n",
      "Saved: catalog/rasters/items/Population_Density.json\n",
      "Saved: catalog/rasters/items/Precipitation_mm.json\n",
      "Saved: catalog/rasters/items/Runoff_mm.json\n",
      "Saved: catalog/rasters/items/Slope.json\n",
      "Saved: catalog/rasters/items/Slope_Categories.json\n",
      "Saved: catalog/rasters/items/Soils.json\n",
      "Saved: catalog/rasters/items/Total_Copper_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Phosphorus_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Suspended_Solids_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Zinc_Concentration.json\n",
      "Saved: catalog/rasters/items/Traffic.json\n",
      "Saved: catalog/rasters/items/copper_concentration_ug_per_L.json\n",
      "Saved: catalog/crawl_summary.json\n",
      "\n",
      "✅ Catalog generation complete!\n",
      "   - Master catalog: catalog/catalog.json\n",
      "   - Vector collection: catalog/vectors/collection.json\n",
      "   - Raster collection: catalog/rasters/collection.json\n",
      "   - Vector items: 5 items in catalog/vectors/items/\n",
      "   - Raster items: 20 items in catalog/rasters/items/\n",
      "   - Crawl summary: catalog/crawl_summary.json\n",
      "\n",
      "📁 Generated directory structure:\n",
      "   catalog/\n",
      "   ├── catalog.json\n",
      "   ├── crawl_summary.json\n",
      "   ├── vectors/\n",
      "   │   ├── collection.json\n",
      "   │   └── items/\n",
      "   │       └── [individual vector items].json\n",
      "   └── rasters/\n",
      "       ├── collection.json\n",
      "       └── items/\n",
      "           └── [individual raster items].json\n",
      "\n",
      "💡 If you encountered authentication errors:\n",
      "   1. Install: pip install google-cloud-storage\n",
      "   2. Set up authentication:\n",
      "      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\n",
      "      - Or use: gcloud auth application-default login\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to crawl bucket and generate catalog.\"\"\"\n",
    "    # Configuration - update these values for your specific bucket\n",
    "    bucket_name = \"swhm_data\"  # Just the bucket name, not the full URL\n",
    "    prefix = \"public/layers/\"  # Path prefix within the bucket\n",
    "    project_id = None  # Set your GCP project ID if needed\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = GCPBucketCrawler(bucket_name, prefix, project_id)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    print(\"Starting bucket crawl...\")\n",
    "    crawl_data = crawler.crawl_bucket()\n",
    "    \n",
    "    print(f\"Found {len(crawl_data['vectors'])} vectors and {len(crawl_data['rasters'])} rasters\")\n",
    "    \n",
    "    # Generate collections and catalog\n",
    "    generator = CatalogGenerator(crawl_data)\n",
    "    \n",
    "    # Generate all STAC items\n",
    "    print(\"Generating STAC items...\")\n",
    "    stac_items = generator.generate_all_stac_items()\n",
    "    \n",
    "    # Generate collections\n",
    "    vector_collection = generator.generate_vector_collection()\n",
    "    raster_collection = generator.generate_raster_collection()\n",
    "    \n",
    "    # Generate master catalog\n",
    "    master_catalog = generator.generate_master_catalog(vector_collection, raster_collection)\n",
    "    \n",
    "    # Save all files\n",
    "    print(\"\\nSaving catalog files...\")\n",
    "    \n",
    "    # Save master catalog\n",
    "    save_json(master_catalog, \"catalog/catalog.json\")\n",
    "    \n",
    "    # Save collections\n",
    "    save_json(vector_collection, \"catalog/vectors/collection.json\")\n",
    "    save_json(raster_collection, \"catalog/rasters/collection.json\")\n",
    "    \n",
    "    # Save individual STAC items\n",
    "    print(\"Saving individual STAC items...\")\n",
    "    \n",
    "    # Save vector items\n",
    "    for item in stac_items['vector_items']:\n",
    "        item_path = f\"catalog/vectors/items/{item['id']}.json\"\n",
    "        save_json(item, item_path)\n",
    "    \n",
    "    # Save raster items\n",
    "    for item in stac_items['raster_items']:\n",
    "        item_path = f\"catalog/rasters/items/{item['id']}.json\"\n",
    "        save_json(item, item_path)\n",
    "    \n",
    "    # Save summary with enhanced metadata\n",
    "    summary = {\n",
    "        \"crawl_summary\": {\n",
    "            \"bucket_name\": bucket_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"crawl_time\": datetime.now().isoformat(),\n",
    "            \"total_items\": crawl_data['total_items'],\n",
    "            \"vectors_found\": len(crawl_data['vectors']),\n",
    "            \"rasters_found\": len(crawl_data['rasters']),\n",
    "            \"stac_items_generated\": len(stac_items['vector_items']) + len(stac_items['raster_items'])\n",
    "        },\n",
    "        \"discovered_vectors\": crawl_data['vectors'],\n",
    "        \"discovered_rasters\": crawl_data['rasters'],\n",
    "        \"stac_structure\": {\n",
    "            \"catalog\": \"catalog/catalog.json\",\n",
    "            \"vector_collection\": \"catalog/vectors/collection.json\",\n",
    "            \"raster_collection\": \"catalog/rasters/collection.json\",\n",
    "            \"vector_items\": [f\"catalog/vectors/items/{item['id']}.json\" for item in stac_items['vector_items']],\n",
    "            \"raster_items\": [f\"catalog/rasters/items/{item['id']}.json\" for item in stac_items['raster_items']]\n",
    "        }\n",
    "    }\n",
    "    save_json(summary, \"catalog/crawl_summary.json\")\n",
    "    \n",
    "    print(f\"\\n✅ Catalog generation complete!\")\n",
    "    print(f\"   - Master catalog: catalog/catalog.json\")\n",
    "    print(f\"   - Vector collection: catalog/vectors/collection.json\")\n",
    "    print(f\"   - Raster collection: catalog/rasters/collection.json\")\n",
    "    print(f\"   - Vector items: {len(stac_items['vector_items'])} items in catalog/vectors/items/\")\n",
    "    print(f\"   - Raster items: {len(stac_items['raster_items'])} items in catalog/rasters/items/\")\n",
    "    print(f\"   - Crawl summary: catalog/crawl_summary.json\")\n",
    "    \n",
    "    # Print directory structure\n",
    "    print(f\"\\n📁 Generated directory structure:\")\n",
    "    print(f\"   catalog/\")\n",
    "    print(f\"   ├── catalog.json\")\n",
    "    print(f\"   ├── crawl_summary.json\")\n",
    "    print(f\"   ├── vectors/\")\n",
    "    print(f\"   │   ├── collection.json\")\n",
    "    print(f\"   │   └── items/\")\n",
    "    print(f\"   │       └── [individual vector items].json\")\n",
    "    print(f\"   └── rasters/\")\n",
    "    print(f\"       ├── collection.json\")\n",
    "    print(f\"       └── items/\")\n",
    "    print(f\"           └── [individual raster items].json\")\n",
    "    \n",
    "    # Print authentication help if needed\n",
    "    print(f\"\\n💡 If you encountered authentication errors:\")\n",
    "    print(f\"   1. Install: pip install google-cloud-storage\")\n",
    "    print(f\"   2. Set up authentication:\")\n",
    "    print(f\"      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\")\n",
    "    print(f\"      - Or use: gcloud auth application-default login\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bfb5bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: swhm_data\n",
      "Starting bucket crawl...\n",
      "Crawling bucket 'swhm_data' with prefix 'public/layers/'...\n",
      "Found raster: Age_of_Imperviousness\n",
      "Found raster: Flow_Duration_Index\n",
      "Found raster: HSPF_Land_Cover_Type\n",
      "Found raster: Hydrologic_Response_Units\n",
      "Found raster: Imperviousness\n",
      "Found raster: Land_Cover\n",
      "Found raster: Land_Use\n",
      "Found raster: Population_Density\n",
      "Found raster: Precipitation_mm\n",
      "Found raster: Runoff_mm\n",
      "Found raster: Slope\n",
      "Found raster: Slope_Categories\n",
      "Found raster: Soils\n",
      "Found raster: Total_Copper_Concentration\n",
      "Found raster: Total_Kjeldahl_Nitrogen_Concentration\n",
      "Found raster: Total_Phosphorus_Concentration\n",
      "Found raster: Total_Suspended_Solids_Concentration\n",
      "Found raster: Total_Zinc_Concentration\n",
      "Found raster: Traffic\n",
      "Found raster: copper_concentration_ug_per_L\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: cig_grid_wgs\n",
      "Found vector: cig_grid_wgs\n",
      "Found vector: collection\n",
      "Processed 48 objects from bucket\n",
      "Found 5 vectors and 20 rasters\n",
      "Generating STAC items...\n",
      "\n",
      "Saving catalog files...\n",
      "Saved: catalog/catalog.json\n",
      "Saved: catalog/vectors/collection.json\n",
      "Saved: catalog/rasters/collection.json\n",
      "Saving individual STAC items...\n",
      "Saved: catalog/vectors/items/PugetSoundWA.json\n",
      "Saved: catalog/vectors/items/PugetSoundWA.json\n",
      "Saved: catalog/vectors/items/cig_grid_wgs.json\n",
      "Saved: catalog/vectors/items/cig_grid_wgs.json\n",
      "Saved: catalog/vectors/items/collection.json\n",
      "Saved: catalog/rasters/items/Age_of_Imperviousness.json\n",
      "Saved: catalog/rasters/items/Flow_Duration_Index.json\n",
      "Saved: catalog/rasters/items/HSPF_Land_Cover_Type.json\n",
      "Saved: catalog/rasters/items/Hydrologic_Response_Units.json\n",
      "Saved: catalog/rasters/items/Imperviousness.json\n",
      "Saved: catalog/rasters/items/Land_Cover.json\n",
      "Saved: catalog/rasters/items/Land_Use.json\n",
      "Saved: catalog/rasters/items/Population_Density.json\n",
      "Saved: catalog/rasters/items/Precipitation_mm.json\n",
      "Saved: catalog/rasters/items/Runoff_mm.json\n",
      "Saved: catalog/rasters/items/Slope.json\n",
      "Saved: catalog/rasters/items/Slope_Categories.json\n",
      "Saved: catalog/rasters/items/Soils.json\n",
      "Saved: catalog/rasters/items/Total_Copper_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Phosphorus_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Suspended_Solids_Concentration.json\n",
      "Saved: catalog/rasters/items/Total_Zinc_Concentration.json\n",
      "Saved: catalog/rasters/items/Traffic.json\n",
      "Saved: catalog/rasters/items/copper_concentration_ug_per_L.json\n",
      "Saved: catalog/crawl_summary.json\n",
      "\n",
      "✅ Catalog generation complete!\n",
      "   - Master catalog: catalog/catalog.json\n",
      "   - Vector collection: catalog/vectors/collection.json\n",
      "   - Raster collection: catalog/rasters/collection.json\n",
      "   - Vector items: 5 items in catalog/vectors/items/\n",
      "   - Raster items: 20 items in catalog/rasters/items/\n",
      "   - Crawl summary: catalog/crawl_summary.json\n",
      "\n",
      "📁 Generated directory structure:\n",
      "   catalog/\n",
      "   ├── catalog.json\n",
      "   ├── crawl_summary.json\n",
      "   ├── vectors/\n",
      "   │   ├── collection.json\n",
      "   │   └── items/\n",
      "   │       └── [individual vector items].json\n",
      "   └── rasters/\n",
      "       ├── collection.json\n",
      "       └── items/\n",
      "           └── [individual raster items].json\n",
      "\n",
      "💡 If you encountered authentication errors:\n",
      "   1. Install: pip install google-cloud-storage\n",
      "   2. Set up authentication:\n",
      "      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\n",
      "      - Or use: gcloud auth application-default login\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "id": "c43e66f9",
   "metadata": {},
   "outputs": [],
   "source": "class GCSUploader:\n    \"\"\"\n    Handles uploading STAC catalog files to Google Cloud Storage.\n    Supports both gsutil and native GCS client approaches.\n    Sets Cache-Control headers to prevent browser caching of JSON files.\n    \"\"\"\n    \n    def __init__(self, bucket_name: str, project_id: Optional[str] = None):\n        \"\"\"\n        Initialize the GCS uploader.\n        \n        Args:\n            bucket_name: Name of the GCS bucket\n            project_id: GCP project ID (optional)\n        \"\"\"\n        self.bucket_name = bucket_name\n        self.project_id = project_id\n        self.use_gsutil = shutil.which(\"gsutil\") is not None\n        \n        # Try to initialize GCS client as fallback\n        self.client = None\n        self.bucket = None\n        if not self.use_gsutil:\n            try:\n                if project_id:\n                    self.client = storage.Client(project=project_id)\n                else:\n                    self.client = storage.Client()\n                self.bucket = self.client.bucket(bucket_name)\n                print(\"Using native GCS client for uploads\")\n            except Exception as e:\n                print(f\"WARNING: Could not initialize GCS client: {e}\")\n                print(\"Upload functionality will be limited\")\n        else:\n            print(\"Using gsutil for uploads\")\n    \n    def upload_directory(self, root_dir: str, prefix: str = \"\", dry_run: bool = False) -> Dict:\n        \"\"\"\n        Upload all STAC JSON files from a directory structure to GCS.\n        \n        Args:\n            root_dir: Local directory containing STAC files\n            prefix: GCS path prefix (e.g., \"stac/\")\n            dry_run: If True, show what would be uploaded without doing it\n            \n        Returns:\n            Dictionary with upload results\n        \"\"\"\n        root_path = Path(root_dir).resolve()\n        \n        if not root_path.is_dir():\n            raise ValueError(f\"Directory does not exist: {root_path}\")\n        \n        if prefix and not prefix.endswith(\"/\"):\n            prefix += \"/\"\n        \n        print(f\"Scanning directory: {root_path}\")\n        print(f\"Target GCS location: gs://{self.bucket_name}/{prefix}\")\n        print(f\"Dry run: {dry_run}\")\n        print(\"-\" * 50)\n        \n        results = {\n            \"uploaded\": [],\n            \"skipped\": [],\n            \"failed\": [],\n            \"total_files\": 0\n        }\n        \n        # Find all JSON files to upload\n        json_files = list(root_path.rglob(\"*.json\"))\n        results[\"total_files\"] = len(json_files)\n        \n        if not json_files:\n            print(\"No JSON files found to upload\")\n            return results\n        \n        print(f\"Found {len(json_files)} JSON files to upload\")\n        print(\"-\" * 50)\n        \n        for json_file in json_files:\n            try:\n                self._upload_single_file(json_file, root_path, prefix, dry_run, results)\n            except Exception as e:\n                print(f\"ERROR: Failed to upload {json_file}: {e}\")\n                results[\"failed\"].append({\n                    \"file\": str(json_file),\n                    \"error\": str(e)\n                })\n        \n        # Print summary\n        self._print_summary(results)\n        return results\n    \n    def _upload_single_file(self, file_path: Path, root_path: Path, prefix: str, \n                          dry_run: bool, results: Dict):\n        \"\"\"Upload a single file to GCS.\"\"\"\n        relative_path = file_path.relative_to(root_path)\n        gcs_path = f\"{prefix}{relative_path.as_posix()}\"\n        gcs_url = f\"gs://{self.bucket_name}/{gcs_path}\"\n        \n        print(f\"📄 {relative_path}\")\n        print(f\"   → {gcs_url}\")\n        \n        if dry_run:\n            print(\"   → DRY RUN: Would upload with Cache-Control headers\")\n            results[\"skipped\"].append(str(file_path))\n            return\n        \n        # Try gsutil first, fall back to native client\n        success = False\n        \n        if self.use_gsutil:\n            success = self._upload_with_gsutil(file_path, gcs_url)\n        \n        if not success and self.client:\n            success = self._upload_with_client(file_path, gcs_path)\n        \n        if success:\n            print(\"   ✅ Upload successful with Cache-Control headers\")\n            results[\"uploaded\"].append(str(file_path))\n        else:\n            print(\"   ❌ Upload failed\")\n            results[\"failed\"].append({\n                \"file\": str(file_path),\n                \"error\": \"All upload methods failed\"\n            })\n        \n        print()\n    \n    def _upload_with_gsutil(self, file_path: Path, gcs_url: str) -> bool:\n        \"\"\"Upload using gsutil command with Cache-Control headers.\"\"\"\n        try:\n            # Set Cache-Control header to prevent browser caching\n            # This forces browsers to re-fetch the STAC catalog files on each request\n            cmd = [\n                \"gsutil\", \n                \"-h\", \"Cache-Control:no-cache, no-store, must-revalidate\",\n                \"-h\", \"Content-Type:application/json\",\n                \"cp\", \n                str(file_path), \n                gcs_url\n            ]\n            result = subprocess.run(\n                cmd, \n                capture_output=True, \n                text=True, \n                check=True,\n                timeout=30\n            )\n            return True\n        except subprocess.CalledProcessError as e:\n            print(f\"   gsutil error: {e.stderr.strip()}\")\n            return False\n        except subprocess.TimeoutExpired:\n            print(\"   gsutil timeout\")\n            return False\n        except Exception as e:\n            print(f\"   gsutil exception: {e}\")\n            return False\n    \n    def _upload_with_client(self, file_path: Path, gcs_path: str) -> bool:\n        \"\"\"Upload using native GCS client with Cache-Control headers.\"\"\"\n        try:\n            blob = self.bucket.blob(gcs_path)\n            \n            # Set Cache-Control header to prevent browser caching\n            blob.cache_control = \"no-cache, no-store, must-revalidate\"\n            blob.content_type = \"application/json\"\n            \n            blob.upload_from_filename(str(file_path))\n            return True\n        except Exception as e:\n            print(f\"   GCS client error: {e}\")\n            return False\n    \n    def _print_summary(self, results: Dict):\n        \"\"\"Print upload summary.\"\"\"\n        print(\"=\" * 50)\n        print(\"📊 UPLOAD SUMMARY\")\n        print(\"=\" * 50)\n        print(f\"Total files found: {results['total_files']}\")\n        print(f\"Successfully uploaded: {len(results['uploaded'])}\")\n        print(f\"Skipped (dry run): {len(results['skipped'])}\")\n        print(f\"Failed: {len(results['failed'])}\")\n        \n        if results['failed']:\n            print(f\"\\n❌ Failed uploads:\")\n            for failure in results['failed']:\n                if isinstance(failure, dict):\n                    print(f\"   • {failure['file']}: {failure['error']}\")\n                else:\n                    print(f\"   • {failure}\")\n        \n        if results['uploaded']:\n            print(f\"\\n✅ Upload complete! Files available at:\")\n            print(f\"   gs://{self.bucket_name}/\")\n            print(f\"\\n🔄 Cache-Control headers set to 'no-cache, no-store, must-revalidate'\")\n            print(f\"   This ensures STAC Browser always fetches the latest catalog data\")\n\n\ndef upload_stac_catalog(root_dir: str, bucket_name: str, prefix: str = \"\", \n                       dry_run: bool = False, project_id: Optional[str] = None) -> Dict:\n    \"\"\"\n    Convenience function to upload STAC catalog files to GCS with cache-busting headers.\n    \n    Args:\n        root_dir: Local directory containing STAC files\n        bucket_name: GCS bucket name\n        prefix: GCS path prefix\n        dry_run: If True, show what would be uploaded without doing it\n        project_id: GCP project ID (optional)\n        \n    Returns:\n        Dictionary with upload results\n    \"\"\"\n    uploader = GCSUploader(bucket_name, project_id)\n    return uploader.upload_directory(root_dir, prefix, dry_run)"
  },
  {
   "cell_type": "code",
   "id": "2e9dfdad",
   "metadata": {},
   "outputs": [],
   "source": "import shutil\nimport shlex\nimport subprocess\n\n# Configuration\nGCS_BUCKET = \"swhm_data\"\nGCS_PREFIX = \"public/layers/\"\nCATALOG_DIR = \"/Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog\"\n\n# Example usage with new refactored uploader\nprint(\"=== UPLOADING STAC CATALOG TO GCS ===\\n\")\n\n# Upload with dry run first to see what would be uploaded\nprint(\"1. Dry run to preview uploads:\")\nresults = upload_stac_catalog(\n    root_dir=CATALOG_DIR,\n    bucket_name=GCS_BUCKET,\n    prefix=GCS_PREFIX,\n    dry_run=True\n)\n\nprint(f\"\\n2. Actual upload:\")\n# Uncomment the line below to perform actual upload\n# results = upload_stac_catalog(\n#     root_dir=CATALOG_DIR,\n#     bucket_name=GCS_BUCKET,\n#     prefix=GCS_PREFIX,\n#     dry_run=False\n# )"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}