{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6a8b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "GCP Bucket Crawler and Catalog Generator\n",
    "Crawls a GCP storage bucket to discover vector and raster data,\n",
    "then generates collections, individual STAC items, and a comprehensive catalog.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import shutil\n",
    "import shlex\n",
    "import subprocess\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f28e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPBucketCrawler:\n",
    "    def __init__(self, bucket_name: str, prefix: str = \"\", project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the crawler with GCP bucket details.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the GCP storage bucket (e.g., 'swhm_data')\n",
    "            prefix: Prefix to filter objects (e.g., 'public/layers/')\n",
    "            project_id: GCP project ID (optional, will use default if not provided)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "        self.project_id = project_id\n",
    "        self.vectors = []\n",
    "        self.rasters = []\n",
    "        self.processed_items = set()  # Track processed items to prevent duplicates\n",
    "        \n",
    "        # Initialize the GCS client\n",
    "        try:\n",
    "            if project_id:\n",
    "                self.client = storage.Client(project=project_id)\n",
    "            else:\n",
    "                self.client = storage.Client()\n",
    "            self.bucket = self.client.bucket(bucket_name)\n",
    "            print(f\"Successfully connected to bucket: {bucket_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing GCS client: {e}\")\n",
    "            print(\"Make sure you have proper authentication set up:\")\n",
    "            print(\"1. Set GOOGLE_APPLICATION_CREDENTIALS environment variable\")\n",
    "            print(\"2. Or run 'gcloud auth application-default login'\")\n",
    "            self.client = None\n",
    "            self.bucket = None\n",
    "        \n",
    "    def crawl_bucket(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Crawl the GCP bucket to discover all vectors and rasters.\n",
    "        Returns a dictionary with discovered items.\n",
    "        \"\"\"\n",
    "        if not self.client or not self.bucket:\n",
    "            print(\"No valid GCS client available, creating sample data...\")\n",
    "            return self._create_sample_data()\n",
    "            \n",
    "        print(f\"Crawling bucket '{self.bucket_name}' with prefix '{self.prefix}'...\")\n",
    "        \n",
    "        try:\n",
    "            # List all blobs in the bucket with the specified prefix\n",
    "            blobs = self.bucket.list_blobs(prefix=self.prefix)\n",
    "            \n",
    "            blob_count = 0\n",
    "            for blob in blobs:\n",
    "                blob_count += 1\n",
    "                self._process_blob(blob)\n",
    "                \n",
    "            print(f\"Processed {blob_count} objects from bucket\")\n",
    "            print(f\"Found {len(self.vectors)} unique vectors and {len(self.rasters)} unique rasters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error crawling bucket: {e}\")\n",
    "            return self._create_sample_data()\n",
    "            \n",
    "        return {\n",
    "            'vectors': self.vectors,\n",
    "            'rasters': self.rasters,\n",
    "            'total_items': len(self.vectors) + len(self.rasters)\n",
    "        }\n",
    "    \n",
    "    def _process_blob(self, blob):\n",
    "        \"\"\"Process a single blob to determine if it's a vector or raster.\"\"\"\n",
    "        blob_name = blob.name\n",
    "        blob_path = Path(blob_name)\n",
    "        \n",
    "        # Skip directories (blobs ending with '/')\n",
    "        if blob_name.endswith('/'):\n",
    "            return\n",
    "            \n",
    "        # Check for vector files - ONLY .geojson files, NOT .json files\n",
    "        if 'vector/' in blob_name and blob_path.suffix.lower() == '.geojson':\n",
    "            self._add_vector_item(blob)\n",
    "            \n",
    "        # Check for raster files - TIFF files including .gtiff  \n",
    "        elif 'raster/' in blob_name and blob_path.suffix.lower() in ['.tiff', '.tif', '.gtiff']:\n",
    "            self._add_raster_item(blob)\n",
    "    \n",
    "    def _add_vector_item(self, blob):\n",
    "        \"\"\"Add a vector item to the collection.\"\"\"\n",
    "        blob_path = Path(blob.name)\n",
    "        item_name = blob_path.stem\n",
    "        \n",
    "        # Create unique identifier to prevent duplicates\n",
    "        item_key = f\"vector:{item_name}\"\n",
    "        if item_key in self.processed_items:\n",
    "            print(f\"Skipping duplicate vector: {item_name}\")\n",
    "            return\n",
    "        \n",
    "        # Create public URL\n",
    "        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "        \n",
    "        # Get the directory path where the STAC item should be saved\n",
    "        # e.g., public/layers/vector/PugetSoundWA/PugetSoundWA.geojson -> vector/PugetSoundWA/\n",
    "        parent_dir = blob_path.parent\n",
    "        stac_dir = str(parent_dir).replace(self.prefix, '')\n",
    "        \n",
    "        vector_item = {\n",
    "            'name': item_name,\n",
    "            'filename': blob.name,\n",
    "            'url': public_url,\n",
    "            'type': 'vector',\n",
    "            'format': 'GeoJSON',\n",
    "            'size_bytes': blob.size,\n",
    "            'content_type': blob.content_type,\n",
    "            'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "            'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "            'discovered_at': datetime.now().isoformat(),\n",
    "            'etag': blob.etag,\n",
    "            'md5_hash': blob.md5_hash,\n",
    "            'stac_dir': stac_dir  # Directory where STAC item should be saved\n",
    "        }\n",
    "        \n",
    "        self.vectors.append(vector_item)\n",
    "        self.processed_items.add(item_key)\n",
    "        print(f\"Found vector: {item_name}\")\n",
    "    \n",
    "    def _add_raster_item(self, blob):\n",
    "        \"\"\"Add a raster item to the collection.\"\"\"\n",
    "        blob_path = Path(blob.name)\n",
    "        item_name = blob_path.stem\n",
    "        \n",
    "        # Create unique identifier to prevent duplicates\n",
    "        item_key = f\"raster:{item_name}\"\n",
    "        if item_key in self.processed_items:\n",
    "            print(f\"Skipping duplicate raster: {item_name}\")\n",
    "            return\n",
    "        \n",
    "        # Create public URL\n",
    "        public_url = f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "        \n",
    "        # Get the directory path where the STAC item should be saved\n",
    "        # e.g., public/layers/raster/Age_of_Imperviousness/Age_of_Imperviousness.tif -> raster/Age_of_Imperviousness/\n",
    "        parent_dir = blob_path.parent\n",
    "        stac_dir = str(parent_dir).replace(self.prefix, '')\n",
    "        \n",
    "        raster_item = {\n",
    "            'name': item_name,\n",
    "            'filename': blob.name,\n",
    "            'url': public_url,\n",
    "            'type': 'raster',\n",
    "            'format': 'GeoTIFF',\n",
    "            'size_bytes': blob.size,\n",
    "            'content_type': blob.content_type,\n",
    "            'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "            'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "            'discovered_at': datetime.now().isoformat(),\n",
    "            'etag': blob.etag,\n",
    "            'md5_hash': blob.md5_hash,\n",
    "            'stac_dir': stac_dir  # Directory where STAC item should be saved\n",
    "        }\n",
    "        \n",
    "        self.rasters.append(raster_item)\n",
    "        self.processed_items.add(item_key)\n",
    "        print(f\"Found raster: {item_name}\")\n",
    "    \n",
    "    def get_blob_info(self, blob_name: str) -> Optional[Dict]:\n",
    "        \"\"\"Get detailed information about a specific blob.\"\"\"\n",
    "        if not self.bucket:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            blob = self.bucket.blob(blob_name)\n",
    "            if blob.exists():\n",
    "                return {\n",
    "                    'name': blob.name,\n",
    "                    'size': blob.size,\n",
    "                    'content_type': blob.content_type,\n",
    "                    'created': blob.time_created.isoformat() if blob.time_created else None,\n",
    "                    'updated': blob.updated.isoformat() if blob.updated else None,\n",
    "                    'etag': blob.etag,\n",
    "                    'md5_hash': blob.md5_hash,\n",
    "                    'public_url': f\"https://storage.googleapis.com/{self.bucket_name}/{blob.name}\"\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting blob info for {blob_name}: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def _create_sample_data(self):\n",
    "        \"\"\"Create sample data structure when bucket can't be crawled directly.\"\"\"\n",
    "        print(\"Creating sample data structure...\")\n",
    "        \n",
    "        base_url = f\"https://storage.googleapis.com/{self.bucket_name}\"\n",
    "        \n",
    "        # Sample vectors based on your structure\n",
    "        sample_vectors = [\n",
    "            {\n",
    "                'name': 'vector1',\n",
    "                'filename': f'{self.prefix}vector/vector1/vector1.geojson',\n",
    "                'url': f\"{base_url}/{self.prefix}vector/vector1/vector1.geojson\",\n",
    "                'type': 'vector',\n",
    "                'format': 'GeoJSON',\n",
    "                'size_bytes': None,\n",
    "                'content_type': 'application/geo+json',\n",
    "                'created': None,\n",
    "                'updated': None,\n",
    "                'discovered_at': datetime.now().isoformat(),\n",
    "                'etag': None,\n",
    "                'md5_hash': None,\n",
    "                'stac_dir': 'vector/vector1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Sample rasters based on your structure\n",
    "        sample_rasters = [\n",
    "            {\n",
    "                'name': 'raster1',\n",
    "                'filename': f'{self.prefix}raster/raster1/raster1.tiff',\n",
    "                'url': f\"{base_url}/{self.prefix}raster/raster1/raster1.tiff\",\n",
    "                'type': 'raster',\n",
    "                'format': 'GeoTIFF',\n",
    "                'size_bytes': None,\n",
    "                'content_type': 'image/tiff',\n",
    "                'created': None,\n",
    "                'updated': None,\n",
    "                'discovered_at': datetime.now().isoformat(),\n",
    "                'etag': None,\n",
    "                'md5_hash': None,\n",
    "                'stac_dir': 'raster/raster1'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.vectors = sample_vectors\n",
    "        self.rasters = sample_rasters\n",
    "        \n",
    "        return {\n",
    "            'vectors': self.vectors,\n",
    "            'rasters': self.rasters,\n",
    "            'total_items': len(self.vectors) + len(self.rasters)\n",
    "        }\n",
    "\n",
    "class CatalogGenerator:\n",
    "    def __init__(self, crawler_data: Dict):\n",
    "        \"\"\"Initialize with data from the crawler.\"\"\"\n",
    "        self.data = crawler_data\n",
    "        self.stac_items = []\n",
    "        \n",
    "    def generate_stac_item(self, item_data: Dict, item_type: str) -> Dict:\n",
    "        \"\"\"Generate a STAC item for vector or raster data.\"\"\"\n",
    "        item_id = item_data['name']\n",
    "        \n",
    "        # Base STAC item structure\n",
    "        stac_item = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": item_id,\n",
    "            \"properties\": {\n",
    "                \"title\": item_data['name'].replace('_', ' ').title(),\n",
    "                \"description\": f\"{item_type.title()} dataset: {item_data['name']}\",\n",
    "                \"datetime\": item_data['discovered_at'],\n",
    "                \"created\": item_data.get('created') or item_data['discovered_at'],\n",
    "                \"updated\": item_data.get('updated') or item_data['discovered_at'],\n",
    "                \"providers\": [\n",
    "                    {\n",
    "                        \"name\": \"SWHM Data\",\n",
    "                        \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                        \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            \"geometry\": None,  # Would need to extract from actual data\n",
    "            \"bbox\": None,  # Would need to calculate from geometry/bounds\n",
    "            \"assets\": {},\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": f\"./{item_id}.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"../collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"collection\",\n",
    "                    \"href\": \"../collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"../../catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add assets based on type\n",
    "        if item_type == 'vector':\n",
    "            stac_item[\"assets\"][\"data\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"application/geo+json\",\n",
    "                \"title\": \"GeoJSON data\",\n",
    "                \"description\": \"Vector data in GeoJSON format\",\n",
    "                \"roles\": [\"data\"],\n",
    "                \"file:size\": item_data.get('size_bytes'),\n",
    "                \"file:checksum\": item_data.get('md5_hash')\n",
    "            }\n",
    "        elif item_type == 'raster':\n",
    "            stac_item[\"assets\"][\"data\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"image/tiff; application=geotiff\",\n",
    "                \"title\": \"GeoTIFF data\",\n",
    "                \"description\": \"Raster data in GeoTIFF format\",\n",
    "                \"roles\": [\"data\"],\n",
    "                \"file:size\": item_data.get('size_bytes'),\n",
    "                \"file:checksum\": item_data.get('md5_hash')\n",
    "            }\n",
    "            \n",
    "            # Add COG asset if it's a Cloud Optimized GeoTIFF\n",
    "            stac_item[\"assets\"][\"cog\"] = {\n",
    "                \"href\": item_data['url'],\n",
    "                \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                \"title\": \"Cloud Optimized GeoTIFF\",\n",
    "                \"description\": \"Cloud Optimized GeoTIFF for web access\",\n",
    "                \"roles\": [\"data\", \"overview\"]\n",
    "            }\n",
    "        \n",
    "        # Only add metadata and thumbnail assets if they exist (no placeholders)\n",
    "        # This prevents STAC Browser from trying to load non-existent resources\n",
    "        \n",
    "        return stac_item\n",
    "    \n",
    "    def generate_vector_collection(self) -> Dict:\n",
    "        \"\"\"Generate a vector collection with individual STAC items.\"\"\"\n",
    "        collection = {\n",
    "            \"type\": \"Collection\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-vector\",\n",
    "            \"title\": \"SWHM Vector Collection\",\n",
    "            \"description\": \"Collection of vector datasets from SWHM data bucket\",\n",
    "            \"keywords\": [\"vector\", \"geojson\", \"swhm\"],\n",
    "            \"license\": \"proprietary\",\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\n",
    "                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n",
    "                },\n",
    "                \"temporal\": {\n",
    "                    \"interval\": [[None, None]]\n",
    "                }\n",
    "            },\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"./collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"../catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"../catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ],\n",
    "            \"item_assets\": {\n",
    "                \"data\": {\n",
    "                    \"type\": \"application/geo+json\",\n",
    "                    \"title\": \"GeoJSON data\",\n",
    "                    \"roles\": [\"data\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add links to individual items - crawler already handles deduplication\n",
    "        for vector in self.data['vectors']:\n",
    "            item_id = vector['name']\n",
    "            # Create relative path to item in its own directory\n",
    "            # e.g., for item in vector/PugetSoundWA/ -> ../PugetSoundWA/PugetSoundWA.json\n",
    "            item_dir = vector['stac_dir'].split('/')[-1]  # Get just the item directory name\n",
    "            collection[\"links\"].append({\n",
    "                \"rel\": \"item\",\n",
    "                \"href\": f\"../{item_dir}/{item_id}.json\",\n",
    "                \"type\": \"application/json\",\n",
    "                \"title\": vector['name'].replace('_', ' ').title()\n",
    "            })\n",
    "            \n",
    "        return collection\n",
    "    \n",
    "    def generate_raster_collection(self) -> Dict:\n",
    "        \"\"\"Generate a raster collection with individual STAC items.\"\"\"\n",
    "        collection = {\n",
    "            \"type\": \"Collection\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-raster\",\n",
    "            \"title\": \"SWHM Raster Collection\",\n",
    "            \"description\": \"Collection of raster datasets from SWHM data bucket\",\n",
    "            \"keywords\": [\"raster\", \"geotiff\", \"swhm\"],\n",
    "            \"license\": \"proprietary\",\n",
    "            \"extent\": {\n",
    "                \"spatial\": {\n",
    "                    \"bbox\": [[-180, -90, 180, 90]]  # Global bbox - update with actual bounds\n",
    "                },\n",
    "                \"temporal\": {\n",
    "                    \"interval\": [[None, None]]\n",
    "                }\n",
    "            },\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"./collection.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"parent\",\n",
    "                    \"href\": \"../catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"root\",\n",
    "                    \"href\": \"../catalog.json\",\n",
    "                    \"type\": \"application/json\"\n",
    "                }\n",
    "            ],\n",
    "            \"item_assets\": {\n",
    "                \"data\": {\n",
    "                    \"type\": \"image/tiff; application=geotiff\",\n",
    "                    \"title\": \"GeoTIFF data\",\n",
    "                    \"roles\": [\"data\"]\n",
    "                },\n",
    "                \"cog\": {\n",
    "                    \"type\": \"image/tiff; application=geotiff; profile=cloud-optimized\",\n",
    "                    \"title\": \"Cloud Optimized GeoTIFF\",\n",
    "                    \"roles\": [\"data\", \"overview\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add links to individual items - crawler already handles deduplication\n",
    "        for raster in self.data['rasters']:\n",
    "            item_id = raster['name']\n",
    "            # Create relative path to item in its own directory\n",
    "            # e.g., for item in raster/Age_of_Imperviousness/ -> ../Age_of_Imperviousness/Age_of_Imperviousness.json\n",
    "            item_dir = raster['stac_dir'].split('/')[-1]  # Get just the item directory name\n",
    "            collection[\"links\"].append({\n",
    "                \"rel\": \"item\",\n",
    "                \"href\": f\"../{item_dir}/{item_id}.json\",\n",
    "                \"type\": \"application/json\",\n",
    "                \"title\": raster['name'].replace('_', ' ').title()\n",
    "            })\n",
    "            \n",
    "        return collection\n",
    "    \n",
    "    def generate_master_catalog(self, vector_collection: Dict, raster_collection: Dict) -> Dict:\n",
    "        \"\"\"Generate the master catalog containing all collections.\"\"\"\n",
    "        catalog = {\n",
    "            \"type\": \"Catalog\",\n",
    "            \"stac_version\": \"1.0.0\",\n",
    "            \"id\": \"swhm-data-catalog\",\n",
    "            \"title\": \"SWHM Data Catalog\",\n",
    "            \"description\": \"Master catalog for SWHM vector and raster datasets\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"updated\": datetime.now().isoformat(),\n",
    "            \"keywords\": [\"swhm\", \"vector\", \"raster\", \"geospatial\"],\n",
    "            \"providers\": [\n",
    "                {\n",
    "                    \"name\": \"SWHM Data\",\n",
    "                    \"roles\": [\"producer\", \"processor\", \"host\"],\n",
    "                    \"url\": \"https://storage.googleapis.com/swhm_data/\"\n",
    "                }\n",
    "            ],\n",
    "            \"links\": [\n",
    "                {\n",
    "                    \"rel\": \"self\",\n",
    "                    \"href\": \"./catalog.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"SWHM Data Catalog\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"child\",\n",
    "                    \"href\": \"./vector/collection.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"Vector Collection\"\n",
    "                },\n",
    "                {\n",
    "                    \"rel\": \"child\",\n",
    "                    \"href\": \"./raster/collection.json\",\n",
    "                    \"type\": \"application/json\",\n",
    "                    \"title\": \"Raster Collection\"\n",
    "                }\n",
    "            ],\n",
    "            \"conformsTo\": [\n",
    "                \"https://api.stacspec.org/v1.0.0/core\",\n",
    "                \"https://api.stacspec.org/v1.0.0/collections\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return catalog\n",
    "    \n",
    "    def generate_all_stac_items(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Generate all STAC items for vectors and rasters.\"\"\"\n",
    "        vector_items = []\n",
    "        raster_items = []\n",
    "        \n",
    "        # Generate vector items - crawler already handles deduplication\n",
    "        for vector in self.data['vectors']:\n",
    "            stac_item = self.generate_stac_item(vector, 'vector')\n",
    "            vector_items.append(stac_item)\n",
    "            \n",
    "        # Generate raster items - crawler already handles deduplication\n",
    "        for raster in self.data['rasters']:\n",
    "            stac_item = self.generate_stac_item(raster, 'raster')\n",
    "            raster_items.append(stac_item)\n",
    "            \n",
    "        return {\n",
    "            'vector_items': vector_items,\n",
    "            'raster_items': raster_items\n",
    "        }\n",
    "\n",
    "def save_json(data: Dict, filepath: str):\n",
    "    \"\"\"Save data to JSON file with pretty formatting.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43e66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCSUploader:\n",
    "    \"\"\"\n",
    "    Handles uploading STAC catalog files to Google Cloud Storage.\n",
    "    Supports both gsutil and native GCS client approaches.\n",
    "    Sets Cache-Control headers to prevent browser caching of JSON files.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bucket_name: str, project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the GCS uploader.\n",
    "        \n",
    "        Args:\n",
    "            bucket_name: Name of the GCS bucket\n",
    "            project_id: GCP project ID (optional)\n",
    "        \"\"\"\n",
    "        self.bucket_name = bucket_name\n",
    "        self.project_id = project_id\n",
    "        self.use_gsutil = shutil.which(\"gsutil\") is not None\n",
    "        \n",
    "        # Try to initialize GCS client as fallback\n",
    "        self.client = None\n",
    "        self.bucket = None\n",
    "        if not self.use_gsutil:\n",
    "            try:\n",
    "                if project_id:\n",
    "                    self.client = storage.Client(project=project_id)\n",
    "                else:\n",
    "                    self.client = storage.Client()\n",
    "                self.bucket = self.client.bucket(bucket_name)\n",
    "                print(\"Using native GCS client for uploads\")\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: Could not initialize GCS client: {e}\")\n",
    "                print(\"Upload functionality will be limited\")\n",
    "        else:\n",
    "            print(\"Using gsutil for uploads\")\n",
    "    \n",
    "    def upload_directory(self, root_dir: str, prefix: str = \"\", dry_run: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Upload all STAC JSON files from a directory structure to GCS.\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Local directory containing STAC files\n",
    "            prefix: GCS path prefix (e.g., \"stac/\")\n",
    "            dry_run: If True, show what would be uploaded without doing it\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with upload results\n",
    "        \"\"\"\n",
    "        root_path = Path(root_dir).resolve()\n",
    "        \n",
    "        if not root_path.is_dir():\n",
    "            raise ValueError(f\"Directory does not exist: {root_path}\")\n",
    "        \n",
    "        if prefix and not prefix.endswith(\"/\"):\n",
    "            prefix += \"/\"\n",
    "        \n",
    "        print(f\"Scanning directory: {root_path}\")\n",
    "        print(f\"Target GCS location: gs://{self.bucket_name}/{prefix}\")\n",
    "        print(f\"Dry run: {dry_run}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results = {\n",
    "            \"uploaded\": [],\n",
    "            \"skipped\": [],\n",
    "            \"failed\": [],\n",
    "            \"total_files\": 0\n",
    "        }\n",
    "        \n",
    "        # Find all JSON files to upload\n",
    "        json_files = list(root_path.rglob(\"*.json\"))\n",
    "        results[\"total_files\"] = len(json_files)\n",
    "        \n",
    "        if not json_files:\n",
    "            print(\"No JSON files found to upload\")\n",
    "            return results\n",
    "        \n",
    "        print(f\"Found {len(json_files)} JSON files to upload\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                self._upload_single_file(json_file, root_path, prefix, dry_run, results)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Failed to upload {json_file}: {e}\")\n",
    "                results[\"failed\"].append({\n",
    "                    \"file\": str(json_file),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "        \n",
    "        # Print summary\n",
    "        self._print_summary(results)\n",
    "        return results\n",
    "    \n",
    "    def _upload_single_file(self, file_path: Path, root_path: Path, prefix: str, \n",
    "                          dry_run: bool, results: Dict):\n",
    "        \"\"\"Upload a single file to GCS.\"\"\"\n",
    "        relative_path = file_path.relative_to(root_path)\n",
    "        gcs_path = f\"{prefix}{relative_path.as_posix()}\"\n",
    "        gcs_url = f\"gs://{self.bucket_name}/{gcs_path}\"\n",
    "        \n",
    "        print(f\"üìÑ {relative_path}\")\n",
    "        print(f\"   ‚Üí {gcs_url}\")\n",
    "        \n",
    "        if dry_run:\n",
    "            print(\"   ‚Üí DRY RUN: Would upload with Cache-Control headers\")\n",
    "            results[\"skipped\"].append(str(file_path))\n",
    "            return\n",
    "        \n",
    "        # Try gsutil first, fall back to native client\n",
    "        success = False\n",
    "        \n",
    "        if self.use_gsutil:\n",
    "            success = self._upload_with_gsutil(file_path, gcs_url)\n",
    "        \n",
    "        if not success and self.client:\n",
    "            success = self._upload_with_client(file_path, gcs_path)\n",
    "        \n",
    "        if success:\n",
    "            print(\"   ‚úÖ Upload successful with Cache-Control headers\")\n",
    "            results[\"uploaded\"].append(str(file_path))\n",
    "        else:\n",
    "            print(\"   ‚ùå Upload failed\")\n",
    "            results[\"failed\"].append({\n",
    "                \"file\": str(file_path),\n",
    "                \"error\": \"All upload methods failed\"\n",
    "            })\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    def _upload_with_gsutil(self, file_path: Path, gcs_url: str) -> bool:\n",
    "        \"\"\"Upload using gsutil command with Cache-Control headers.\"\"\"\n",
    "        try:\n",
    "            # Set Cache-Control header to prevent browser caching\n",
    "            # This forces browsers to re-fetch the STAC catalog files on each request\n",
    "            cmd = [\n",
    "                \"gsutil\", \n",
    "                \"-h\", \"Cache-Control:no-cache, no-store, must-revalidate\",\n",
    "                \"-h\", \"Content-Type:application/json\",\n",
    "                \"cp\", \n",
    "                str(file_path), \n",
    "                gcs_url\n",
    "            ]\n",
    "            result = subprocess.run(\n",
    "                cmd, \n",
    "                capture_output=True, \n",
    "                text=True, \n",
    "                check=True,\n",
    "                timeout=30\n",
    "            )\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"   gsutil error: {e.stderr.strip()}\")\n",
    "            return False\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"   gsutil timeout\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"   gsutil exception: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _upload_with_client(self, file_path: Path, gcs_path: str) -> bool:\n",
    "        \"\"\"Upload using native GCS client with Cache-Control headers.\"\"\"\n",
    "        try:\n",
    "            blob = self.bucket.blob(gcs_path)\n",
    "            \n",
    "            # Set Cache-Control header to prevent browser caching\n",
    "            blob.cache_control = \"no-cache, no-store, must-revalidate\"\n",
    "            blob.content_type = \"application/json\"\n",
    "            \n",
    "            blob.upload_from_filename(str(file_path))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   GCS client error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _print_summary(self, results: Dict):\n",
    "        \"\"\"Print upload summary.\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"üìä UPLOAD SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total files found: {results['total_files']}\")\n",
    "        print(f\"Successfully uploaded: {len(results['uploaded'])}\")\n",
    "        print(f\"Skipped (dry run): {len(results['skipped'])}\")\n",
    "        print(f\"Failed: {len(results['failed'])}\")\n",
    "        \n",
    "        if results['failed']:\n",
    "            print(f\"\\n‚ùå Failed uploads:\")\n",
    "            for failure in results['failed']:\n",
    "                if isinstance(failure, dict):\n",
    "                    print(f\"   ‚Ä¢ {failure['file']}: {failure['error']}\")\n",
    "                else:\n",
    "                    print(f\"   ‚Ä¢ {failure}\")\n",
    "        \n",
    "        if results['uploaded']:\n",
    "            print(f\"\\n‚úÖ Upload complete! Files available at:\")\n",
    "            print(f\"   gs://{self.bucket_name}/\")\n",
    "            print(f\"\\nüîÑ Cache-Control headers set to 'no-cache, no-store, must-revalidate'\")\n",
    "            print(f\"   This ensures STAC Browser always fetches the latest catalog data\")\n",
    "\n",
    "\n",
    "def upload_stac_catalog(root_dir: str, bucket_name: str, prefix: str = \"\", \n",
    "                       dry_run: bool = False, project_id: Optional[str] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Convenience function to upload STAC catalog files to GCS with cache-busting headers.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Local directory containing STAC files\n",
    "        bucket_name: GCS bucket name\n",
    "        prefix: GCS path prefix\n",
    "        dry_run: If True, show what would be uploaded without doing it\n",
    "        project_id: GCP project ID (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with upload results\n",
    "    \"\"\"\n",
    "    uploader = GCSUploader(bucket_name, project_id)\n",
    "    return uploader.upload_directory(root_dir, prefix, dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "main",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to bucket: swhm_data\n",
      "Starting bucket crawl...\n",
      "Crawling bucket 'swhm_data' with prefix 'public/layers/'...\n",
      "Found raster: Age_of_Imperviousness\n",
      "Found raster: Flow_Duration_Index\n",
      "Found raster: HSPF_Land_Cover_Type\n",
      "Found raster: Hydrologic_Response_Units\n",
      "Found raster: Imperviousness\n",
      "Found raster: Land_Cover\n",
      "Found raster: Land_Use\n",
      "Found raster: Population_Density\n",
      "Found raster: Precipitation_mm\n",
      "Found raster: Runoff_mm\n",
      "Found raster: Slope\n",
      "Found raster: Slope_Categories\n",
      "Found raster: Soils\n",
      "Found raster: Total_Copper_Concentration\n",
      "Found raster: Total_Kjeldahl_Nitrogen_Concentration\n",
      "Found raster: Total_Phosphorus_Concentration\n",
      "Found raster: Total_Suspended_Solids_Concentration\n",
      "Found raster: Total_Zinc_Concentration\n",
      "Found raster: Traffic\n",
      "Found raster: copper_concentration_ug_per_L\n",
      "Found vector: PugetSoundWA\n",
      "Found vector: cig_grid_wgs\n",
      "Processed 23 objects from bucket\n",
      "Found 2 unique vectors and 20 unique rasters\n",
      "Found 2 vectors and 20 rasters\n",
      "Generating STAC items...\n",
      "\n",
      "Saving catalog files...\n",
      "Saved: catalog/catalog.json\n",
      "Saved: catalog/vector/collection.json\n",
      "Saved: catalog/raster/collection.json\n",
      "Saving individual STAC items...\n",
      "Saved: catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "Saved: catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "Saved: catalog/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "Saved: catalog/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "Saved: catalog/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "Saved: catalog/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "Saved: catalog/raster/Imperviousness/Imperviousness.json\n",
      "Saved: catalog/raster/Land_Cover/Land_Cover.json\n",
      "Saved: catalog/raster/Land_Use/Land_Use.json\n",
      "Saved: catalog/raster/Population_Density/Population_Density.json\n",
      "Saved: catalog/raster/Precipitation_mm/Precipitation_mm.json\n",
      "Saved: catalog/raster/Runoff_mm/Runoff_mm.json\n",
      "Saved: catalog/raster/Slope/Slope.json\n",
      "Saved: catalog/raster/Slope_Categories/Slope_Categories.json\n",
      "Saved: catalog/raster/Soils/Soils.json\n",
      "Saved: catalog/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "Saved: catalog/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "Saved: catalog/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "Saved: catalog/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "Saved: catalog/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "Saved: catalog/raster/Traffic/Traffic.json\n",
      "Saved: catalog/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "Saved: catalog/crawl_summary.json\n",
      "\n",
      "‚úÖ Catalog generation complete!\n",
      "   - Master catalog: catalog/catalog.json\n",
      "   - Vector collection: catalog/vector/collection.json\n",
      "   - Raster collection: catalog/raster/collection.json\n",
      "   - Vector items: 2 items in their respective directories\n",
      "   - Raster items: 20 items in their respective directories\n",
      "   - Crawl summary: catalog/crawl_summary.json\n",
      "\n",
      "üìÅ Generated directory structure:\n",
      "   catalog/\n",
      "   ‚îú‚îÄ‚îÄ catalog.json\n",
      "   ‚îú‚îÄ‚îÄ crawl_summary.json\n",
      "   ‚îú‚îÄ‚îÄ vector/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ collection.json\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ ItemName1/\n",
      "   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName2/\n",
      "   ‚îÇ       ‚îî‚îÄ‚îÄ ItemName2.json\n",
      "   ‚îî‚îÄ‚îÄ raster/\n",
      "       ‚îú‚îÄ‚îÄ collection.json\n",
      "       ‚îú‚îÄ‚îÄ ItemName1/\n",
      "       ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\n",
      "       ‚îî‚îÄ‚îÄ ItemName2/\n",
      "           ‚îî‚îÄ‚îÄ ItemName2.json\n",
      "\n",
      "üí° If you encountered authentication errors:\n",
      "   1. Install: pip install google-cloud-storage\n",
      "   2. Set up authentication:\n",
      "      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\n",
      "      - Or use: gcloud auth application-default login\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to crawl bucket and generate catalog with fixed structure.\"\"\"\n",
    "    # Configuration - update these values for your specific bucket\n",
    "    bucket_name = \"swhm_data\"  # Just the bucket name, not the full URL\n",
    "    prefix = \"public/layers/\"  # Path prefix within the bucket\n",
    "    project_id = None  # Set your GCP project ID if needed\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = GCPBucketCrawler(bucket_name, prefix, project_id)\n",
    "    \n",
    "    # Crawl the bucket\n",
    "    print(\"Starting bucket crawl...\")\n",
    "    crawl_data = crawler.crawl_bucket()\n",
    "    \n",
    "    print(f\"Found {len(crawl_data['vectors'])} vectors and {len(crawl_data['rasters'])} rasters\")\n",
    "    \n",
    "    # Generate collections and catalog\n",
    "    generator = CatalogGenerator(crawl_data)\n",
    "    \n",
    "    # Generate all STAC items\n",
    "    print(\"Generating STAC items...\")\n",
    "    stac_items = generator.generate_all_stac_items()\n",
    "    \n",
    "    # Generate collections\n",
    "    vector_collection = generator.generate_vector_collection()\n",
    "    raster_collection = generator.generate_raster_collection()\n",
    "    \n",
    "    # Generate master catalog\n",
    "    master_catalog = generator.generate_master_catalog(vector_collection, raster_collection)\n",
    "    \n",
    "    # Save all files with corrected structure\n",
    "    print(\"\\nSaving catalog files...\")\n",
    "    \n",
    "    # Save master catalog\n",
    "    save_json(master_catalog, \"catalog/catalog.json\")\n",
    "    \n",
    "    # Save collections (singular names)\n",
    "    save_json(vector_collection, \"catalog/vector/collection.json\")\n",
    "    save_json(raster_collection, \"catalog/raster/collection.json\")\n",
    "    \n",
    "    # Save individual STAC items in same directories as source data\n",
    "    print(\"Saving individual STAC items...\")\n",
    "    \n",
    "    # Save vector items in their respective directories\n",
    "    for item in stac_items['vector_items']:\n",
    "        # Find the corresponding vector data to get the directory\n",
    "        vector_data = next((v for v in crawl_data['vectors'] if v['name'] == item['id']), None)\n",
    "        if vector_data:\n",
    "            item_path = f\"catalog/{vector_data['stac_dir']}/{item['id']}.json\"\n",
    "            save_json(item, item_path)\n",
    "    \n",
    "    # Save raster items in their respective directories\n",
    "    for item in stac_items['raster_items']:\n",
    "        # Find the corresponding raster data to get the directory\n",
    "        raster_data = next((r for r in crawl_data['rasters'] if r['name'] == item['id']), None)\n",
    "        if raster_data:\n",
    "            item_path = f\"catalog/{raster_data['stac_dir']}/{item['id']}.json\"\n",
    "            save_json(item, item_path)\n",
    "    \n",
    "    # Save summary with enhanced metadata\n",
    "    summary = {\n",
    "        \"crawl_summary\": {\n",
    "            \"bucket_name\": bucket_name,\n",
    "            \"prefix\": prefix,\n",
    "            \"crawl_time\": datetime.now().isoformat(),\n",
    "            \"total_items\": crawl_data['total_items'],\n",
    "            \"vectors_found\": len(crawl_data['vectors']),\n",
    "            \"rasters_found\": len(crawl_data['rasters']),\n",
    "            \"stac_items_generated\": len(stac_items['vector_items']) + len(stac_items['raster_items'])\n",
    "        },\n",
    "        \"discovered_vectors\": crawl_data['vectors'],\n",
    "        \"discovered_rasters\": crawl_data['rasters'],\n",
    "        \"stac_structure\": {\n",
    "            \"catalog\": \"catalog/catalog.json\",\n",
    "            \"vector_collection\": \"catalog/vector/collection.json\",\n",
    "            \"raster_collection\": \"catalog/raster/collection.json\",\n",
    "            \"vector_items\": [f\"catalog/{v['stac_dir']}/{v['name']}.json\" for v in crawl_data['vectors']],\n",
    "            \"raster_items\": [f\"catalog/{r['stac_dir']}/{r['name']}.json\" for r in crawl_data['rasters']]\n",
    "        }\n",
    "    }\n",
    "    save_json(summary, \"catalog/crawl_summary.json\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Catalog generation complete!\")\n",
    "    print(f\"   - Master catalog: catalog/catalog.json\")\n",
    "    print(f\"   - Vector collection: catalog/vector/collection.json\")\n",
    "    print(f\"   - Raster collection: catalog/raster/collection.json\")\n",
    "    print(f\"   - Vector items: {len(stac_items['vector_items'])} items in their respective directories\")\n",
    "    print(f\"   - Raster items: {len(stac_items['raster_items'])} items in their respective directories\")\n",
    "    print(f\"   - Crawl summary: catalog/crawl_summary.json\")\n",
    "    \n",
    "    # Print directory structure\n",
    "    print(f\"\\nüìÅ Generated directory structure:\")\n",
    "    print(f\"   catalog/\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ catalog.json\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ crawl_summary.json\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ vector/\")\n",
    "    print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ collection.json\")\n",
    "    print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ ItemName1/\")\n",
    "    print(f\"   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\")\n",
    "    print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ ItemName2/\")\n",
    "    print(f\"   ‚îÇ       ‚îî‚îÄ‚îÄ ItemName2.json\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ raster/\")\n",
    "    print(f\"       ‚îú‚îÄ‚îÄ collection.json\")\n",
    "    print(f\"       ‚îú‚îÄ‚îÄ ItemName1/\")\n",
    "    print(f\"       ‚îÇ   ‚îî‚îÄ‚îÄ ItemName1.json\")\n",
    "    print(f\"       ‚îî‚îÄ‚îÄ ItemName2/\")\n",
    "    print(f\"           ‚îî‚îÄ‚îÄ ItemName2.json\")\n",
    "    \n",
    "    # Print authentication help if needed\n",
    "    print(f\"\\nüí° If you encountered authentication errors:\")\n",
    "    print(f\"   1. Install: pip install google-cloud-storage\")\n",
    "    print(f\"   2. Set up authentication:\")\n",
    "    print(f\"      - Service account: export GOOGLE_APPLICATION_CREDENTIALS='path/to/key.json'\")\n",
    "    print(f\"      - Or use: gcloud auth application-default login\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "upload_example",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UPLOADING STAC CATALOG TO GCS ===\n",
      "\n",
      "1. Dry run to preview uploads:\n",
      "Using gsutil for uploads\n",
      "Scanning directory: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog\n",
      "Target GCS location: gs://swhm_data/public/layers/\n",
      "Dry run: True\n",
      "--------------------------------------------------\n",
      "Found 26 JSON files to upload\n",
      "--------------------------------------------------\n",
      "üìÑ scripts/ipynb/catalog/crawl_summary.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/crawl_summary.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/catalog.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/catalog.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/collection.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/collection.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/vector/collection.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/vector/collection.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Traffic/Traffic.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Traffic/Traffic.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Slope/Slope.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Slope/Slope.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Land_Use/Land_Use.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Land_Use/Land_Use.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Total_Suspended_Solids_Concentration/Total_Suspended_Solids_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Land_Cover/Land_Cover.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Land_Cover/Land_Cover.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Population_Density/Population_Density.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Population_Density/Population_Density.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Total_Phosphorus_Concentration/Total_Phosphorus_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Imperviousness/Imperviousness.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Imperviousness/Imperviousness.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/copper_concentration_ug_per_L/copper_concentration_ug_per_L.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Age_of_Imperviousness/Age_of_Imperviousness.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Flow_Duration_Index/Flow_Duration_Index.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Slope_Categories/Slope_Categories.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Slope_Categories/Slope_Categories.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/HSPF_Land_Cover_Type/HSPF_Land_Cover_Type.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Hydrologic_Response_Units/Hydrologic_Response_Units.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Precipitation_mm/Precipitation_mm.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Precipitation_mm/Precipitation_mm.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Total_Copper_Concentration/Total_Copper_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Soils/Soils.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Soils/Soils.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Runoff_mm/Runoff_mm.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Runoff_mm/Runoff_mm.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Total_Kjeldahl_Nitrogen_Concentration/Total_Kjeldahl_Nitrogen_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "üìÑ scripts/ipynb/catalog/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/Total_Zinc_Concentration/Total_Zinc_Concentration.json\n",
      "   ‚Üí DRY RUN: Would upload with Cache-Control headers\n",
      "==================================================\n",
      "üìä UPLOAD SUMMARY\n",
      "==================================================\n",
      "Total files found: 26\n",
      "Successfully uploaded: 0\n",
      "Skipped (dry run): 26\n",
      "Failed: 0\n",
      "\n",
      "2. Actual upload:\n",
      "Using gsutil for uploads\n",
      "Scanning directory: /Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog\n",
      "Target GCS location: gs://swhm_data/public/layers/\n",
      "Dry run: False\n",
      "--------------------------------------------------\n",
      "Found 26 JSON files to upload\n",
      "--------------------------------------------------\n",
      "üìÑ scripts/ipynb/catalog/crawl_summary.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/crawl_summary.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ scripts/ipynb/catalog/catalog.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/catalog.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ scripts/ipynb/catalog/raster/collection.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/raster/collection.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ scripts/ipynb/catalog/vector/collection.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/vector/collection.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ scripts/ipynb/catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/vector/cig_grid_wgs/cig_grid_wgs.json\n",
      "   ‚úÖ Upload successful with Cache-Control headers\n",
      "\n",
      "üìÑ scripts/ipynb/catalog/vector/PugetSoundWA/PugetSoundWA.json\n",
      "   ‚Üí gs://swhm_data/public/layers/scripts/ipynb/catalog/vector/PugetSoundWA/PugetSoundWA.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m2. Actual upload:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#Uncomment the line below to perform actual upload\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m results = \u001b[43mupload_stac_catalog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCATALOG_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGCS_BUCKET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGCS_PREFIX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 217\u001b[39m, in \u001b[36mupload_stac_catalog\u001b[39m\u001b[34m(root_dir, bucket_name, prefix, dry_run, project_id)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03mConvenience function to upload STAC catalog files to GCS with cache-busting headers.\u001b[39;00m\n\u001b[32m    205\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m \u001b[33;03m    Dictionary with upload results\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    216\u001b[39m uploader = GCSUploader(bucket_name, project_id)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muploader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mGCSUploader.upload_directory\u001b[39m\u001b[34m(self, root_dir, prefix, dry_run)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m json_file \u001b[38;5;129;01min\u001b[39;00m json_files:\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_upload_single_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     84\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: Failed to upload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mGCSUploader._upload_single_file\u001b[39m\u001b[34m(self, file_path, root_path, prefix, dry_run, results)\u001b[39m\n\u001b[32m    110\u001b[39m success = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_gsutil:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     success = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_upload_with_gsutil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgcs_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client:\n\u001b[32m    116\u001b[39m     success = \u001b[38;5;28mself\u001b[39m._upload_with_client(file_path, gcs_path)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mGCSUploader._upload_with_gsutil\u001b[39m\u001b[34m(self, file_path, gcs_url)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# Set Cache-Control header to prevent browser caching\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# This forces browsers to re-fetch the STAC catalog files on each request\u001b[39;00m\n\u001b[32m    135\u001b[39m     cmd = [\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgsutil\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m-h\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCache-Control:no-cache, no-store, must-revalidate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    141\u001b[39m         gcs_url\n\u001b[32m    142\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:556\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    558\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:1222\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1219\u001b[39m     endtime = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1221\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     stdout, stderr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1224\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:2125\u001b[39m, in \u001b[36mPopen._communicate\u001b[39m\u001b[34m(self, input, endtime, orig_timeout)\u001b[39m\n\u001b[32m   2118\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout,\n\u001b[32m   2119\u001b[39m                         stdout, stderr,\n\u001b[32m   2120\u001b[39m                         skip_check_and_raise=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[32m   2122\u001b[39m         \u001b[33m'\u001b[39m\u001b[33m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   2123\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mfailed to raise TimeoutExpired.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2125\u001b[39m ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28mself\u001b[39m._check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[32m   2128\u001b[39m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[32m   2129\u001b[39m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:398\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    396\u001b[39m ready = []\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "GCS_BUCKET = \"swhm_data\"\n",
    "GCS_PREFIX = \"public/layers/\"\n",
    "CATALOG_DIR = \"/Users/christiannilsen/Documents/repos/swmh-stac-catalog/catalog\"\n",
    "\n",
    "# Example usage with new refactored uploader\n",
    "print(\"=== UPLOADING STAC CATALOG TO GCS ===\\n\")\n",
    "\n",
    "# Upload with dry run first to see what would be uploaded\n",
    "print(\"1. Dry run to preview uploads:\")\n",
    "results = upload_stac_catalog(\n",
    "    root_dir=CATALOG_DIR,\n",
    "    bucket_name=GCS_BUCKET,\n",
    "    prefix=GCS_PREFIX,\n",
    "    dry_run=True\n",
    ")\n",
    "\n",
    "print(f\"\\n2. Actual upload:\")\n",
    "#Uncomment the line below to perform actual upload\n",
    "results = upload_stac_catalog(\n",
    "    root_dir=CATALOG_DIR,\n",
    "    bucket_name=GCS_BUCKET,\n",
    "    prefix=GCS_PREFIX,\n",
    "    dry_run=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
